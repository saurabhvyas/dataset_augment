hello friends welcome back in this
tutorial we will study about the extras
al canto de extras algorithm is a single
source shortest paths algorithm it is
used to solve single source shortest
paths problem now what is single source
shortest paths problem right in this
problem in single source shortest paths
problem a graph is given to us and we
have to find a shortest path from a
given source vertex to each vertex of
the graph right now
what is the meaning of shortest path
right now see this weighted graph right
it is a weighted graph no the bit of a
path in a graph is the sum of the beat
of its constituent edges now in this
weighted graph see the path from vertex
u to vertex V this path u X V right now
the weight of this path is the sum of
the weight of its constituent edges
it means the weight of this path is the
sum of feeds of the edges UX + XV right
and it is 3 plus 1 it is equal to 4
right so the weight of this path u XV is
for right now see here there are many
paths from what that's you to vertex V
in this graph 1 is this 1 u w XV another
one is u w v right
but the path whose weight is minimum
right between the vertices U and V right
the path with minimum weight between the
vertices U and V is the shortest path
between the vertices U and V right
so in single source shortest path
problem
what we have to do we have to find a
shortest path from a given source vertex
to each vertex of the graph right now
what is the extras algorithm the
techstars algorithm is used to solve
single source shortest path problem for
those weighted directed graphs in which
all H fields are non-negative in this
algorithm we maintain two variables for
each vertex V of the graph right the
variable by V it denotes the predecessor
of vertex V but and the variable DV it
denotes the shortest path estimate of
were what XP from source vertex right
now suppose the vertex S is the source
vertex of the graph like now we have to
find a shortest path from vertex s to
each vertex of the graph by deject
Strasse algorithm right in this
algorithm in deject strands algorithm
first we will set the value of a
variable PI for each vertex of graph as
nits right so first of all what will we
do we will set the value of variable Phi
for each vertex of graph as myth but the
value of will variable D for each vertex
except sauce vertex s is set as infinity
and for source vertex s it is zero right
now up till this what we have to do in
disastrous algorithm is we have to
repeat this procedure until all vertices
of the graph are processed but we have
to repeat this procedure until all the
vertices of the graph are processed now
with this the procedure is right among
unprocessed for
we will choose a vertex with minimum
value of variable T and we will relax
its outgoing edges right now what is the
meaning of a relaxing and edge or a
jhalak session like suppose there is an
edge AV and we have to relax it like an
agile relaxation WeChat if the shortest
path to B can be improved by going
through what XP right now how do we
check this let's write the weight offer
this as a b is w right and as we know
that b-b-but DB is the shortest path
estimate off over deaths be from source
vertex s and de mr. shortest path
estimate of what X a from source vertex
s but now we check if D a plus W as last
time DB or not if da plus W is less than
D B then it means shortest path to what
that's B will be improved by going
through e right so we will set vertex E
as the predecessor of what XP right and
new value of variable D for vertex B
this da plus W right but if we're da
plus W is not less than VB then we will
not make any change right we will leave
it as it is right and this procedure is
called as a relaxation right now it will
be more clear after this example suppose
that this weighted graph is given to us
and here the source vertex is s right
and what we have to do we have to find
the shortest path for each vertex of
this graph from this source vertex s so
first of all what will we do first of
all we will set the predecessor value
right for each vertex as nil right so
first of all what will happen the value
of variable PI for each vertex of this
graph will be nil right
right now and what we have to do we have
to set the shortest path estimate of
each vertex except source vertex s as
infinity right so in this graph we have
set the shortest path estimate of each
vertex except source vertex as infinity
right and now what we have to do now
shortest path estimate of source vertex
s will be sat as 0 right now what we
have to do we have to repeat this
procedure until all the vertices of this
graph are processed right what we have
to do in this procedure first of all we
will choose a vertex with minimum value
of variability among unprocessed what is
s then we will relax the outgoing edges
of that vertex right so in this graph
right now the vertex whose the value of
variable D is minimum is what X s so for
vertex as there are two outgoing edges
one is si and another one is SP right so
we have to relax these two edges right
so if we set the predecessor of vertex B
as vertex s then shortest path estimate
of what XP will become 0 plus 5 that is
fine and 5 is less than infinity right
so what will we do we will set vertex as
as the predecessor of vertex B right so
PI B is that as as right we have relaxed
this edge now we have to relax this edge
as e right now the value of will be e
it's infinity right and the value of D s
is zero right now if we set this vertex
s as the predecessor of what XP then de
right value of variable T for vertex E
will become zero plus 1 that is 1 and it
is less than infinity right so what will
we do we will change the value of
variable D for what X E and we will set
what X as as the predecessors are all
vertex e right so what access is the
predecessor of what X a right so here we
have processed this what access now we
have these five unprocessed vertices the
vertex where the minimum value of
variable D among these unprocessed
vertices is what XP right the value of
variable D for this vertex E is one
right so we will relax all of its
outgoing edges right first of all we
will relax its outgoing edge a B right
value of variable D for vertex B is 5
and while your variable D for vertex a
is 1 right and weight of this edge is 2
right now C 1 plus 2 is 3 and it is less
than 5 right so what will we do
we will set the value of variable D for
vertex B as 3
right and we will start vertex E as the
predecessor of what XP right we have set
the vertex E as the predecessor of
vertex B right
now what XA is the predecessor of vertex
V now we will relax other outgoing edges
of vertex a another outgoing edge or
vertex a is really right value of
variable D for vertex D is infinity
right so here the value of variable D
for vertex a is 1 and the weight of edge
ad is 1 right now what we will do one
plus one is two right and two is less
than the value of variable of D for
vertex D that is 2 is less than infinity
so what will we do we will set vertex E
as the predecessor of vertex V right
we have set the vertex E as the
predecessor of vertex D right and the
value of variable D for what XD is satis
to right now the another outgoing edge
from vertex E is easy right now the
value of variable D for vertex C is
infinity and value of variable D for
vertex a is 1 and the weight of a GC is
2 right so 1 plus 2 is 3 and it is less
than infinity right so what will we do
we will set the value of variability for
vertex C as 3 that we have set the value
of variable D for vertex C as 3 right
and we have sat the predecessor of
vertex C as vertex V here we have shown
in this graph that's what XD is the
predecessor of what XC right so now we
have processed the vertex K also right
now the unprocessed what s sr b c d and
e right now the vertex which has minimum
well
for variable D among these four
unprocessed word s s is what XD right so
we will relax the outgoing catch from
vertex T right there is only one
outgoing edge from vertex T and it is de
right value of variable D for vertex D
is - value of variable D for vertex E is
infinity and the weight of HDE is two
right now see two plus two is four and
it is less than infinity so we will set
the value of variable D for vertex E as
for right because four is less than
infinity and we will set vertex D as the
predecessor of vertex E so value of
variable pi for vertex e SD because we
have side D as the predecessor of what X
e here I have shown it in this graph
what XT is the predecessor of vertex C
right
so we have processed the vertex D also
now we have three unprocessed vertices b
c and e right now here there are two
vertices B and C whose value of variable
D is minimum among them unprocessed
vertices so first of all we will process
the vertex B we can process what I see
also but here I will process the vertex
B now see there is only one outgoing
edge from vertex B that is B D so we
will relax BD right now C value of
variable D for vertex B is 3 weight of
HPD's 2 and value of variable D for
vertex D is 2 now C 3 plus 2 is 5 and it
is greater than 2 but it is not less
than the value of variable D 40 that is
not less than 2 right 3 plus 2 is 5 and
it is not less than 2 so we will not
make any change here we will leave it as
it is right now we have a
to unprocessed what SSC nd right so
first we will find you whose value of
variability is minimum right here see
this what X see it's a variable DS value
is 3 right so it is less than the value
of a variable T former taxi now we will
process the vertex C it has only now
there are two outgoing edges from vertex
C one is c e and another one is C D
right so first we relax this edge C D
right now the value of variable D for
vertex C is 3 and weight of edge C D is
3 right and value of variable D for
vertex D is 2 right now C 3 plus 3 is 6
and it is not less than 2 right so we
will leave it as it is right now see
this as C now we will relax it the value
of variable T for vertex E is 4 and the
value of variable D for vertex E is 3
and the weight of this edge C E is 1 now
C 3 plus 1 is 4 right and it is not less
than 4 so we will leave it as it is but
we will not change it now only one
unprocessed vortex is left that is
vertex E now there is no outgoing edge
from what xe2 relax so we will leave it
here right now we have got this single
source shortest paths tree from vertex s
right here vertex s is the source vertex
right and now see suppose we have to
find the shortest path from vertex s to
vertex E then it will be as a d e right
suppose we have to find the shortest
path
- vertex P from source vertex as then it
will be as ap right so it is a single
source shortest paths tree right
and here source vertex is vertex s right
okay in this video I'm going to briefly
explain
Jessica's algorithm little strange to
say the name
all right so
I'm going to use a weighted graph
all right
okay
let's connect these here
all right so now we're going to give
each point of value so let's label these
points a a will be C D
e F and G okay
then we're going to go ahead and give
that values to each path
okay so we have six here eleven here
eight here and nine all right so our
goal is to find the shortest path from A
to G okay
so here we go what we do here is we find
the shortest path from A to G so we're
going to go ahead and calculate the
shortest path from A to B which is seven
so we put seven here and then we write
its origin right next to it where it
originated where it came from so we'll
put a so the distance is seven and it
originates in a alright so A to C A to C
is seven plus eight which is fifteen
okay and the origin is from A to B
that's its origin and that's its value
15 so we're going to go from A to E from
A to E is 15 plus 5 thats 20 and beside
it right its origins from A to B and B
to C okay so now we're going to go a
different route see how it goes so from
A to D is five and of course the origin
is a from A to F is 11
that's six plus five sorry five plus six
and the origin is a 2d okay
now we're going to go to G A to D D to F
and F to G is 22 with the origin a to D
and D to F okay so we did this path here
we're going to do a to B and B to e so A
to B we know is seven and B to e is
another seven so we have 14 by way of a
to be its origin so 14 is less than 20
so we cross out this path we will not be
taking this path because it's definitely
not the shortest so so far our shortest
path is a to B and B to e so A to B B T
E and E to G gives us 23 by way of A to
B and B to e what we have here is 22 as
a shortest path so we cross out this
path so shortest path using this
algorithm is a 2 D
D to F and F to G and that's it
hi I'm Joe James and today we're going
to talk about Dijkstra's algorithm to
solve single source shortest path
problems the single source shortest path
problem is to find the shortest path
from one designated vertex which we'll
call the source to every other vertex in
the graph
Dijkstra's works in directed or
undirected graphs and it only works in
graphs with non-negative edge weights 0
edge weights are okay but non-negative
edge weights so let's take a look at an
undirected graph this is the example
we'll use we've picked a source vertex
in the top left corner we're going to
run Dijkstra's algorithm on this
undirected graph we'll find the shortest
path from s to every other vertex in the
graph we initialize the distance to the
source to 0 and the distance to all
other vertices to infinity now we relax
all out edges from s and by relaxed I
mean we'll check if this edge can give
us a shorter path to it's connected
vertex than our current shortest path
since all other paths are currently
infinity of course we're going to
improve on those times so we'll relax
the edges in alphabetical order starting
with s a this edge gives us a distance
from s of 8 with the last vertex in the
path is s so we'll mark si as done by
marking it blue next we'll relax edge SC
with a weight of 6 this gives us a
distance from s to C of 6 and the last
vertex in the path is s
next is edge SD which gives us a path to
D with distance of five coming from
vertex s now we've relaxed all of s's
edges so we need to pick the next vertex
to move to we choose the vertex that is
closest to the source based on our
current distance estimates vertex D has
distance of five so D is the closest now
we'll relax all the edges out of D that
we have not yet relaxed there are three
so we'll handle mathematically edge ad
has a weight of two if we add that to DS
current distance from the source of five
we see that we have just found a shorter
path to a than the eight that we had we
can get there by in seven s to D is five
and D to a is two so we can reach a with
a distance of seven by passing through
vertex D next we'll look at C D with a
weight of three but that doesn't give us
a shorter path than what we already have
so we mark CD is done and we move on
next we look at edge D F has a weight of
four we add this for two DS distance
from the source and we get a distance of
nine for F via vertex D now we're done
with all of these edges so what is our
next nearest vertex to the source C with
a distance of six from the source C has
only one remaining edge to relax which
gives us a distance of 15 to F but that
is worse than the distance we already
have of only nine so we mark that edge
is done and move on
the next nearest vertex the source is a
with a distance of seven
a has only one edge that we haven't seen
yet it gives us a distance of eight to
vertex e so we update E's distance and
the preceding vertex which is a next we
visit vertex e he has two edges to relax
edge B E gives us a distance to be of 14
so we update B's distance now you can
see that these predecessor values can be
used to trace the shortest path to any
vertex here's the path we took to get to
B you can see that the edge weights all
add up to 14 and you can also see that
by tracing B's predecessor is e these
predecessor is a a predecessor is d DS
predecessor is s and that's how we got
from the source to vertex B next we'll
relax
edge eg that gives us a distance to G of
12 now we visit vertex F because it's
the closest to the source that we
haven't visited yet F has one edge to
relax which gives us a free ride to G so
we now have a path to G that is three
better than we had before so we're going
to update G's distance to nine coming
from vertex F then we move to vertex G
but G has no edges to relax so we more
moved to vertex B which also has no
edges to relax so now we're done now we
have the shortest distances from s to
every other vertex and we can trace the
path we followed using our predecessor
values
so Dijkstra's algorithm was developed by
Edsger Dijkstra in 1956 it uses a min
priority queue or a min heap to find the
next nearest vertex this is known as a
greedy algorithm because it always
selects the shortest of the available
options and it runs in time Big O of
elog V where E is the number of edges
and B is the number of vertices in a
graph I'm Joe James thank you for
watching
hi I'm Joe James in this video we are
going to apply Dijkstra's algorithm to
this directed graph now just as a
reminder Dijkstra's algorithm can be
applied to either directed or undirected
graphs and it can only be used with
non-negative edge weights so if we have
any negative edge weights we cannot
apply it to this graph in this case we
have one zero edge but no negative edge
weights so we can apply Dijkstra's
algorithm on this graph to find the
shortest path to each vertex from a
single source in this case we'll call
that source s on the top left here so
the first thing we need we're going to
use a table to track the distance of
each vertex from the source and we're
going to initialize those distances all
to infinity with the exception of the
source itself since that's where we're
starting and we'll use another variable
called PI to track the predecessor
vertex or the vertex we just previously
visited to apply Dijkstra's algorithm we
will start at the source and we will
relax each of the outgoing edges in
alphabetical order so we have two
outgoing edges from the source we are
going to start with s to a and with a
distance of two so since we currently
have a distance for a of infinity we can
now reach a in two with the predecessor
of s so we'll update our table to
reflect that we can reach a in two
coming from the source and we're going
to mark this two as already finished and
next we're going to relax edge s D so
now we can reach vertex D with a
distance of 20 previously our best was
infinity so 20 coming from the source
now we'll mark this edge as completed
now we're done relaxing all the outbound
edges from vertex s will mark vertex s
as completed and we'll move on to the
next vertex a why would we choose a well
because that is the closest vertex to
the source at a distance of only two as
we could see in the table here D is at a
distance of 20 and all the other
infinity so we'll jump to vertex a will
relax all of its outbound edges in this
case we only have one edge going to e at
a cost of three so if we can reach a and
two and we add 3 to that we can reach e
in 5 so we'll put a 5 under E and what
is E's predecessor a we just came from
vertex a now we're done with vertex a
well mark that as visited now let's mark
edge a e as already completed we'll
visit vertex E which is the next closest
vertex to the source as you can see ease
only 5 from the source he has 3 outbound
edges to G H and B will visit B first
because it comes first alphabetically so
edge e B has a weight of 1 so we can
reach e in 5 we can now reach B and 6 by
coming from vertex E will mark this edge
is visited now we need to look at ease
next edge which is e G with a weight of
well it takes us 5 to reach e it would
take us 11 to reach G we currently have
infinity so we'll accept that well mark
E is the predecessor for G and then
we'll mark this edge as completed and
lastly we have eh with the way to 4 you
currently don't have a route to H and it
costs us 5 to reach e so we can reach H
now in 9 coming from vertex E will mark
edge 4 as complete and then we'll mark
vertex E as visited now we're going to
choose the next vertex to visit which
one is nearest to the source that we
haven't visited yet well B is 6 so
that's the next closest so let's go to
vertex B B has only one outbound edge B
to C at a weight of 7 we currently can't
reach C at all so anything's better than
that so 7 plus 6 is going to give us 13
to get to vertex C and we'll mark B is
the predecessor then we can mark this
edge as visited and we're done with
vertex B so we'll mark B is finished and
we can move on to our next vertex looks
like H is our next nearest vertex
so let's visit vertex H H has to
outbound edges H II with the weight of 2
and H G with a weight of 1 will visit
them alphabetically first edge which is
H e we can already reach a and 5 we can
reach H and 9 so if we add two more to
that we can get to e + 11 if we wanted
to that doesn't benefit us so we can
mark this edge as complete and now we
can look at HS other edge
so getting to H in 9 and we add 1 to
that to reach G we can get to G + 10 so
we found a better route to G by going
through vertex H so we'll update G's
predecessor as well and we'll mark this
edge as complete we have no more
outbound edges from H so we can mark
vertex H as complete next we'll visit
vertex G which is the next closest
vertex to the source with a distance of
10 G has only one outbound edge going to
D in our current distance to G is 10 we
can add 2 to that to get 2 D with this
edge weight of 2 now we can reach D + 12
that's better than what we currently
have at 20 so let's update DS distance
from the source to 12 DS predecessor is
now going to be G and we can mark this
edge complete now we have no more
outbound edges from G so we can mark G
complete the next nearest vertex from
the source is vertex D so we'll visit
vertex D which has no outbound edges we
can mark D is complete the next nearest
vertex from the source is C with a
distance of 13 so we'll go to C and we
only have one outbound edge which has a
distance of 5 to F so we can reach F in
13 plus 5 or 18 so update s distance and
S predecessor to C we can mark this edge
is complete and then we have no more
outbound edges from C so we can mark
vertex C is complete and our last vertex
to visit is vertex F which currently has
a distance of 18 f has one abound edge
with a weight of 0 that's definitely not
going to help us on B because we already
have a distance of 6 where f has a
distance of 18
so that does not give us a shorter path
to be by passing through F so we can
mark this edge is complete we have no
more outbound edges from F so we can
mark vertex F as complete then then
we're finished now we've applied
Dijkstra's algorithm on a directed graph
to find the shortest distance of each
vertex from the source if you enjoyed
this video please give me the thumbs up
and click the subscribe button I'm Joe
James thanks for watching
and welcome to tutorial 10 on Dijkstra's
algorithm as this comes up in the edit
cell decision 1 maths a-level module but
should be applicable to most other
decision maths courses for any other
help with your math studies GCSE array
level C Hegarty maths or the website
right let's start by looking at what
edit cell says we need to know now
everything in Green's what we've done in
previous tutorials in this tutorial
we're going to do Dijkstra's algorithm
for finding the shortest path then we
will have finished another sort of
subsection of the elliot cell d1 course
right I want to start off by showing you
a few problems I showed you when I
introduced graph to you back in tutorial
6 I think so I showed you this picture
I said let's zoom in and I said on a
weekly basis let's say I Drive my Aston
Martin from Milton Keynes to Chesson how
do I get there in the quickest way do I
go along this route do I go along this
route or do I do some sort of
combination in between how do I get
there in the shortest possible distance
assuming those numbers are say
kilometres between each of the towns
there's a problem that Dijkstra's
algorithm we're going to learn about
today solves for us now in the case here
you could have a look at that and by
trial and error get an answer but it
works Dijkstra's for much bigger
problems let me show you another way how
it might be used here's the world famous
mountain the Eiger which until the sort
of middle of the 20th century was
thought to be unclimbed ball ok and
since then people have climbed the Eiger
but Dijkstra's algorithm a way of
finding a path and can be used for
mounting climate and let me show you how
a lot of modern climbers might profile
the mountain and work out a picture of
it using technology and then they may
say stuff like imagine on a path here
they may say if I climb along here and
say the climbing difficulty is or
difficulty maybe - it's nice and easy
but then that
with a climb up there or maybe
difficulty let's say ten and they might
be comparing it at that point to a more
difficult climb here of let's say six
but then a slightly easier climb here of
three and along the way they might use
Dijkstra's algorithm or some form of it
to climb to the top of a mountain and
for example there's a very famous
climber called uh leash tech who very
recently climbed the Eiger in less than
three hours and I want to show you a
quick video clip of him it's very
inspirational it just in his eye during
his head he's using forms of Dijkstra's
algorithm or um you know he may have
pre-planned a bit of this sort of work
to get up the mountain let's take a look
and after you clamp this if you reach
the point where you're into it one hand
yes you have in your mind you have to
move fast but on the other hand you know
exactly if I do a mistake I fall off and
you see just more than a thousand meter
air below you and you're like on disk on
this edge
[Music]
go on to the final snow field now you
have to get out of your body and sing it
together as fast possible to the summit
[Applause]
[Music]
[Applause]
[Applause]
[Music]
you just see your hair your eyes are
your grandpa's and they have to just
move
[Applause]
[Music]
[Applause]
[Music]
in progressing without something that
it's all about
you wanna keep keep moving having a
progress in your life then you get to
the summit and you push the watch first
is try to breathe a little bit getting
some oxygen in your lungs and I saw
these timers like oh that's not possible
yeah I was a good moment
[Music]
[Music]
so that was a remarkable video there a
foolish debt climbing but modern
climbers do use some of the theories of
Dijkstra's algorithm to find a you know
route of these difficulty upper mounting
for example so let's now introduce
ourselves to Dijkstra's algorithm so we
can go off and Driver Aston Martin on
the weekends or climb mountains so
Dijkstra's algorithm finds for us the
shortest path between two vertices or
nodes in the network right if we take a
look at this say we're starting at a
node s and we want to end at no T how do
I get there in the shortest wait okay
now an easy picture like this you could
just work it out by trial and error but
Dijkstra's algorithm gives us somewhere
to something to work with when the
problem is much more complicated right
and this is what Dijkstra's algorithm
actually says it has the following seven
parts to it and which I'm not going to
read out for you now I suggest you pause
the video and copy them so you've got a
copy of what Dijkstra is saying and you
can refer to it as we do the examples
and but the best way to explain this
algorithm is to do an example so let's
move on to doing an example of Dijkstra
here's the problem that I just spoke to
you about before now you may be shown
the problem in that type of way but in
the examination they will always convert
this problem for you as follows now they
will create a box at each vertex where
over here you put in the name of your
vertex s a DT B or C etc in the final
value you're going to put the weight it
took you to get to that place okay and
in the middle is the order in which you
labeled and that particular vertex at
the bottom we're going to have some
working so this will be transformed into
something like this and that's the same
problem it's just has the boxes as I
talked about there below so we're going
to solve this problem how to get from s
to T in the shortest distance using
Dijkstra's
so let's have a go step 1 so what the
step 1 said well step one says label the
start vertex with a permanent label of 0
and then all the label 1 so the order is
1 and the permanent is 0 what that's
telling me is it took me how far how far
did it take or how much weight did it
take to get from s to this point well
we're already at so it took me nothing
and one is the first vertex i've
actually labeled ok and this as simple
as that okay what does step tuesday step
2 says having done that you assign
temporary labels to all the vertices
that can be reached directly from the
start so the vertices that can be
reached from s are a B and C and you put
in some temporary labels ok temporary
now what you do here is you ask yourself
how much weight does it take to get from
s to a well it takes five what about s
to B it takes six and s to C it takes
two and you're done the next step
these are temporary at the moment they
may convert to permanent in a second ok
with a step 3 study well step 3 sets
select the vertex with the smallest
temporary label in your diagram and make
this permanent and add the correct order
label well what's more about 5 6 + 2 2
is so therefore I'm gonna this is the
second one I'm going to give a permanent
label - and how far did it take to get
to here it took 2 right and we've done
step 3 nice and simple right
step 4 step four says the following now
what that's saying is is has put
temporary labels on each vertex that can
be read from read from the water rates
from the one you've just made permanent
this is the one I've just made permanent
I'm going to assign and temporary labels
to anything I can get to from C well I
can get to B from C with another 2
already took two to get here another two
so that's four now I write in the fall
because it's better than the six I've
previously gotten across that six on
what else can I get to from C well I can
get here by going to and then twelve so
in total 14 I can't get anywhere else
from C so I'm done there
okay and now in this case here if by
going up here here say that distance
there had been something like six and to
get from here to here been a I wouldn't
Rim it in okay I only write it in if
it's smaller than the temporary label
already there and that's what this is
saying here okay right now next stage
now let's go on to the next stage a.m.
Stage five or Step five tells us to now
select the vertex with the smallest
temporary label and make it permanent
okay and then order it correctly
now what we temporal label there's a
five that's the four currently that's a
14 which is the smallest this one
clearly here so it's going to be our
third one that we is the third one in
order and it's going to have of wait to
get there four okay so and that's what
stage right now next stage step six step
six says repeat this until finishing
until the finishing vertex T has a
permanent label so we're going to keep
the process going the one we've just
made permanent okay we consider
everything coming out of there D I can
get to D from there with four currently
to get to be in another four so BA from
B I could get to T it's already four to
get there another eight would be 12 it's
better than 14 cross out the 14 right 12
you can't get anywhere else
from B that hasn't got a permanent label
on it already okay so at this stage what
you do out of everything that's
temporary you consider the smallest or
smaller out of 5 8 and 12 or 5 is so
it's going to be the fourth one you
label up and it's going to have weight
to get there or five right
you consider everything you can get to
from a you can get to D where you can
get to D in another four on top of the
five you've already got but that's not
better than a that would be nine so we
don't bother writing it you can't get
anywhere else from eight so out of D and
T which is smaller out of eight and
twelve related so that's going to be the
fifth one we label and it's going to
have weight of eight then you consider
everywhere you can get to from D but I
can only get to I can only get to T by a
provider this way okay so that would be
the a I've already got an 3 B 11 spare
than 12 so I write it in and therefore T
is the sixth one I've labeled and it had
a weight of 11 so lastly we move on to
step 7 and that says we need to now find
our shortest path we're done we've done
will the algorithm we need to find our
shortest path by starting at the end
vertex retracing your steps to the start
and then stating your route in its
length how do you do that
well at T here it took us 11 to get here
which of these parts into T did I take
well 2 and the 12 is not 11 can't be
there 4 and 8 is 12 it's not 11 can't be
there 8 and 3 is 11 so I must have taken
in that route in to get into T so
highlight it same here
5 + 4 is 9 kind of taking that route
because it's got v8 and and four four
and four is eight so I must have taken
that route okay
from here now what does a well eleven
and a is not for so no and zero and six
is not for so no and two and two is four
so I must have taken that route there
and lastly the only route I could have
taken was this one here because there's
only one path coming out right so I went
s first then C then B then D and then T
so I went s C B D and T and it was of
length 11 make sure when you're doing
your Dijkstra's algorithm you do draw in
the endru
and you state the route and the length
otherwise you've done a problem but you
haven't actually solved it so do make
sure you do that last thing to stay
there for at the end there's a couple of
extra points I wanted to say to you
here's my final picture and Dijkstra's
algorithm haven't done this picture it
also finds for you the shortest route
but between the start and any point
what's the shortest route between s and
D 8s and a 5s and c2 this number here
tells you the shortest route from the
star to that number to that letter or
vertex so it does that for you
automatically and lastly just be careful
if there's a digraph in the network here
I've just changed around I've drawn an
arrow here and okay and what that saying
to you is from my starting point s okay
I couldn't have gone along here to a
with route or with a length of 5 it
would going against the way the error is
it's like a one-way street I can't take
that so don't consider it when you're
assigning your temporary and permanent
labels etc so do be careful of these now
at this stage I think the best thing to
do is to have a go a few examples I'll
put in the question up you pause the
video attempt the question and then mark
your work against mine in ten seconds so
take a look at this I'll stop talking
and in ten seconds I'll go through the
answer
right
thing I do is labeling all the vertices
that was a b c d e f g h i and j lastly
this question says start at a and you
want to end up the ring vertex which is
j so when it end up j right
first thing i do is in here i say that's
the first one i label and it's got a
zero or waiting to get from a to a
everything that comes out of a i
assigned temporary so that would be a
four that would be a seventeen and this
here would be a 10 okay look at all your
temporary labels which is the smallest
this one so label it the second one
you've label permanent with waiting for
now you consider everything that comes
out of B well I could get to e here in a
four plus two which is six so that's
better than ten I could get here in four
plus the 14 which would be 18 and that's
all I could get to from being look at
all your temporaries what's smaller out
of 18 6 and 17 or sixes so that's the
third one you make permanent and it's of
weight six now you can see that
everywhere you can get two for me could
go up here in 11 plus the six you've
already got seventeen that's equal to
the best we've got currently you could
go down here in 18 which would be six to
get here and 18 so that would be 24 you
can go along here with the 6 and the 8
of 14 and there's nowhere else you could
go to look at all your temporary labels
18 17 24 and 14 which one's your
smallness well 14 is so that's going to
be your fourth one of weight 14 okay
consider everything you can get to from
page four go back here in 14 to 16 30 no
good okay can go along here in 14 plus
15 which is 29
okay could go down here in 14 plus 3
which is 17 and and that's it can't get
anywhere else here that would do any
better now consider all your temporary
labels or smaller out of 18 17 24 17 and
29 well 17 is the smallest there are two
of them you can choose any at that stage
in your algorithm if there are two the
same choose any so why don't we choose
this one okay let that be our fifth
label and it's a weight 17 now you can
see where you can get to from D okay
I've get along here in another one plus
17 which would be 18 I can get along
here in another 16 but that oh so I
don't count that because this is already
permanent there's nowhere else I could
get to from D that it to some of that
isn't permanent okay so therefore what
smaller out of all these well this 17 is
still the smallness so that must be the
sixth one and I lay with that 17 okay
from here where can I get to get up here
I could get along here in another seven
so that would be 24 which is better than
29 and could get along back here 17 plus
9 which would be 26 is not better than
24 and so they're all the possible
options I can do the best of my
temporary labels now I've got 18 here
24:18 okay it's definitely 18 so why not
call this my next one here so this would
be the seventh one and of way 18 where
can I get two here like the only place I
could go that's temporary at the moment
is along here another 10 which will be
28 that's not better so I don't want
anything in here consider all my labels
here I've got 24 18 and 24 the smallest
is clearly 18 so that must be the eighth
when I label with wait 18 consider
everywhere I can get you from here well
I can only get along here which is
another 12 would be 30 it's not better
than 24 so I can't write anything in and
the best that these two well why don't I
just stated this one just to this one
doesn't matter
that one's 24 so that would be the ninth
of 24 and I can't get anywhere here
that's any better and this would be the
tenth one of wait 24 okay and I've
carried all of that out there but now I
must work out what path it was so
starting here at 24 18 and 12 is not 24
okay
14 and 15 is not 24 okay 17 and 7 is 24
must have taken that route so at 17 here
okay
and the only option to get there in the
crowd would have been from here because
14 + 3 is 17 so I must have taken that
route from here well it can't be here
it's got to be here that's the only one
must of taking that route because 8 + 6
is 14 from here how did I get here in 6
well not there not there not there it
must have been along here and last one
must have been up here ok so this was my
shortest route and state what it is
whether it's a b e:h i and j and it has
weight and in total 24 so wait 24 the
route and the length is a ruse a b e:h
rj and it's of length 24 ok next one
have a go at this next question and be
careful here there's diagraphs involved
so just take a look at the next one
pause the video attempt the question and
mark yours against mine I'll go through
in 10 seconds
okay we're starting at a okay we're
starting at a and we want to end at the
ring vertex which is G do you notice
this arrow tells us I can only go along
that direction this arrow says I can go
there this arrow there
this arrow back so there's a cop that
arrows going involved where there's not
an air you could go either way okay
start off that was a b c d e f and g
okay and then this is my first point and
it's got weight zero to get there okay
because you're already there then
consider all temporary labels i could go
up here in six I can't go along here no
but I could go along here in three what
smallest out of these two well this one
is so it's the second one and of weight
3 where can I go to from here along here
which would be another 20
so that would be 23 I can't go along
here actually no I cannot go along here
so therefore that's the only place and
get to the smallest out of my temporary
labels is therefore this one which is my
third one and it's of weight six where
can I get to from him well I could go
along here in the six and 11 which would
be 17 which is better or I could go
along here which is six and five which
is 11 okay now I consider which is the
smallest out of everything I've got
everything I've got here is a 17 and 11
that's the smallest so it's the fourth
one I label of weight 11 where can I get
two from II but I can't go long here we
go in the wrong direction but I can go
to along here which would be 11 and 9
which is 20 okay and at this point now I
look around to see what my smallest is
17 and 20 this is my smallest so it's
the fifth one I label and it's of weight
17 at that point okay and then I ask
myself where I can get to from here will
I the only places I could go from here
to a temporary places I go along here
which would be another one which would
be 18 or I can go along here which would
be 17 17 34 to this point which is not
better so I look at all my temporaries
which is smaller 18 and 20 well the 18
is so that's the 6:1 a label of weight
18 and from the one I've just called
permanent here where can I get to from
here well along here with another 14
which would give me 32 which is not
better so therefore and this must be the
seventh one a label and it's of weight
20 okay and finally you just end by
stating your route so I start at the
back how did I get to here I must have
gone along here because 11 at 9 is 20
what about from here well I must have
gone along here because 6 + 5 is 11 and
here I must have gone along here because
0 + 6 is 6 so my route here was a b e
and g and it had weight of 20 get to
ensure you write your route and the
length at the end of the question
finally to finish his some questions to
do further I would suggest you read
chapter 3 page 51 to 58 and look at the
examples in particular and digraphs
because I just mentioned them here but
there's a few good examples there and
exercise 3 D on page 56 I suggest you do
all questions last thing then I suggest
you do is you go to the past paper
questions 10 video it's going to look
something like this it's going to be on
Dijkstra's algorithm some of the past
paper questions that came up so you
could practice the theory you've just
learned on the exam questions I hope you
found the following useful in your work
on decision maths 1 thank you for
watching and tune in next time
I'm Bob Sedgewick, professor of Computer Science
at Princeton. The study of algorithms and
data structures is fundamental to any computer
scientist's education. But it is not just
for programmers and Computer Science undergraduates. Everyone who uses a computer wants it
to run faster, or to solve larger problems. The algorithms in this course represent a body of knowledge
developed over the last 50 years that has
become indispensable. From N-body simulation
problems in physics, to genetic sequencing
problems in molecular biology, the basic methods
we will consider have become essential in
scientific research. From architectural modeling
systems to aircraft simulation, they have
become essential tools in engineering, and
from database systems to internet search engines,
they have become essential parts of modern
software systems. And these are just a few
examples. As the scope of computer applications
continues to grow, so grows the impact of
the basic methods that we will cover. In Part
II of this course, we survey graph-processing
algorithms, including minimum spanning tree
and shortest-paths algorithms, and string processing
algorithms, including string sorts, tries,
substring search, regular expression, and
data compression. We conclude with an overview,
placing the contents of the course in a larger
context.
Hello friends. my name is Tushar and today we are
going to talk about single source shortest path
using Dijkstra's Algorithm.  So, what is single 
source shortest path? so, you are given a graph and
you are gven a source vertex, and you have to find
the shortest path of every other vertex from this
source vertex . So, for example, here, i have this very
simple graph and my source vertex is A. and i have
to find the distance, the shortest distance of every 
other vertex from A. So, for example, C, C can be
reached from A-&gt;B-&gt;C or A-&gt;F-&gt;C or A-&gt;D-&gt;F-&gt;C, so, 
the shortest distance here is via B and the distance
is six. Similarly, E can be reached via A-&gt;F-&gt;D-&gt;E 
or A-&gt;D-&gt;E and the shortest distance here is this
and again the distance is six. So, Dijkstra's
Algorithm works both on directed and undirected
graph as long as the weight on the edge is
non-negative number. Also, Dijkstra's Algorithm is a
greedy algorithm and it is very very similar to 
Prim's Algorithm of finding Minimum Spanning Tree,
which i have already discussed in another video. Also,
for Dijkstra's Algorithm, we need this Data
Structure, Heap plus Map which supports these four
operations. add, extractMin, contains and decrease
key and i have already talked about this Data
Structure in Prim's Algorithm. This Data Structure
is nothing but a combination of a Binary Heap and a
Hash Map and the link to that video is also attached
in this video here. So, watch that video first before
watching this algorithm here. So, now, in
the next section, let's see how Dijkstra's Algoithm
works. So, we are able to run Dijkstra's Algorithm
on this undirected graph here. Here, the source
vertex is A and we are trying to find the shortest
distance of every other vertex from A. So, the way
this algorithm works is, I have this Heap plus Map
Data Structure and then i am going to put all these
vertices in this Data Structure with the distance
infinity from A.  because i dont know how far they
are from A at this point of time. Then, we are going
to set A's distnce to zero because A is at distance
zero from itself and then we're going to do an
extractMin here to basically get A out and then
explore all its neighbours and update their distance
from A. So, we will get B,D,E and updae their
distance, so, B five, D nine and then E two from A
and then i will do another extractMin, so, get E
out and explore its neighbours, so, that's F and the
distance of F from A will be whatever the distance
E is from A and plus this weight here, so, that's
five, so put five there in place of F and then keep
doing this extractMin until we have elements in this
Data Structure here. This Distance Map is going to
store the distance, the shortest distance of the
vertex from A while this path Map is going to help
us trace back the path, the shortest path from A to
every vertex. So, let's do a Dry Run of this
algorithm on this graph here. So, initially, our
source vertex is A, so, we will put this value
as zero, then we do an extractMin here and then we make
our current as A, so then we are going to explore
neighbours of A. So, the first neighbour of A is B,
so, let's first remove A from here and then update
A's distance as zero and A's parent as null because
A is the source vertex. Then, we are going to explore
neighbours of A, so first neighbour of A is
B, so, B exists in this map here, yes, and then
the distance attached with B is greater than this edge
weight here, so we will update this distance to this,
basically saying that we found a path which is
shorter than what you know right now, so, we will
update this to five. Then we are going to explore.
Also, we are going to say that B's current value
is coming from A. Also, then we are going to explore
another neighbour, so, D and then the current
distance attached with D is infinity, while this
edge, while this guy says that you can reach D from
A in distance nine, so, we will update this with nine
and also say that this nine is coming from A.
So, D is also coming directly from A and then explore
another neighbour, E, so the distance of E from A
is two and E exists here, so we will replace this with
two and we will say that E is also coming from A.
So, at this point of time, A is done exploring all
its neighbours, so, we are going to go in here again
and do another extractMin. So, the least value here
is E, so we'll take E out,so our current will become
E and we will take E out and put its distance as two
and now we will explore neighbours of E. So, the
first neighbour of E is A and A doesn't exist in
this Heap plus Map Data Structure, so we ignore A
and then the second neighbour of E is F and F does
exist here and the value attached with F is infinity
while if we went via E, it can reach A in distance
two, whatever the minimum distance it takes to reach
E plus the weight of this edge. So, value of F goes
to five. Two plus three five. So, also we are going
to say that F got introduced by E. So, at this point
of time, we are done exploring all the neighbours
of E, so we are again going to do an extractMin on this
Data Structure. So, we can pick either of them, so,
let's pick B and say that B's minimum distance from
A is five and okay, so, we will explore neighbours
of B, so, first neighbour of B is A but A doesn't
exist here, so, we will just ignore A and then
another neighbour of B is C and C does exist here and
the value attached with C is infinity while if we
go via route B, then it can reach A in distance two
plus whatever is attached with B, so, that's two
plus five seven and then we're going to say that C
got introduced by B. At this point of time, B
doesn't have any other neighbours. So, we are again
going to go and do an extractMin here. So, this
time, five comes out because five is the least value
of all three. So, F is the least value of all three,
so, F comes out and we are going to update F's
distance to five and then we are going to explore
neighbours of F. So, first neighbour of F is E but E
doesn't exist in this map here,so, we're going to
ignore E because we've already found the shortest
distance of E from A. Then we are going to explore
another neighbour which is D, so currently D does
exist here and currently we found one path to D via
this route but if we took this route, basically this
route, we can reach D in a shorter Distance because
we can reach F in distance five and from F to D, it
will take five plus two seven, so, that distance
is shorter than what it is right now, so, we will
replace D with seven. So, before D could be reached
with distance nine but if we took this path via F
and F can be reached in five and weight of this edge
is two. So, we can reach D in seven. So, we're going to
go here and say that now D's parent is F because that's
the guy who introduced seven for D. Then, now
F has no more neighbours to explored, so we will again
go and do an extractMin here. So, let's say we got C
out. So, out current neighbour is C, our current value
is C and we will say C is seven. So, now we will explore
neighbours of C. So, first neighbour of C is B and B
is not here, so we will ignore B. Another neighbour of C
is D and the value to reach D via C will be whatever it
takes to reach C which is seven plus the weight of this
edge which is ten which is greater than what we have
here. So, there is no reason to reach D via this route,
so, we will just ignore that value. So, we're done
exploring all the neighbours of C, so finally, we're
going to do an extractMin here and our current will
become D and we will update D with seven and then D
will explore its neighbours, A which is not there,
F which is also not there and C which is also not there.
So, at this point of time, we are done with the
Algorithm because this Data Structure is now empty. So,
this is the distance of every vertex from A. E is at
distance two, F is at distance five, B is at distance
five, C is at distance seven and D is at distance
seven.If you wanted to find the path, like how do i
know which path to take to reach D from A. So, we just
go here and go to D and D is coming from F. So, we go
to F and F is coming from E, then we go to E and E
is coming from A. So, we can clearly take this path to
reach A from D. Another example is if you want to know
how to reach C, so,we go here, C is coming from B, so C
is coming from B and then B is coming from A, so, B is
coming from A, so we take this path to reach C. So the
time complexity for this algorithm is similar to Prim's
Algorithm which is O(Elog(V)). The reason is, we're
going to apply this, the size of this data structure
in the worst case is going to be V and then we're going
to apply decreaseKey, extractMin on this data structure
E times which is the total number of edges and V is the
total number of vertices. So, the time complexity will
be O(ElogV) and the space complexity will be O(E+V)
because we might have to store as many edges or all
vertices in this 3 data structures here. So, in the
next section, let's quickly look at the code
for this algorithm.
Name of the function is shortestPath. It takes in a
graph and sourceVertex from which we want to find
the shortest distance of every other vertex. Then, it
returns a map of vertex and the shortest distance of
that vertex from the source vertex. And then let's
initialize our data structure. The first one is a Heap
+ Map data structure which supports extractMin, add,
decrease and contains operation and then second data
structure is a map which stores the shortest distance
from source vertex to every vertex and then this one is
a parent which is useful for tracing the shortest path
from source vertex to every vertex. First thing we do
is, we initialize all the vertices with infinite value
as we discussed before in the video. Then, we are going
to decrease the value of source vertex to zero because
source vertex is at distance zero from itself and then
put this value in the distance map and also set the
parent of source vertex as null. Now, we're going to
iterate through minHeap until minHeap has elements
in it. So, then we're going to extract minimum value
vertex from this minHeap and it returns me a Heap
node which has both actual vertex and distance of
that vertex from the source vertex. Once we get this
heap node, we get the current vertex from it and also
update the distance map, in distance map, we put this
current vertex and the distance of this vertex from the
source vertex. Then, we're going to iterate through all
its neighbours one by one, so, we get its adjacent
vertex, then what we're gonna check is if the adjacent
vertex is present in the minHeap or not. If the adjacent
vertex is not present in minHeap, it means that we've
already found out the shortest distance of that adjacent
vertex from the source vertex. So, there is nothing to
do for this adjacent vertex. So, we just continue and go
back to the top of the for loop and look for the next
adjacent vertex. If the minHeap does contain the
adjacent vertex, so we come to this point and try to
find the distance of that adjacent vertex from the
source vertex while going through this current vertex.
So, distance.get(current)is going to give me minimum
distance of the current vertex from the source vertex
and edge.getWeight() is going to give me the distance
between the current vertex and the adjacent vertex.
So, this new distance is the total distance from source
vertex to the adjacent vertex, going through this
current vertex. What we're going to do is check if
the value stored in the minHeap, the distance stored
in the minHeap for the adjacent vertex, if it is greater
than this newDistance, then we're going to decrease
the value in the minHeap to this new distance and also
update the parent of the adjacent vertex to current
vertex and then, we're going to look at the next
adjacent vertex and once we are done exploring all
neighbours, we just go back to top of this while loop
and do extractMin and repeat the same process for
another vertex and we keep doing this till the Heap is
not empty and in the end, all we do is return this
distance which has distance of every vertex from the
source vertex. So, this is all i have to talk about
Dijkstra's Algorithm. The link to this code is in the
description secton of the video. Please like this
video, share this video, comment on this video,
check out my fb page, facebook.com/tusharroy25
and checkout my github link
github.com/mission-peace/interview/wiki
Thanks again for watching this video.
Dijkstra's shortest path algorithm was
invented by the late great Edsger
Dijkstra a famous Dutch computer
scientist the objective of the algorithm
is to find the shortest path between any
two vertices in a graph in fact
Dijkstra's algorithm will find the
shortest path from a given starting
vertex to every other vertex in the
graph consider this simple graph our
objective is to find the shortest path
from a to every other vertex once it is
run Dijkstra's algorithm will generate
this information which includes
everything we need to know we have the
shortest total distance from the
starting vertex to all of the other
vertices the total shortest distance
from A to B is 3 from A to C it's 7 from
A to D 1 and from A to E it's 2 we also
have the shortest sequence of vertices
from a to every other vertex in other
words the shortest path for example to
get from A to C notice that we arrived
at C via E this is shown in the previous
vertex column when we examine the
information for e we can see that we
arrived at E via D and when we examine
the information for D we can see that we
arrived at D via a so the previous
vertex column actually gives us the
shortest path from a to every other
vertex so how does Dijkstra's algorithm
go about generating this information
Dijkstra's shortest path algorithm works
as follows we're using two lists one to
keep track of the vertices that we
visited and another to keep track of the
vertices that we haven't visited yet so
let's consider the starting vertex a the
distance to a from a is zero that seems
rather obvious but you'll see that
becomes important a bit later the
distances to all other vertices from a
are unknown so for the purposes of the
algorithm we're going to set them to a
very high value let's say infinity we
can include this information in our
table straight away now the algorithm
begins good and proper
we'll start by visiting the unvisited
vertex with the smallest known distance
from the start vertex first time around
this is the start vertex itself it's a
for the current vertex we then examine
its unvisited neighbors well we're
currently visiting a and its unvisited
neighbors are B and D these are the
vertices that a shares edges with for
the current vertex we calculate the
distance of each neighbor from the start
vertex this may seem like overstating
the obvious at the moment but it'll
become apparent why we state it like
this a little bit later for now the
distance from A to B is nothing plus 6
which is 6 and from A to D is nothing
plus 1 which is 1 if the calculated
distance of a vertex is less than the
known distance we can update the
shortest distance in our table well at
the moment all of our shortest distances
are infinity so we can update these two
distances
now we update the previous vertex for
each of the updated distances in this
case we visited B and D via a so we'll
write this information into the previous
vertex column now add the current vertex
to the list of visited vertices we won't
be visiting a again now the algorithm
begins to repeat visit the unvisited
vertex with the smallest known distance
from the start vertex this time around
we can see in the table that its vertex
D
for the current vertex examine its
unvisited neighbors we're currently
visiting D and its unvisited neighbors
are B and E for the current vertex
calculate the distance of each neighbor
from the start vertex the distance of B
from a is the one which we've already
written into the table the distance from
A to D plus another two giving us a
total distance of three the distance
from A to E is the distance from A to D
which we've already calculated and
written into the table one plus another
one from D to e giving us a total
distance of two if the calculated
distance of a vertex is less than the
known distance update the shortest
distance the shortest known distance
from A to B as written in the table is
six we've just calculated a new shortest
distance of three the shortest known
distance from A to E as per the table is
infinity and again we've calculated a
much shorter distance from A to E so we
can write these values into the table
replacing the previous values we found
some shorter paths now we update the
previous vertex for each of the updated
distances in this case we visited B and
D via D so we write this information
into the table add the current vertex to
the list of visited vertices we won't be
visiting D again and once again visit
the unvisited vertex with the smallest
known distance from the start vertex
this time around the information in the
table tells us that it's vertex E for
the current vertex examine its unvisited
neighbors we're currently visiting E and
its unvisited neighbors are b and c for
the current vertex calculate the
distance of each neighbor from this
start vertex and again using the
information in the table we can see that
the total distance to B is 2 from the
table the distance to e plus another 2
giving us 4 and we can see that the
total distance to see from a is 2 the
distance to e from the table plus
another 5 giving us a total of 7 if the
calculated distance of a vertex is less
than the known distance update the
shortest distance well we don't need to
update the distance of B this time we've
calculated for but our table indicates
that we've already got a shorter path so
we'll leave that alone on the other hand
the total distance to see that we've
just calculated is 7 in the table it's
currently infinity so obviously we're
going to overwrite that and since we've
updated the value for C we're going to
update the previous vertex per C in this
case we visited C by re we add the
current vertex to the list of visited
vertices we won't be visiting a again
and as before we visit the unvisited
vertex with the smallest known distance
from the start vertex this time around
it's B for the current vertex examine
its unvisited neighbors while we're
currently visiting B and it's only
unvisited neighbor is C for the current
vertex calculate the distance of each
neighbor from the start vertex so
performing the same calculation again we
find that our total distance from A to C
is 8 if the calculated distance of a
vertex is less than the known distance
update the shortest distance well the
value in the table for C is currently 7
so we don't need to do this update the
previous vertex for each of the updated
distances no distances were updated so
we don't need to do this either add the
current vertex to the list of visited
vertices B won't be visited again visit
the unvisited vertex with the smallest
known distance from the starting vertex
this time around it's C and for the
current vertex examine its unvisited
neighbors well we're currently visiting
C but it doesn't have any unvisited
neighbors there's nothing to do but to
add the current vertex to the list of
visited vertices there are no more
vertices to visit so our table of
information is complete let's just put
those steps together into an algorithm
you can see all I've done is rewrite
those steps inside a repeat until loop
it's worth pointing out that Dijkstra's
shortest path algorithm is an example of
a greedy algorithm and that's because of
this step in which we're selecting the
next vertex to visit the algorithm
chooses the unvisited vertex with the
smallest known distance from the start
vertex the truth is we could select the
next unvisited vertex using pretty much
any criteria we like but the assumption
here is that if we select the closest
one to the start every time we will get
to the end more quickly to define a
greedy algorithm more succinctly we say
that it makes locally optimal choices at
each stage in the hope of finding the
global optimum with some graphs this
greedy approach is desirable
particularly if we want to find the
shortest paths from a starting vertex to
all of the others but what if we simply
wanted to find a shortest path from A to
E in this graph the short-sighted greedy
approach would result in unnecessary
processing here I've just refined it
slightly but it's essentially the same
information a little bit closer to
something that we can implement
hello students it is already midnight on
the night before you are calculus exam
and I figure in that promised you that I
will make this video to show you how to
calculate the shortest path using
Dijkstra algorithm so this is my first
time recording this kind of video so it
will be a little bit awkward for me to
talk to myself and explaining things on
this screen so let's start we use
Dijkstra algorithm to determine the
shortest path starting from a and ending
at Z each step will follow the
pseudocode as outlined in the lecture so
I will start that by first changing the
color here to red okay and the first
step of Dexter algorithms is to set all
the cost of different of each vertex
that are not the origin to infinity so
now the cost of b c d e and z are
initially set to infinity and the cost
of a is initially set to zero then
iteratively which means we use the loops
for this it's relatively we will select
one vertex that has the lowest cost and
include that vertex into a set of paths
and then after the vertex is included we
will update all the other vertices the
cause of all the other vertices so that
it reflect the choice the choice that
you make so for the six vertices you can
see that a has the lowest wave has the
lowest cost
sorry so we initially select a to the
set when a is selected to the set we
have to update neighbors of a which are
B and C because they have they have
edges that connect a to that vertices we
if you make the cost of that vertex
lower how much lower is that depending
on or how much higher depending on the
existing constant which we have here
equals 0 the new cost will be 0 +44 B
and plus 2 for C and you can see that if
you add 0 to 4 which is equal to 4 this
4 is less than infinity the current cost
of B so we now set the course of B 2 for
the new value and the cost of C to 0
plus 2 equals 2 then from 5 vertices b c
d e and z c is the lowest so we now
select c into the list when say is
selected to the list we have to update
our tail vertices so that the cost of
all the vertices correspond to the
shorts that we make so when we select c
to the list the path from A to B passing
through C has the lower weight which
which is 2 plus 1 equal 3 comparing to
the current wave current cost which is 4
2 plus 1 equals 3 is less than 4 so we
update this cause of be 2/3 which
corresponding to moving in this
direction and then moving up here
the next one D we have to which is here
2 plus 8 equal 10 so the cost of D + 10
is less than infinity
the current cost so the cost of D here
will be updated to 10 and the last one e
the constabie be updated to 2 plus 10
equals 12 then for for what it says B is
the lowest so we will now select B into
the list this means something like we
traversing from A now to C and C now to
B but please be careful because this is
not traversing actual traversing we just
trying to determine the shortest path
it is possible like in your in-class
exercise that in some cases this path my
stuck at some dead end and we may need
to use to explore all the paths
depending on the current cost in the
network but for now you can see that it
is quite simple in this case because it
flow from the left to the right without
any dead ends for the now we selected B
we selected B into the list we have to
update the cost of de and Z the cost of
D view now be updated to 3 plus 5 equal
M which is less than 10 and the cost of
oh just that because D is the only
neighbor to B now for a and 12 and
infinity we can see that a is less than
the rest so you now selected D into the
list when do you select it to the list
we have to update e and z
because they are both neighbors 2d the
cost of e will now be updated to 8 plus
2 which is 10 and it is less than e the
existing cost of Eve each is 12 so this
is now updated to 10 and cut at Z from
infinity we reduce to 8 / 6 which is 14
from E and Z you cannot jump to see yet
because you can see that the concept
traversing from de to Z is 14 and you
have a better opportunity for now which
is 10 so we select e into the list and
after selecting a e to the list we have
to still updating the cause of Z the
last one from 14 to 10 plus T 10 plus 3
equal 13 and the last one will be
included wishes I noticed the
destination and from this we have the
path like this and the cost is 13
so my talk is on finding case simple
parts and cycles in a graph directed
edge weighted graph and this is joint
work with my student with another wall
okay so finding cape shortest paths from
s to t one can think of three different
variants of this problem and we are
staying with a graph a directed graph
with non-negative edge weights and
you're given a pair of vertices s and T
and the goal is to find K shortest paths
from s to T and we can define three
different versions of this problem and
we are going to concentrate on the last
one so the first version is well find
the K shortest paths from s to T which
means that you find the shortest path
and then K minus one additional paths
that are shortest you know after
removing the parts that were found
before then before the before this part
and the point is that here you are
allowed to have cycles in the path and
if you do this then if you allow this
then this problem is easy and there's an
algorithm by epsilon that works you know
for K paths
it's basically Dijkstra plus K the
second version of the problem is to say
find the K shortest paths but they
should all have distinct edge weights so
if you have multiple shortest paths with
the same weight then they don't count
you just output one path with each shall
wait now this turns out to be really
hard in the sense that it's np-hard even
for K equals two and the problem that I
will be looking at is the third one
which is intermediate and complexity and
this is a well studied problem I find
case simple paths from s to T so you
don't allow cycles and in fact the paper
by UPS trained clips that this version
the version I'm going to look at is in
fact more common in practice than the
one that he looked at so even though
this is not that well-known in the
theory in literature it's actually quite
a common problem and for this problem
there's an
than a classical algorithm by yen that
runs in k times o tilde of K times M n
time again we'll always be using M as
the number of edges and n as the number
of vertices and there's been have been
small improvements since then but it
remains an autoloader MN and and there's
a reason for that because Williams and
Williams it belongs to the class that is
hard for a PSP on the sub-q bukh
algorithms and so they established that
if the graph is dense so M equals N
squared yeah but but if you just run the
condition is that you fix SN T and you
want a sequence of K parts that are
shortest from s to T so of course they
may be they may be multiple but then
once if there's only one shortest path
from s to T then you have to generate
the next path which may have lengths
that's longer than the shortest path so
you just keep doing that you know making
overlap oh yeah they can oh they just
have to be they can't you can't output
two identical parts yes okay and then
delete every find the shortest path in
the remaining graph there's certainly
one algorithm to do it and in fact okay
so in the that's basically being the
technique in the past yes that the
problem that you are referring to so the
problem that Russell was mentioning is
well the way you would find the second
shortest path is to find the shortest
path then find a shortest path for each
edge on the path remove the edge and
find a shortest path in the graph and
then take the path of minimum weight
among these spots that by the way this
problem is called the replacement parts
problem and be talking about this like
midway through my talk but thank you for
pointing that out okay and in fact all
of the previous methods in fact work
that way and by the way I'm looking at
the all pairs version of this problem
you know I'm going to present a
different method of finding it so that's
in fact the algorithm maker so is that
clear now so that's yeah so and that
turns out to be a lot more challenging
than just finding for the case of
shortest paths without requiring them to
be simple you can essentially do
something like that star but I mean you
have to be a little bit more Epson has
that yeah the first one will or you know
you can always get the first path to be
simple but successive shortest paths
you're requiring them to be simple I
mean okay so let me take good so you
have s and T so this is the shortest
path and it may turn out that the the
shortest way to get to s to eat a second
time is to take two edges in here then
go back to s and then come in this way
this is shorter than any other path that
goes to T without having a cycle okay so
the first version of the problem the
easy version of the problem doesn't care
of cycles are performed so it it could
generate paths like this by free meaning
that each successive path the only
requirement for your kate part is that
it be shortest such that it doesn't
exactly coincide with any any of the
paths generated earlier depending on the
edge weights you know so it may turn out
that all edge all paths
but you have no control over the weights
its you're just given an arbitrary graph
and here it is a shortest path from s to
T and if you look at my third version of
the problem and consider maybe there's
just one other path that goes from s to
T without actually creating a cycle and
that has a very expensive edge on it
well there are plenty of paths that by
looking at two of them and then the
third one goes like this and then goes
like this and that's a lot more cheaper
than you know going doesn't have to go
back to s it can go to something that's
sorry it doesn't have to go back to s I
am into this because I have an example
later on where I do go back to s but in
general any cycle is not allowed so you
may have another I mean yeah yes so you
could have something like this and then
just goes in like this and then goes
like that and you know but and you know
K can be arbitrary so if you allow paths
like this then you know you can roughly
call it Dijkstra like techniques can
allow you to generate successive paths
very efficiently while if you insist
that each successive path is simple
I mean the best known algorithm is tilde
MN for constant K and Williams and
Williams have even shown that for dense
graphs it's equivalent to a PSB so it is
a hard problem and of course for K
equals one all three problems are asking
for exactly the same thing short to spot
and you know it always is a shortest
path that's simpler so the problem that
we are looking at and they are looking
at a collection of problems related to
finding K simple shortest paths and case
and put shorter cycles in a graph and
the prior work as I said there's been
quite a bit of work on case simple
shortest paths from s to T and I'm going
to be using acronyms K s is B instead of
K because that shortest path to the SP
simple shortest paths KS is B and the N
and then some refinements to you know
within the Hotel de by Buckley for
Loewenstein and Rho DT and Zwick what a
different method and then there is I'm
also going to look at case and push or
the cycles and for shortest cycles the
only prior word that we are aware office
just enumerating case simple cycles in
the overall graph and in no particular
order you're not asking for non
decreasing order of weight or length and
this is again a classical problem just
like the case is P in the 70s there was
a lot of work in this problem and the
first polynomial time algorithm was
given by Carson in 73 and that was then
improved to linear time by Johnson in 75
so kind of solved but only in no
particular order so the variants that we
study are the following so okay we
consider the Esaias p problem for all
pairs and I should mention that well
I'll talk in the next slide maybe we
don't have a full answer but we do have
some interesting improvements and MIT
method is interesting it's quite
different from prior methods and then we
go to the cycles version and it consider
both variants
find the case simple shortest cycles to
expect through a specified vertex that
corresponds to KS is P with just a
simple shortest path from a given vertex
to a given vertex and the overall graph
version is what we call K all vertices
simple cycles so for every vertex in the
graph find the K shortest simple cycles
ok so we consider those two versions and
then corresponding to enumeration we
have two more problems we study it is
enumerate the case simple shortest
cycles
K all simple cycles and a case simple
shortest paths K all simple paths okay
and so what are our main contributions
of course we did a systematic study of
all of these different variants but we
also found new algorithms to address
this this this property four paths
through
technique we call path extensions and
with this technique we saw two ApS is B
that is SSP for all paths in this same
Oh till that time as for a single pair
so for a single pair it was Hotel de kmn
and so it will be Odell de MN for to a
PSI SB and we had the hardness result
from Williams and Williams and we match
that same result for the all pairs
version for all theta of N squared
versions and with this new technique
that we call path extensions and the
same technique gives an improved
algorithm for three ApS si speed but
it's a factor of M slower but that's
still better than what's the best known
which is you know basically running the
SIS P algorithm and squared times so
that's like mno2 diamine cubed
unfortunately our current algorithm
doesn't scale to larger k and that's an
interesting open question then we use
path extensions in a different way to
come up with a very efficient algorithm
for enumeration of simple shortest paths
so we solve K all s is P that is
generating simple paths in the graph in
non decreasing order of weight where we
spend all em time for the first path and
that's inevitable because the first the
shortest path in the graph is just an
edge of minimum weight and clearly you
need to spend em time to find the edge
of minimum weight but thereafter we just
spend or tilde of J comma n time for the
J spot so essentially for the in if K is
a constant then each additional path is
found in off log n time so it's very
efficient and it uses path extensions
but a different path extension method
from what we used in for the first
problem so those are our algorithmic up
and those are ya know non-trivial
algorithms and then we give several
reductions to obtain Hotel de MN time
algorithms for the remaining problem so
we consider six problems of which for
one AK s is B KS is P there are already
algorithms at IO tilde and there were
two more that were addressed
algorithms in here so that leaves three
other problems the cycles problem for
all three versions and we give for que
si si so simple shortest cycles and K
all generating all simple cycles in non
decreasing order of weight we give o
tilde MN time so this problem is where
there was prior work for the case when
you didn't care about the ordering you
just wanted to generate simple cycles
that you could do in linear time but
here we spend o tilde MN time but we
still have a reasonably efficient
algorithm and similar to all pair simple
shortest paths for all all vertices
simple cycles for two abs is a we have
an efficient algorithm it degrades
slightly not as bad as what we have up
here this becomes super MN for K greater
than two and finally he relevant to this
workshop we show hardness results and
our hardness results because they're
mainly interested in sparse graphs our
hardness results are with respect to
finding a minimum weight cycle in a
sparse graph now in other hard under the
for dense graphs this is known to be
equivalent to a PSP so n cubed but for
sparse graphs we can solve it in O of MN
time and you know it's a very well-known
problem I mean people have certainly
been trying to get better than all of MN
for it so we showed reductions from
minimum weight cycle for all our
problems except of course for K all s
ISP for which we actually have a very
efficient algorithm and we the
reductions we use we call them MN
reductions because you know it's going
to preserve the number of vertices and
the number of edges in the graph okay so
the hardness class just to reiterate so
the a PSP hardness class is very you
know well known and it's like a major
theme of this workshop by Williams and
Williams but the problem is that if you
are mainly interested in sparse graphs
you know which is often the case it does
not distinguish between dense and sparse
graphs so we just consider this
that preserves path sparsity starting
with minlid cycle and so let's call that
sparse m/min wait cycle hardness with
respect to sub m n algorithm so if you
could yeah if you could get a sub m m n
algorithm for any of our problems you
would get sub m and for been week cycle
which has been open for a long time and
I should also mention that you know
there are other ways of approaching n
cubed from sparsity too dense to dense
graphs and for example for min weight
triangle there's a very nice algorithm I
should have mentioned by it I and Rohde
which does it in oh of M to the three
halves time so that again becomes n
cubed for dense graphs and it's faster
than MN but it's not quite as big a
class MN there are lots of problems that
have or tilde graph problems that have
four tilde MN running care for sparse
graphs so anyway so that's not class
that's relevant for our problems MN is
the one that's relevant for our problems
okay this is the maybe I'll just hit the
highlights here so this is all of our
results and so four cycles at least four
cycles in non decreasing order of weight
there's nothing known before except as I
said for K all SISC if you didn't care
about the ordering there was there's a
very efficient algorithm linear time per
cycle so our new algorithmic result
which is which I think it's you know
kind of the most non-trivial part of our
contribution actually for just the case
of two ApS is P you can actually use
something called distance sensitivity
oracles that I will define a little
later in the talk and I'll use DSO for
short for that and using them you can do
it in or encode tilde and cube so it's
fine for dense graphs but again we want
efficiency for sparse graphs and we get
o tilde MN and with the new method for
this and building on bootstrapping on
that method we get om square em N
squared for three ApS ISP and that's
again a factor of an improvement over
previous results and there is a lower
bound here trivially both of these
problems are at least as hard as a PSP
because they include
the output includes the a PSP solution
so I didn't put that in there and then
when it comes to 2x is P so this is the
lower bound that Williams and Williams
scale for to SSB for under n cube
reductions and they use the reduction
from in weight rank triangles but we can
also that and that reduction in fact
works for sparse graphs so we do have
that mean way triangle reduces to s is P
but as I pointed out min weight triangle
actually has an M to the three-halves
algorithm so we give a different
reduction from min weight cycles to to
sis B and KS is P is the same and so
these are all new results so be sure
that KS is P is equivalent to KS is C so
all the results for KS is a translate to
ksi speedboat lower bounds and
algorithms and all of the remaining
problems except of course the last rule
which is for that efficient result they
run in ho tilde MN time for here again
this is the all vertices version so it's
a little analogous to a PS is P so the
two for K equals two it's a little
faster its tilde MN but then you go K
greater than two right now you can only
get km and square but we can stay at km
n squared we don't have to grow further
yeah and then for all s I see we have I
guess tight bounds relative to a
hardness assumption so we show that it
reduces to two all SISC and we can in
fact generate each successive path in
hotel diamond time ok so I won't talk I
mean these algorithms I'll talk about
some of the reductions for the
algorithms in here I mean they did a
quite a little bit of work but they are
not you know they're fairly
straightforward so I will concentrate on
some of our reductions and then talk
about our up about that algorithm with
the novel technique for abs is be okay
so amend reductions I mean these are
fairly strong reductions and they work
for our class of problems we could have
defined weaker reductions and still got
the desired result that you know in
provements below MN would imply
improvements for drum Minh weight cycle
but we just considered reductions where
we say if you have to graph graph
problems P and Q and MN reduction from P
to Q means that given an input to P with
ember to see M edges and n vertices we
can transform in linear time n plus 1
time this input into a graph G prime
which if we solve with an algorithm for
Q will in turn return a solution for P
in o of n plus n time so a very strong
reduction we can work with this
definition for all of our reductions and
our main reductions and the ones that I
will show you is min wait cycle reduces
to 2 s is P of course this is trivial
from 2 to K and also that KS is B is
equivalent to KS is C and and of course
the remaining hardness results are
pretty much trivial because it turns out
the left turns out to be a special case
of the right and also there are some
prior known results for instance I think
it's folklore that many wait cycle
reduces to a PSB and it was shown by got
leaf and loewenstein that to s is p
reduces to a PS p plus o of n square
processing so there is this reduction
and in fact you can yeah and then also
the the Tian's make sure that KS is p
reduces to k calls to to s is B we won't
be using any of things but just to point
out reductions that are known okay so
the showing the equivalence is very
simple so we'll just do it in both
directions so KS is P reduces to KS is C
we just create add a new vertex u prime
to the graph and put 0 weight edges from
u prime to s and T to u Prime and then
we see that the Kate's simple shortest
cycle in this updated graph G Prime
through u prime is precisely the kid
simple SD path from s to
and the reverse ksi SC reduces KS ISP it
was slightly different reduction they
have to go from cycle to path so the the
goal is to compute K simple shorter
cycles through V given vertex V M G so
we split V into VI and vo so all
incoming edges to we are made in coming
to VI all outgoing edges from we become
outgoing from vo
and then we can observe that a simple
cycle through B in G is transformed into
a simple path from vo to VI in G Prime
with the same same weight so we have the
reverse reduction KS I see reduces to KS
is B so the two problems are equivalent
okay so for min weight cycle reduces to
2 s is B we use this generic
transformation where to convert from g
to g prime where for every vertex v we
split it into VI and vo as we did for
the SI c reduction but we actually put
an edge from VI to vo of way to 0 and we
and then all incoming edges come become
incoming to vi and all outgoing edges
from we become outgoing from vo and we
do this for every vertex so the incoming
edges the other end point is coming out
of the over text for that but except for
see it's an Co and the outgoing edges go
to the i but X a sub I so we do this and
this is the graph G prime that we will
find you can see that because they put
weight 0 a simple cycle through vertex
any given vertex B and G is transformed
to a simple path from V o to VI in G
Prime with the same weight ok now we
need to add a little more few more
gadgets in order to complete the
transformation and I'll illustrate that
with an example here so this is a
reduction so in blue is a graph G Prime
and I've drawn it for 3 vertices so N
equals 3 here so you have and they're
numbered what vertices are numbered 1 2
you have the edge zero weight edge from
the eye vertex to the overt X then we
have a part and that's the part in here
on n plus 1 vertices all with zero
weight edges so that's p 0 to P 3
because N equals 3 and now we are going
to connect up this path to the vertices
in G prime so the so if we consider the
jate vertex in here J i2j oh there will
be an incoming edge to J Oh
from P I minus 1 so this is 2 O so it
comes from P 1 and there will be an
outgoing edge from J I to P I so those
are the green edges and now we assign
weights and they're going to assign
fairly large weights first let little W
be the maximum edge weight and G and
they're going to set capital W to be n
times little W and the purpose point of
this is to observe about capital W is
that W capital W is greater than the
weight of any simple shortest path in G
prime because the maximum weight on an
edge is little W and any simple path
contains at most n minus 1 edges
okay so then we observed that so what
would a the shortest path from s there
from so we are going to consider the
second simple shortest path problem from
p 0 to P 3 to s is P from p 0 to P 3 the
shortest path is of course the black
path because of weight 0 and any other
path is going to have to take a red edge
and a green edge because those are the
only edges going out of the part and
they are going to have very large weight
so the second shortest path will take
one of these edges out and then traverse
through the graph to get to the I vertex
some a vertex and then come back to the
path and then continue to the end and we
make another simple observation which is
that it would never be you never the
second simple shortest path will never
if it starts out of PJ it would never go
to a vertex that PJ plus 2 or greater or
and of course clearly it wouldn't go
back to a path vertex before PJ because
that would create a sight
that is the part we'll have to start
from PJ - one that's all we have it here
and then take a path through the graph
and then come back to PJ and that's
because if you sum up the weights of the
corresponding red edge and the green
edge so it's n minus J plus 1 W if you
went directly to the next vertex 2 + JW
so that gives you n plus 1 times W if
you went to the next took the green path
green edge to the vertex next to the
vertex from which you started out and we
went to any vertex green vertex green
edge vertex green edge to a vertex
beyond the next vertex you will add at
least one more capital W and that would
never be useful unless the graph has no
cycles at all or something because any
path through the graph has weight less
than W so the second simple shortest
path starts from some pj minus 1 goes in
here takes a path to the corresponding
ji and comes back and of course for it
to be shortest it has to take the
shortest simple path in G from that Jo
to that J I and that would be the
shortest simple second shortest simple
side the shutter simple cycle in the
original graph G which is what we needed
to show okay so and this is a linear
size reduction and doable in linear time
I mean oh this graph has all of M edges
and of n vertices and and so this is a
remand reduction okay so those are the
only two reductions I wanted to show I
just wanted to conclude this part of the
talk to just point out that you know o
tilde MN just for the pro type of
problems we've been considering it's
quite rich and also there are lots of
books there are lots of refinements
within this class so if you look at
unweighted we have Oh exactly MN form in
weight cycle and unweighted a PSP both
undirected and directed next you come to
weighted and then for the undirected
case we have an almost MN time algorithm
MN log alpha MN for undirected minimum
edge cycle and undirected weighted a TSP
now if you come to directed then
have many of the problems we considered
here and also directed a PSP with MN
plus N squared log log n and then the
next class is MN plus N squared log n
which is our algorithm for to a PS is P
for to AV of several of our algorithms
here have this type of bound and then
they so which I'm going to define very
soon this is the distance sensitivity
oracles they have an additional log
factor higher but they're still Hotel de
MN now the remaining problems we
considered hair still polynomial time
but a larger larger then little Omega of
M and till Otel diamond little Omega
tilde okay so so the rest of the talk
will cover the algorithm which as I said
use this path extensions that may not be
detours I will explain that why that's
interesting and talk and talk about the
our algorithmic contributions okay so
detour sir okay what is known as I said
previously the only problem that has
been studied this case is P and has been
studied a fair amount and all of the
known algorithms for KS is P say and now
we are considering KS is P from X to Y
compute D tools around each edge in the
graph so if you know it what is the
shortest but this is referring to what
Russell asked what is the shortest path
in the graph from X to Y if I remove a
particular edge along the path and you
find that power path for each edge along
the path and then take the minimum of
minimum weight edge among this
collection as the answer for 2's is P
and this can be actually extended for
any K and in fact what they are
computing in the process is what's
called replacement parts and that's
again a problem that's been very well
studied and it's what's only for K
equals two and here you ask you want to
have a table so that you can quickly
answer queries of the form what is the
shortest simple path from X to Y if a
particular edge of the path fails so you
have a
you know a table that has such an answer
for each edge and in along the spot and
all algorithms currently for 2's is P or
KS is be essentially compute replacement
parts and all replacement parts can
compute what's called a detour from the
shortest path and a detour is just you
take your shortest path and then you're
only allowed to create a new path by
exiting the current path and of course
you put exit right at X find a shortest
they find a path that avoids the path
this shutter spot and comes back at some
further point like B 3 here and then
continues along the path up to Y all
known algorithms for either KS ISP or
replacement parts have used this
strategy and in fact so much so that
this in fact even a lower bound
hershberger toy gave a lower bound m
square root of n lower bound for both
2's ISP and replacement parts in this
path weight comparison model assuming
that the algorithm only examines these
Lito so you know why would you need to
examine anything else you can show that
the second every year every replacement
path has to have this form so assuming
that this is the case then this is a
lower bound now turns out that our
algorithm for 2's ApS is P and engender
for K doesn't good generate and examine
paths that are not d2 so it actually
falls outside the framework of this
lower bound and any of these previous
methods to solve that as is B okay so
one more slide on replacement parts I
said replacement parts are very well
studied and closely related to K 2 s is
be you know pretty much the same for
known algorithms and send a cost is you
would take an algorithm for replacement
parts and spend linear time a time
proportional to the length of the path
to select the minimum weight replacement
parts now you can ask about all pairs
replacement parts now this has been
studied before bite a bit even though
all pairs K simple shortest paths hasn't
been as far as we know hasn't been
studied before now the problem here is
that if we try to do the same strategy
as up
here we notice that the output would
potentially have size and cubed because
for every pair XY we have to have an
output for every edge on the shortest
path the replacement path for it so
that's pretty large so what has been
done for replacement parts for the old
paths case is to instead construct
what's called distance sensitivity
oracles and this is the DSO a compact
table of size o tilde N squared that
from which any specific you could query
that table and in constant time and what
they say you know what about the
starting from X ending to Y but avoiding
this particular edge a B then this in
constant time you can query the table
and get the answer to this so this was
the first deer so was constructed by
Dmitri scary at all but the current best
bound for DSO is again o tilde MN but it
has an additional logarithmic factor
over the other problems but still it
runs pretty fast unfortunately to get
now ap to ApS si SP from this deer so
takes n cubed time for the same reason
that we have this n cubed because we
have to find the minimum path among all
of the possible replacement paths I mean
for it for all pairs so so for dense
graphs this is still good but for sparse
graphs and cubed is too much so this is
where our method comes in so what we do
is we have a two-phase approach we first
compute what we were collection of sets
called nearly sis BK k nearly Esaias
business now what we want to find out
the exact K is B sets for every pair XY
we define this quantity K nearly s is B
sets and then we give a general
algorithm compute a P a very efficient
algorithm that if you have that nearly
sis B sets you can actually compute the
exact answer pretty efficiently in KN
squared plus N squared log in time ok so
I will go ahead I'll define the qk sets
first for q2 and then then for
the general candidates yeah so here's
the definition of the set given a pair
of vertices X Y Q 2 X Y contains a
shortest path from X to Y and then the
second part in Q 2 is going to be the
shortest path from X to Y or shortest
path from X to Y that avoids the first
edge xB okay so for example in this
problem in this graph let's say the two
the exact output we want so assume all
edges awaited 1 so we have two paths
shortest paths both with three edges so
the exact answer you want are the paths
X BA D Y and X B CY however Q 2 is
asking for well you take one of these
two paths let's say it takes X B D Y the
second path has to avoid the edge X B so
it will have it as a second part xfg dy
okay now if it happened that the two
shortest paths already started with two
different edges the Q 2 will be the same
as the exact output we want okay
so the point about this the Q 2 sets is
that they can be computed pretty
efficiently with DSO because each second
path for a given pair X Y the second
part can be obtained with a query to DSO
saying give me the shortest path from X
to Y that avoids this edge on the
shortest path first stage on the
shortest path so now we can in fact the
the previous bottleneck that we had that
it took and cube to just find query the
DSO no longer holds at least the Q 2
sets can be found in awe of N squared
time plus the time for DSO and and in
fact there are even faster ways of
computing this which I won't go into to
compute the Q 2 sets for K equals 2 but
this is good enough DSO is good enough
so all that remains is to come so we can
compute the first path in the pew two
sets with a single a PSP algorithm and
the second path using their source
constant number of queries for each pair
and all that remains is to compute to a
PSP from the Q two sets and for which I
will show you this algorithm
that does it with path extensions and
more generally the Q key sets
generalizes the q2 sets and let me just
introduce this notation so we will use P
star K X Y as the actual output we want
the case simple shortest paths from X to
Y and G and QK is just going to be if
the K minus one shortest paths in this
set all start with the same edge then
the the next path would be the shortest
path from X to Y that avoids that common
edge so there's the edge XA if it
happens that the K minus one shortest
paths from X to Y all share the same
first edge then you'll kick cable not it
will always have the next path the Cate
path as a path that avoids the edge XA
so it's a natural generalization of the
queue to case maybe in suggesting the
shortest path and the second shortest
path look at the first K minus part one
paths and only if they all share the
same first edge would qk x y potentially
be different because it has to use a
different first stage and so now with
this if suppose we have somehow computed
the qk x Y sets and we know we can do it
for q2 the task for algorithm compute
the ApS ISP is very very very you know
specific it says you already have a Q K
X Y you just examine it and check are
the K minus 1 shortest paths in it all
starting with the same hedge XA and if
it is then somehow determine if the kid
simple shorter spot also starts with a
jacksie because if not QK already has it
it has the shortest edge outside of the
edges that start with the edge XA so
that looks manageable and in fact we
show that it is manageable and for any K
I mean here this works for any K very
efficiently in K N squared plus n square
n log n time so this is again the same
observation and the way we find this
gate path when needed is with this
following fairly simple observation
which is that if all paths and P XY
start with the same first edge XA
that's the case that we have to kind of
address then if you know go so this was
a common first stage then they all end
at vertex a so if I now go to P star K
of a why so there are K short simple
shorter spots from a to why it must
consist of the right sub parts of the
paths in P star XY that is removing edge
ax a so let me just explain that so what
this is saying is suppose they have the
case let's say K is equal to 3 and I
consider the the 3 shortest paths from
simple shortest paths from X to Y and
they all start with the same first stage
X a so they look like this dilemma is
saying then it must be the case that in
P star a Y well at least the widths of
the K 3 shortest paths from air 8 y must
be equal to the widths in here and I
mean they are really this and I'm just
saying it that way in case you had
multiple shortest paths with the same
weight and then you chose one and you
know this is showing something else so
this may look trivial and maybe this
addresses some questions that were asked
in the beginning but and it's true of
course for this lemma but just observe
that need not be true in general because
if I look at the three shortest paths on
a to Y it might be the case that say
this red path actually used X in order
to go to Y that was the best way of the
cheapest way to go to Y was to go
through X then that's no longer then
it's no longer the case that this lemma
holds okay so in general this would
happen and this is really what
complicates the algorithms for simple
shortest paths are supposed to just the
version where cycles are allowed but of
course we can see that in the case of
this lemma the premise of this lemma
this cannot happen because if this
happened then it means that this portion
of the path from X to Y it cannot use
edge XA because that will create a cycle
it uses some other edge going out of a X
and this portion of the path is a path
from X to Y and it has wait no more than
this red path and and you know so for
the purposes of our algorithm
it would already be in the qk set okay
so either it's already there and the
clock is set because you know it was in
fact a path that went differently or it
has this very structured property that
all cap shorter spots use X as the first
edge and in fact the pH of the spot is
just an extension by edge XA of the case
shorter Spartan P star a Y and that's
where the path extinction method comes
omote three minutes okay
they started late right yeah okay yeah
so anyway so and once we have this
observation but you still have to make
sure that the algorithm is correct there
are some technical details here but we
just create you know you can examine the
cue cue sets to it because you know it
takes us input the QK sets by examining
the Q key sets you can see which are the
problem pairs X Y and because the
problem pairs X Y or those for which the
K minus one shortest paths all start
with the same edge and for those you
know you potentially may need to extend
the tape shortest paths from NP star a Y
so you create that path and let's forget
about extensions you create that path
and what we do is we put it on a
priority queue because it may not be
correct at this point because even P
star a Y might be incorrect it might be
of the same form and you may need update
it but it turns out so we pre extend to
X and B place it in a priority queue
here's the entire algorithm it's a very
simple algorithm in the sense it just
uses a priority queue and you don't even
need a Dijkstra you don't even need a
Fibonacci heap a binary heaps of
surfaces and we can show that if you
just put these parts in there and do
this structured type of you know
extraction examination and deciding what
to do with the path that it will
correctly create the sets P star correct
sets P star and so if it's running time
is easily seen to be K and squared plus
N squared log N and so that gives or
tilde
MN for DSO and actually we get a
slightly faster algorithm we don't need
the full power of the air so there's a
very simple algorithm
give us this mn+ N squared log N and for
K equals three and in general for
general K the best method that we could
come up with was to recursively use the
K minus 1 ApS is P algorithm to compute
the QK sets and the problem with that as
we had to do that on all n vertices so
the running time increases by a factor
of n with each increase in K so it
becomes worthless after cake what's for
but at K equals 3 this already gives us
far faster by a linear buy at 8 of an
open factor over the best previous
algorithm and yeah so and okay so just
one comment so this is the second
algorithm that uses spot extensions this
is for finding that K enumerate in the
case shortest paths simple shortest
paths in the graph and you can see just
in here that it again that's part
extensions but it is completely it's a
different type of path succession a
different criterion for doing the path
extension and for example one difference
in terms of the invariance of the two
algorithms maintained is that in this
algorithm we guarantee that when we do
an extension and add it to the priority
queue again it has a priority queue it's
guaranteed to be simple in the previous
algorithm we could actually create add
to the priority queue apart with the
cycle it's only when we extract it that
we you know the criterion that we Esteve
we apply to whether or not we include it
guarantees that we would never add a
path with cycle okay so as I said it's
very efficient and somebody okay so Mary
so a may encounter the most novel
contribution here is that we've come up
with a new methodology for this
classical problem of finding multiple
simple shortest paths where all previous
methods use detours and now we are using
this path extension and very annoyingly
our current method I mean it because a
dramatic improvement for two ApS is P
basically gets the o tilde a man bound
it's also harmless or tilde MN and
pretty good for three it'll be great if
we could generalize it and perhaps
there's a different way of doing it that
would give us the QK sets
and we gave reductions and we you know
created the spasming wait cycle which
hopefully this community thinks is a
good hardness class and again and we
have a different very fast algorithm
okay or the SI SP and algorithmic e the
questions I think this is the question I
mean being used a recursive method to do
it now the QK sets are easier then
because it gives that it relaxes what we
need for the P star K sets so you're
computing something not as difficult as
the P star K sets maybe there is some
other way of constructing them that will
give us a better algorithm now space
usage is high in all our oil paste
algorithms so it will be nice if we can
obtain space efficient versions and
finally the hardness dispersement weight
cycle now in contrast to the n cubed sub
cubic hardness results where most of the
problems were reducible to one another
we here we are more like the I guess the
threesome situation we have the spasming
weight cycle and many other problems you
know which reduces to many other
problems so they are all you know at
least as hard as past minute cycle but
so far we have not closed close the you
know the reduction to give equivalence
so the one equivalence we had was
between KS is P and KS is C but in
general I think this is a very
interesting class to explore first
because in practice glasses drafts tend
to be sparse so it'll be good to get
good results and also it tends to be
more challenging as I think Emmanuel
viola said in gave a talk on the first
day where he talked about triangles
enumerated triangles and he the
challenges that come up when you go from
just the dense case the sparse case you
have to consider you know how the edges
get distributed if you had to solve
recursively so that's not so interesting
I think the questions to be addressed
been trying to you know to find this
class of specially-made cycle okay and
that's my talk
harden trying to show poisonous sort of
okay so well yeah I know but for many of
these problems for any constant K it was
you know Hotel de ml but there were a
few for which it was not the case let me
show you I mean pretty soon now here so
the ones for which we don't have yeah so
of course we have two and three but you
know okay two to three things I think we
don't I can't think of a natural
candidate from which to show this
conditional had cardless what above but
I actually think I mean now that this
hope all of you will work on finding the
two K sets more efficiently it's just
very annoying that right now I think
that perhaps K ApS is P can be brought
down you know I'm not convinced that
there is that lower but but yeah and
also I haven't thought about it and
there is this one and this one is just
it's just n applications of sis P yeah
so to answer your question no I haven't
thought about it and if you were going
to think about it it'll be interesting
to see you have to think of a sort of a
static problem that has resisted you
know sub M N squared or something for a
long time here I think min weight cycle
you know has certainly stood the test of
time and but that's a good question
so you know one-handed yes I think
because of just using distances it every
hour goes your work just implies thanks
for the dance case as well oh yeah all
of my work yeah okay oh four small ways
okay it took two comments here one is
that we have not been considering any
dependence on weights and I see it so
that's interesting because you know with
distance sensitive article with this if
you don't consider the edge weight size
we have a separate method that does it
faster than distance and City but yeah
but if you have a faster method for
distance density or occur for small
weights then that would be preferable to
our algorithm well that's cool yeah yeah
yeah so it's sub Amen oh no no no it's
no for dense graphs it would be n cubed
it so it is sub n cubed okay Oh
interesting
yeah okay thank you
the following content is provided under
a Creative Commons license your support
will help MIT OpenCourseWare continue to
offer high quality educational resources
for free
to make a donation or view additional
materials from hundreds of MIT courses
visit MIT opencourseware at ocw.mit.edu
we start a brand new exciting topic
dynamic programming yeah so exciting
actually I'm really excited because
dynamic programming is my favorite thing
in the world in algorithms and it's
going to be the next four lectures it's
so exciting it has lots of different
facets it's a very general powerful
design technique we don't talk a lot
about algorithm design in this class but
dynamic programming is one that's so
important and also takes a little while
to settle in we we like to inject it
into you now in double-o 6 so in general
our motivation is designing new
algorithms and dynamic programming also
called DP is a great way all right a
very general powerful way to do this
it's especially good and intended for
optimization problems things like
shortest paths you want to find the best
way to do something shortest paths you
want to find the shortest path the
minimum length path you want to minimize
maximize something that's an
optimization problem and typically good
algorithms to solve them involve dynamic
programming it's a bit of a broad
statement you can also think of dynamic
programming as a kind of exhaustive
search which is usually a bad thing to
do because it leads to exponential time
but if you do it in a clever way by
dynamic programming you typically get
polynomial polynomial time so one
perspective is the dynamic programming
is approximately a careful brute force
it's kind of a funny combination
and I'm gonna oxymoron we take the idea
of brute force which is try all
possibilities and you do it carefully
and you get it to polynomial time there
are a lot of problems where essentially
the only known polynomial time algorithm
is via dynamic programming doesn't
always work there's some problems where
we don't think there are polynomial time
algorithms but when it's possible DP is
a nice sort of general approach to it
and we're going to be talking a lot
about dynamic programming has a lot of
different there's a lot of different
ways to think about it we'll look at a
few today we're going to warm up today
with some fairly easy problems that we
already know how to solve namely
computing Fibonacci numbers which is
pretty easy and computing shortest paths
and then in the next three lectures
we're going to get to more interesting
examples where it's pretty surprising
that you can even solve the problem in
polynomial time probably the first
burning question on your mind though is
why is it called dynamic programming
what does that even mean and I used to
have this feel about well you know
programming refers to the I think is the
British notion of the word where it's
about optimization optimization in
American English is something like
programming in British English where you
know you want to set up the program the
schedule for your trains or something I
think is where programming comes from
originally but I looked up the actual
history of why is it called dynamic
programming dynamic programming is
invented by a guy named Richard bellman
you may have heard of bellman being the
bellman-ford algorithm so this is
actually the precursor to bellman-ford
and we're going to see bellman-ford come
up naturally in this setting so here's
here's a quote about him says bellman
explained that he invented the named
dynamic programming to hide the fact
that he was doing mathematical research
happen he was working at this place
called rant and under a secretary of
defense who had a pathological fear and
hatred for the term research uh-huh so
he settled on the term dynamic
programming because it would you it
would be difficult to give a
short of meaning to it and because it
was something not even a congressman
could object to basically it sounded
cool so that's that's the origin of the
name dynamic programming so why is it
called that who knows I mean now you
know but it's not it's a weird term just
take it for what it is it may make sense
some kind of sense pen all right so we
are going to start with this example of
how to compute Fibonacci numbers and
maybe before we actually start I'm going
to give you a a sneak peek what you can
think of dynamic programming s and this
this equation so to speak is going to
change throughout today's lecture at the
end we'll settle on a sort of more
accurate perspective the basic idea of
dynamic programming is to take a problem
split it into subproblems solve those
subproblems and reuse the solutions to
your subproblems it's like a lesson in
recycling okay so we'll see that in
Fibonacci numbers so you remember
Fibonacci numbers right number of
rabbits you have on day n if they
reproduce we've we've mentioned them
before we're talking about AVL trees I
think so this is a usual you can think
of it as a recursive definition or
recurrence on Fibonacci numbers it's a
definition of what the antenna tree
number is so let's suppose our goal an
algorithmic problem is compute the nth
Fibonacci number and I'm going to assume
here that that fits in a word and so
basic arithmetic addition whatever is
constant time per operation so how do we
do it you all know how to do it anyways
but
I'm going to give you the dynamic
programming perspective on things so
this will seem kind of obvious but it is
a except we're going to apply exactly
the same principles that we will apply
over and over in dynamic programming but
here it's in a very familiar setting so
we're going to start with the naive
recursive algorithm and that is if you
want to compute the Fibonacci number you
check whether you're in the base case
I'm going to write it this way so f is
just our return value you'll see why
write it this way in a moment then you
return F in the base case it's 1
otherwise you recursively call Fibonacci
of n minus 1 you recursively call
Fibonacci of n minus 2 add them together
return that this is a correct algorithm
is it a good algorithm no very bad
exponential time how do we know it's
exponential time other than from
experience well we can write the running
time as a recurrence t of n represents
the time to compute the nth Fibonacci
number how can I write the recurrence
good throwback to the early lectures
divide and conquer
here was this yeah
yeah n minus one plus T of n minus two
plus constant audition I don't know how
many you have by now okay so to create
the nth Fibonacci number we have to
compute the N minus first Fibonacci
number and the n minus second Fibonacci
number that's these two recursions and
then we take constant time otherwise we
do constant number of additions
comparisons return all these operations
take constant time so that's a
recurrence how do we solve this
recurrence well one way is to see this
is Fibonacci recurrence so it's the same
thing this is plus whatever but in
particularly this is at least the entry
of adachi number and if you know
fibonacci stuff that's about the golden
ratio to the nth power which is bad we
had a similar recurrence in AVL trees
and so another way to solve it let's
just good review say oh well that's at
least two times T of n minus two it's
going to be monotone the bigger n is the
more work you have to do because for the
n thing you have to do the n minus first
thing so we could just reduce enmity of
n minus 1 to T of n minus 2 that will
give us a lower bound and now these two
terms now this is sort of an easy thing
you see that you're multiplying by two
each time you're subtracting two from
any time how many times can I subtract
two from N and over two times before I
get down to a constant and so this is
equal to 2 to the N over 2 I mean times
some constant which is what you get in
the base case so I guess I should say
theta this thing is theta that okay so
it's at least that big and the right
constant is fee and the base of the
exponent
okay so that's a bad algorithm we all
know it's a bad algorithm but I'm going
to give you a general approach for
making bad algorithms like this good and
that general approach is called
memoization we're good here and this is
a technique of dynamic programming I'm
going to call this the memorized dynamic
programming out so um I settle on using
memo in the notes yeah the idea is
simple whenever we compute a Fibonacci
number we put it in a dictionary if it
and then when we need to compute the nth
Fibonacci number we check is it already
in the dictionary did we already solve
this problem if so return that answer
otherwise compute it okay you'll see the
transformation is very simple
okay these two lines are identical to
these two lines okay so you can see how
the transformation works in general you
can do this with any recursive algorithm
memoization transformation on that
algorithm which is we initially make an
empty dictionary filled memo and before
we actually do the computation we say
well check whether this this version of
the Fibonacci problem computing F of n
is already in our dictionary so if that
key is already in the dictionary we
return the corresponding value in the
dictionary so and then once we've
computed the and Fibonacci number if we
bother to do this if this didn't apply
then we store it in the memo table so we
store we say well if you ever need to
compute F of n again here it is and then
we return that value okay so this is a
general procedure apply to any recursive
algorithm with no side-effects I guess
technically and turns out this makes the
algorithm efficient now there's a lot of
ways to see why it's efficient in
general maybe it's helpful to think
about the recursion tree so if you want
to compute F n in the old algorithm we
compute FN minus 1 and FN minus 2
completely separately to compute FN
minus 1 we compute FN minus 2 and FN
minus 3 to compute FN minus 2 we compute
FN minus 3 and FN minus 4 and so on and
you can sort of see why that's
exponential
because we're only decrementing and by
one or two each time but then you
observe hey these FN minus 3s are the
same
I should really only have to compute the
once and that's what we're doing here
the first time you call FN minus 3 you
do work but once it's done and you go
over to this other recursive call this
will just get cut off there's no tree
here here we might have some recursive
calling here we won't because it's
already in the memo table ok in fact
this already happens FN minus 2 this
whole this whole tree disappears because
FN minus 2 has already been done ok so
it's clear why it improves things so in
fact you can you can argue that this
call will be free because you already
did the work in here but I want to give
you a very particular way of thinking
about why this is efficient which is
following so you could write down a
recurrence for the running time here but
in some sense recurrences aren't quite
the right way of thinking about this
because recursion is kind of a rare
thing if you're calling Fibonacci of
some value K you're only going to make
recursive calls the first time you call
Fibonacci of K because henceforth it's
been the you've put it in the memo table
you will not recurse so you can think of
there being two versions of calling
Fibonacci of K there's the first time
which is the non memorized version that
does recursion doesn't work and then
every time henceforth you're you're
doing memorized calls with Fibonacci
okay and those cost constant time
so the memorized calls cost constant
time so we can think of them as
basically free that when you call
Fibonacci of n minus 2 because that's a
memorized call it's you really doesn't
pay anything for it I mean you already
paying constant time to do addition and
whatever so you don't have to worry
about the time there's no recursion here
okay and then what we care about is that
the number of non memorized calls which
is the first time you call Fibonacci of
K is in no theta is even necessary we
are going to call Fibonacci of 1 at some
point we're going to call Fibonacci of 2
at some point and the original call is
Fibonacci then all of those things will
be called at some point that's pretty
easy to see but in particular certainly
at most this we never call Fibonacci of
n plus 1 to compute Fibonacci of n so
it's at most n calls indeed it will be
exactly n calls that are not memorized
those ones we have to pay for how much
do we have to pay well if you don't
count the recursion which is what this
recurrence does if you ignore recursion
then the total amount of work done here
is constant okay so I will say the non
recursive work per call is constant and
therefore I claim that the running time
is constant I'm sorry is linear constant
would be pretty amazing this is actually
not the best algorithm as an aside the
best algorithm for computing the nth
Fibonacci number uses log n arithmetic
operations so you can do better but if
you want to see that you should take six
oh four six okay we're just going to get
to linear today which is a lot better
than exponential so why linear because
there's n
nann memorized calls and each of them
cost constant so it's the product of
those two numbers okay
this is an important idea and it's so
important I am going to write it down
again in a slightly more general
framework in general in dynamic
programming
I didn't say why it's called memoization
a ideas you have this memo pad where you
write down all your scratch work that's
this memo dictionary and to memorize is
to write down on your memo pad I didn't
make it up another crazy term it means
remember and then you remember all the
solutions that you've done and then you
reuse those solutions
now these solutions are not really a
solution to the problem that I care
about the problem I care about is
computing the nth Fibonacci number to
get there I had to compute other
Fibonacci numbers why uh you know
because I had a recursive formulation
this is not always the way to solve a
problem but usually when you're solving
something you can split it in two parts
into subproblems we call them they're
not always of the same flavor as your
original goal problem but there's some
kind of related parts and this is the
big challenge in designing a dynamic
program is to figure out what are the
subproblems let's say always the first
thing I want to know about a dynamic
program is what are the subproblems
somehow they are designed to help solve
your actual problem
and the idea of memoization is once you
solve a subproblem write down the answer
if you ever need to solve that same
subproblem again
you reuse the answer so that is the core
idea and so in this sense dynamic
programming is essentially recursion
plus memorization and so in this case
these are the subproblems Fibonacci of 1
through Fibonacci of n the one we care
about is Fibonacci of n but to get there
we solve these other subproblems in all
cases if this is the situation so for
any dynamic program the running time is
going to equal equal to the number of
different subproblems you might have to
solve or that you do solve times the
amount of time you spend per subproblem
ok in this situation we had n
subproblems and for each of them we
spent constant time and when I measure
the time for subproblem which in the
fibonacci case I claim is constant I
ignore recursive calls that's the key we
don't have to solve recurrences with
dynamic programming yay
no recurrence is necessary ok don't
count recursions obviously don't count
memorized recursions the reason is I
only need to count them once after the
first time I do it it's free so I count
how many different subproblems do I need
to do these are there going to be the
expensive recursions where I do work I
do some amount of work but I don't count
the recursions because otherwise I'd be
double counting I only want to count
each subproblem once and then this will
solve it so a simple idea in general
dynamic programming is super simple idea
it's nothing fancy it's basically just
memorization there is one extra trick
we're going to pull out
but that's the idea all right let me
tell you another perspective this is the
one maybe most commonly taught is to
think of I'm not a particular fan of it
I really like memorization I think it's
a simple idea and as long as you
remember this formula here it's really
easy to work with okay but some people
like to think of it this way
and so we can pick whichever way you
find most intuitive instead of thinking
of a recursive algorithm which in some
sense starts at the top of the big of
what you want to solve and works its way
down you can do the reverse you can
start at the bottom and work your way up
and this is probably how you normally
think about computing Fibonacci numbers
or how you learned it before I'm going
to write it in a slightly funny way the
point I want to make is that the
transformation I'm doing from the naive
recursive algorithm to the memorized
algorithm to the bottom-up algorithm is
completely automated I'm not thinking
I'm just doing ok it's easy this code is
exactly the same as this code and is
that code except I replace n by K just
because I need a couple of different n
values here or I want to iterate over n
values and then there's this stuff
around that code which is just formulaic
a little bit of thought goes into this
for loop but that's it ok this does
exactly the same thing as the memorized
algorithm
maybe takes a little bit of thinking to
realize if you unroll all the recursion
that's happening here and just write it
out sequentially this is exactly what's
happening ok this code does exactly the
same additions exactly the same
computations as this the only difference
is how you get there here we're using a
loop here we're using recursion but the
same things happen in the same order
it's really no difference between the
code this code is probably going to be
more efficient in practice because you
don't make function calls so much in
fact I stick made a little mistake here
it's not a function call it's just a
lookup into a table here I'm using a
hash table to be simple but of course
you could use an array okay be it but
they're both constant time with good
hashing all right so is it clear what
this is doing I think so I think I made
a little typo so we have to compute the
typos we have to compute f1 up to FN
which in Python is that and you know we
compute it exactly how we used to except
now instead of recursing I know that
when I'm computing the K Fibonacci
number and so many typos guys are
laughing when I compute the K Fibonacci
number I know that I've already computed
the previous two why because I'm doing
them an increasing order nothing fancy
then I can just do this and the
solutions will just be waiting there if
they weren't I'd get a key error so I'd
know that there's a bug but in fact I
won't get a cure I will have always
computed these things already
then I store it in my table then I ate
eventually I've solved all the
subproblems f1 through FN and the one I
cared about was the end point ok so
straightforward I'm do this because I
don't really want to have to go through
this transformation for every single
problem we do I'm doing it in Fibonacci
because it's super easy to write the
code out explicitly but you can do it
for all of the dynamic programs that we
cover in the next four lectures ok I'm
going to give you now the general case
this was the special Fibonacci version
in general the bottom up does exactly
the same computation as the memorized
version and what we're doing is actually
a topological sort of the subproblem
dependency dag so in this case the
dependency dag is very simple in order
to compute i'll do it backwards in order
to compute FN i need to know FN minus 1
and FN minus 2 if I can do if I know
those I can compute FN then there's FN
minus 3 which is necessary to compute
this one and that one and so on so you
see what this dag looks like now I've
drawn it conveniently so all the edges
go left to right so this is a
topological order from left to right and
so I just need to do f1 f2 up to FN in
order ok usually it's totally obvious
what order to solve the subproblems in
but in general what we what you should
have in mind is that we are doing a
topological sort here we just did it in
our heads because it's so easy and
usually it's so easy it's just a for
loop nothing fancy alright missing an
arrow
and let's do something a little more
interesting shall we
alright one thing you can do from this
bottom up perspective is you can save
space storage space in your algorithm we
don't usually worry about space in this
class but it matters in reality so here
we're building a table of size n but in
fact we really only need to remember the
last two values so you could just store
the last two values and each time you
make a new one
delete the oldest so by thinking a
little bit here you realize you only
need constant space still linear time at
constant space and that's it that's
often the case from the bottom-up
perspective you see what you really need
to store what you need to keep track of
all right I guess another nice thing
about this perspective is the running
time is totally obvious right this is
clearly constant time so this is clearly
linear time whereas in this memo i's
algorithm you have to think about when's
going to be memorized once it's not I
still like this perspective because with
this rule just multiply a number of
subproblems by time per subproblem you
get the answer but it's a little less
obvious than then code like this so you
know choose however you like to think
about it all right we move on to
shortest paths
so I'm again as usual thinking about
single source shortest paths so we want
to compute the shortest path weight from
s to V for all V ok I'd like to write
this initially as a naive recursive
algorithm which I can then memorize
which I can then bottom up if I I just
made that up so how could I write this
as a naive recursive algorithm
it's not so obvious okay but first I'm
going to tell you how just as a yeah
an Oracle tells you here's what you
should do but then we're going to think
about go back step back
well actually I mean it's up to you we
could either I could tell you the answer
and then we could figure out how we got
there
or we could just figure out the answer
preferences
figure it out alright good no divine
inspirational laugh so let me give you a
tool tool is guessing okay this may
sound silly but it's a very powerful
tool okay the general idea is suppose
you don't know something but you'd like
to know it so I'd like you know what's
the answer this question I don't know
and I really want a cushion how am I
gonna answer the question guess okay
it's a tried and tested method for
solving any problem okay I'm belaboring
the point here uh-huh
the algorithmic concept is don't just
try any guess try them all okay also
pretty simple
I said dynamic programming was simple
okay try all guesses this is central to
the phonemic programming I know it
sounds obvious but if I want to fix my
equation here dynamic programming is
roughly recursion plus memoization this
should really be plus guessing
memorization which is obvious guessing
which is obvious are these central
concepts to dynamic programming trying
to make it sound easy because usually
people have trouble with dynamic
programming it is easy try all the
guesses that's something a computer can
do great this is the brute-force part
okay but we're going to do it carefully
not that carefully I mean we're just
trying all the guesses take the best one
that's kind of important that we can
choose one to be called best that's why
dynamic programming is good for
optimization problems you want to
maximize something minimize something
you try them all and then you can forget
about all of them and just reduce it
down to one thing which is the best one
or a best one okay so now I want you to
try to apply this principle to shortest
paths now I'm going to draw a picture
which may help have the source s we have
some vertex V we'd like to find the
shortest a shortest path from s to V and
suppose I want to know what this
shortest path is suppose this was it you
have an idea ready yeah
everywhere
good so I can look at all the places I
could go from s and then look at the
shortest paths from there to V so we
could call this an S Prime so here's the
idea there's some hypothetical shortest
path I don't know where it goes first so
I will guess where it goes first I know
it the first edge must be one of the
outgoing edges from s I don't know which
one try them all very simple idea then
from each of those if somehow I could
compute the shortest path from there to
V just do that and take the best choice
for what that first edge was so this
would be the guess first edge approach
it's a very good idea not quite the one
I wanted because unfortunately that
changes s and so this would work it
would just be slightly less efficient if
I'm solving single source shortest paths
so I'm going to tweak that idea slightly
by guessing the last edge instead of the
first edge they're really equivalent if
I was doing this I'd essentially be
solving a single target shortest paths
which we talked about before so I'm
going to draw the same picture I want to
get to V I'm going to guess the last
edge call it UV I know it's one of the
incoming edges to be unless s equals V
then there's a special case as long as
this path has length at least one
there's some last edge what is it I
don't know guess guess all the possible
incoming edges to V and then recursively
compute the shortest path from s to u
and then add on the edge V ok so what
does this shortest path it's Delta of s
comma U which is looks the same it's
another subproblem that I want to solve
there's the subproblems here I care
about so that's good I take that I add
on the weight of the edge UV
and that should hopefully give me Delta
of s comma V well if I was lucky and I
guess the right choice of you now in
reality I'm not lucky so I have to
minimize over all edges you V so this is
the we're minimizing over the choice of
U vo is already given here so I take the
minimum over all edges of the shortest
path from s to u plus the weight of the
edge UV that should give me the shortest
path because this gave me the shortest
path from s to u then I add it on the
edge I need to get there and wherever
the shortest path is it has some it uses
some last edge UV there's got to be some
choice of U that's the right one that's
the that's the good guess that we're
hoping for we don't know what the good
guess is so we just try them all but
whatever it is this will be the weight
of that path it's going to take the best
path from s to u because sometimes the
shortest paths are shortest paths
optimal substructure so this part will
be Delta of SU this part is obviously W
of UV so this will give the right answer
hopefully ok it's certainly gonna I mean
this is the analog of the naive
recursive algorithm for Fibonacci so
it's not going to be efficient if I I
mean this is an algorithm right
you could set you this is a recursive
call and treat this as a recursive call
instead of just a definition then this
is a recursive algorithm how good or bad
is this recursive algorithm terrible
very good very bad I should say it's
definitely going to be exponential
without memorization so but we know we
know how to make algorithms better we
memorize okay so you can I think you
know how to write this as a memorized
algorithm to define the function Delta
of SV you first check is s comma V in
the memo table if so return that value
otherwise do this computation
this is a recursive call and then store
it in the memo table okay I don't think
I need to write that down just like the
memorize code over there just there's
now two arguments instead of one in fact
s isn't changing so I only need to store
with V instead of s comma V is that a
good algorithm my claim memorization
makes everything faster is that a fast
algorithm
that's so obvious I guess
yes okay how many people think yes it's
a good algorithm better definitely
better can't be worse how many people
think it's a bad algorithm still okay so
three four yes zero for know how many
people aren't sure yeah including the
yes votes good uh-huh all right it's not
so tricky let me draw you a graph
something like that so we wanted to
commute Delta of s comma V let me give
these guys names a and B so we compute
Delta of s comma B to compute that we
need to know Delta of s comma a and
Delta of s comma B those are the two
ways or sorry actually we just need one
only one incoming edge to V so it's
Delta of s comma a sorry I should have
put a base case here too Delta of s
comma s equals zero say okay Delta of s
comma a plus the edge okay there's some
shortest path to a to compute the true
path to a we look at all the incoming
edges to a there's only one so Delta of
s comma B now I want to compute the
shortest path from B oh well there's two
ways to get to be one of them is Delta
of s comma B sorry s comma s came from s
the other way is Delta of s comma V do
you see a problem yeah
Delta of s comma B what we were trying
to figure out now you might say oh it's
okay because we're going to memorize our
answer to Delta s comma V and then we
can reuse it here except we haven't
finished computing Delta of s comma B we
can only put it in the memo table once
we're done so at this one this call
happens the memo table has not been set
and we're going to do the same thing
over and over and over again this is an
infinite algorithm oops not so hot so
it's going to be infinite time on grass
with cycles okay for DAGs or a cyclic
graphs it actually runs in V Plus E time
this is the good case in this situation
we can use this formula the time is
equal to the number of subproblems times
the time person problem so I guess we
have to think about that a little bit
where's my code here's my code a number
of subproblems is V there's V different
subproblems that I'm using here I'm
always reusing subproblems of the form
Delta s comma something something could
be any of the V vertices how much time
do I spend per subproblem hmm that's a
little tricky it's a number of incoming
edges to V so time for subproblem Delta
of SV is the in degree of V the number
of incoming edges to V so I this depends
on V so I can't just take a
straightforward product here what this
is really saying is you should sum up
over all subproblems of the time per
subproblem so total time is the sum over
all the V of the in degree V and we know
this is number of edges a it's really
sub in degree plus 1 and 3 plus 1 so
this is V Plus V ok handshaking lemma
again okay now we already knew an
algorithm for shortest paths and DAGs
and it ran in V + e time so it's another
way to do the same thing if you think
about it long enough
this algorithm memorized is essentially
doing a depth-first search to do a
topological sort to run one round of
bellman-ford so we had topological sort
plus one round of bellman-ford this is
kind of it all rolled into one
this should look kind of like the
bellman-ford relaxation step or a
shortest path relaxation step it is this
man is really doing the same thing so
it's really the same algorithm but we
come at it from a different perspective
okay but I claim I can use this same
approach to solve shortest paths in
general graphs even when they have
cycles oh how am I going to do that
dag seemed fine Oh what was the lesson
learned here lesson learned is that
sub-problem dependencies should be a
cyclic otherwise we get an infinite
algorithm for memorization to work this
is what you need that's all you need
okay we've almost seen this already
because I said that to do a bottom-up
algorithm you do a topological sort of
the subproblem dependency dag I already
said it should be a cyclic okay we just
forgot I didn't tell you yet so for that
to work it better be a cyclic 4dp to
work for memorization to work it better
be a cyclic if you're a cyclic then this
is the running time so that's all
general
okay so somehow I need to take a cyclic
graph and make it a cyclic hmm you've
actually done this already in recitation
so if I have a graph now let's let's
take a very simple cyclic graph okay one
thing I could do is explode it into
multiple layers you did this on quiz two
in various forms okay it's like the only
cool thing you can do is shortest paths
I feel like you want to make a shortest
path from harder require that you reduce
your graph to K copies of the graph all
right I'm going to do it in a particular
way here which I think you've seen in
recitation which is to think of this
axis as time or however you want and
make all of the edges go from each layer
to the next layer okay this should be a
familiar technique so the idea is every
time I follow an edge I go down to the
next layer this makes any graph a cyclic
done what in the world is does this
meaning what is it doing what does it
mean double rainbow all right so I don't
know how I've gone so long in the
semester without referring to double
rainbow I used to be my favorite alright
so here's what it means delta sub-k MSV
I'm going to define this first this is a
new kind of sub problem which is what is
the shortest what's the weight of the
shortest
s2v path that uses at most K edges
so I want it to be shortest in terms of
total weight but I also wanted to use
few edges total so this is going to be 0
some sense if you look at so here's
here's s and I'm always going to make s
this and then this is going to be V in
the zero situation this is going to be V
in the one situation V so if I look at
this V I look at the shortest path from
s to V that is Delta sub-zero of SV so
maybe I'll call this V sub-zero piece of
one piece of - ok shortest path from
here to here is the there's no way to
get there and 0 edges shortest path from
here to here that's is the best way to
get there with at most one edge shortest
path from here to here well if I add
some vertical edges to I guess cheating
a little bit then this is the best way
to get from s to V using at most two
edges and then you get a recurrence
which is the min over all last edges so
just copying that recurrence but
realizing that the the s to u part uses
one fewer edge and then I use the edge
UV ok that's our new recurrence by
adding this K parameter I've made this
recurrence on subproblems a cyclic
unfortunately I've increased the number
of subproblems number of subproblems now
is V squared
technically V times V minus 1 because I
really actually V squared so okay right
I start at 0 and what I care about the
my goal is Delta sub V minus 1 of SV
because my bellman-ford analysis I know
that I only care about simple paths has
a length at most B minus 1 I'm assuming
here no negative weight cycles should
have said that earlier if you assume
that then this is what I care about
so K ranges from 0 to V minus 1 so there
are V choices for K there V choices for
V so the number of subproblems is V
squared how much time do I spend per
subproblem well same as before the in
degree
here the in degree of that problem so
what I'm really doing is summing over
all V of the in degree and then I
multiply it by V so the running time
total running time is V E sound familiar
this is bellman-ford algorithm again and
this is actually where Bellman's for
bellman-ford algorithm came from is this
view on dynamic programming so we've
seen yet another way to do bellman-ford
it may seem familiar but in the next
three lectures we're going to see a
whole bunch of problems that can succumb
to the same approach and super cool
hi this is sesh welcome to another
lesson in data structures and algorithms
in two previous lessons I described the
different kinds of graphs and how to
store graphs in programs as far as the
data structure of a graph goes the
storage is pretty much it the specific
operations are used on the graph
structure depend on the algorithms that
are applied on graphs of which there is
a large variety in real world
applications one such application is to
find the shortest path between any two
points or vertices in a directed graph
with edge weights in this lesson I'm
going to describe the widely used
Dijkstra's shortest path algorithm named
after its inventor Edsger Dijkstra the
algorithm works on any directed graph
that has positive weights on its edges
let's start by defining the problem and
a solution on an example then see how
Dijkstra's algorithm arrives in the
solution here is a directed graph with
edge weights we want to find the
shortest path from point A to point F in
other words a is the source and F the
destination
there are several paths from A to F
there is a BD f there is a B EDF and
there is a be e gf now let's look at the
lengths of each of these paths the a BD
f path has a total length of 17 obtained
by adding the weights of the edges a to
be B to D and D to F the length of the a
B EDF path is 16 and that of the a B e
GF path is 12 so the shortest path is a
b e GF of course enumerate all possible
paths from source to destination then
picking the shortest of those is a
brute-force approach that will quickly
become impractical as the graph gets
bigger this is where Dijkstra comes to
the rescue he proposed an algorithm that
works by trial and error start at the
source and work through the graph by
increments in each increment
and estimate the next best step to take
but if the estimate is off then it is
corrected in a future step this is a
so-called greedy approach because it
goes for instant gratification
then repairs the damage of any later and
surprisingly against all intuition this
algorithm works correctly and is much
faster than the brute-force approach
okay let's get into it
imagine that you're starting at the
source vertex a about to embark on a
journey that will eventually get you to
F along the shortest path at the heart
of the algorithm is the notion of the
current distance from the source for
every vertex in the graph the distance
of a from itself is zero you can get to
B with one hop traveling a distance of
five or you can reach C in one hop
traveling a distance of ten the other
vertices are not reachable from a in a
single hop in other words there are not
neighbors of a and because you can only
see the neighbors the other distances
may as well be infinity in addition to
keeping the current distance of a vertex
from the source we will also keep the
previous vertex from which we got to
this vertex we got to be from a so we
record a as B's previous vertex and we
do the same for C this will help us
flesh out the sequence of edges in the
shortest path when we hit the
destination this initial step sets up
what's called a fringe containing B and
C both reachable from a with one hop
next you pick the vertex from the fringe
that has a minimum distance this is the
vertex B you remove B from the fringe
and go from A to B now here's the neat
thing Dijkstra's algorithm says that
when a vertex is taken out of the fringe
the shortest path to it has been found
here you've found the shortest path B
which is the edge a to B and if the
objective from the get-go had been to
find the shortest path from A to B then
you're all done
now you might be thinking what if the
graph were different and there was
another path that went through other
vertices and eventually landed at B with
an even shorter distance the magic of
Dijkstra's approach is that this is
guaranteed not to happen meaning that
when you remove a vertex on the fringe
you are guaranteed to have found the
shortest path to it and you will not in
the future find an even shorter path
this is why I stress greedy algorithm is
practical you don't need to look at all
paths from A to B okay let's carry on
we removed B from the fringe and went
from A to B from there we can get to D
and E the weight of the edge BD is 6 and
the distance to B from the source is 5
so adding these we get 11 the distance
of D from a we replace the infinity with
this new distance because it is less and
we record the fact that B is the
previous vertex to D similarly the
distance of E is computed as 8 and its
previous vertex is set to B since D and
E have now been found to be reachable
from a they're both added to the fringe
as we repeat the steps of the algorithm
it would be helpful to track progress in
a table as shown here after the initial
or zeroth step and the first step that
we just completed the third through last
columns show the current shortest
distance of each vertex from a B and C
on the fringe after the initial step in
the first iteration or step one B's pick
from the fringe and gets done meaning
the shortest path to be from a has been
found the B column effectively goes out
of consideration since B's distance will
not change after this point the
distances of D and E are lowered to 11
and 8 respectively and the fringe now
contains C D and E
on to the next iteration of the vertices
in the fringe he has the lowest distance
so does removed n is done going to e its
neighbors are D G and C let's look at
each in turn the new distance of D via E
is 8 plus 2 which is 10 this is less
than the current distance 11 meaning
that the path a B II D is shorter than
the path a B D so the distance of D is
changed to 10 and its previous vertex is
changed to e looking at G its new
distance is a plus 2 also 10 so its
distance is reduced from infinity to 10
in other words it is now seen to be
reachable from a the previous vertex of
G is set to e and it is added to the
fringe as for the neighbor see the new
distance via is a plus 2 which is 10
this is the same as these current
distance which means whether we go
directly from A to C or a - B - E to C
it's the same distance so there is no
point in changing the distance of C
let's update the table to reflect these
changes in the third step we find that
all the fringe vertices see D and G have
the same distance when more than one
vertex in the fringe has the least
distance Dijkstra's algorithm says you
can pick any of them arbitrarily and you
will still find the shortest path to the
destination if you were to pick one
instead of the other you may find a
different shortest path there could be
more than one shortest path in the graph
so here let's pick C and remove it from
the fringe going to see we find that it
does not have any neighbors so there is
nothing more to be done in this
iteration the table is updated to
complete the step now we're left with D
and G both with distance 10 say we pick
D and remove it from the fringe going to
D we find a lone neighbor F the distance
of F can be reduced from infinity to 10
plus 6 which is 16 so we update its
distance set its previous vertex to D
and added to the
and here are the updates to the table
next iteration of F and G in the fringe
G has a smaller distance of 10 so it's
pick next and remove from the fringe
going to G we see a single neighbor F
whose distance can be reduced to 12 if
we get added via G so we go ahead and do
it and set F previous vertex to G and
let's not forget to update the table now
this is the last step there's only one
vertex F in the fringe it's removed at
which point we have found the shortest
path to the destination and here's the
completed table
the distance of the shortest path to F
is at our fingertips but what of the
sequence of edges that make up this path
this is also available but we need to
trace it using the trail of previous
vertices we have created going backward
from F we can do this using a stack
start by pushing F onto the stack then
go to its previous vertex G push that on
the stack continuing in this manner we
trace the chain of previous vertices G
leads to e which leads to B which leads
to a at which point we stop because a is
the source now all we need is to pop the
vertices from the stack and out comes a
shortest path and that's it but the
story is not fully told yet there's one
interesting twist and that is Dijkstra's
algorithm runs until all vertices are
done and the fringe is empty even if the
destination vertex was already
encountered in our example the
destination could have been any vertex
and the algorithm would still have run
until the last vertex in the fringe F
was removed there's a reason for this
apparent overkill
remember that as soon as the vertex is
removed from the fringe the shortest
path to it has been found we can save
this information for every vertex then
any time we need to get the shortest
distance to any vertex starting at the
same source such as a we can simply look
up the stored results which would be
much faster than running the algorithm
over again this makes Dijkstra's
algorithm what's called a single source
algorithm if the source changes we run
it again and save all the shortest paths
ok to wrap it all up here is the
algorithm as is the source vertex in our
example that will be a in the initial
step for each neighbor of the source
will set its distance to the weight of
the edge from the source to it so B's
distance would be 5 C's would be 10 for
all the other vertices that are not
reachable from a at the distances will
be set to and
and then we start up the loop and keep
spinning it until the fringe becomes
empty in every iteration we're going to
remove the minimum distance vertex from
the fringe and when we do that we know
its shortest distance has been found and
that would be B in this case in the very
first iteration and then for each
neighbor of this vertex we just pick if
the distance of the neighbor is infinity
then we would set its distance to the
distance of the minimum vertex plus the
weight of the edge from that to the
neighbors so for example for D it would
be 5 which is B's distance plus 6 so 11
and for e it would be 5 plus 3 8 and
these two vertices would be added to the
fringe and otherwise if the vertex is
already in the fringe so for its example
if you think of the next iteration in
which we pick E as the minimum distance
vertex we would look at its neighbor D
and find that D is already in the fringe
in which case we'll be in the otherwise
step of the algorithm and the distance
of D would be set to the current
distance which is 11 or a new distance
which is East distance plus 2 which is
10 whichever is the lesser of the two so
obviously the distance from a is 10 less
than 11 so these distance were replaced
and we just keep doing this until all
the vertices in the fringe are removed
all right that's about all of it hope
you enjoyed the algorithm and I will see
you later

That happened in Costa Rican soil
[Music]
especially when it is obvious getting chis
[Music]
I saw in my case
[Music]
albert
the girl said in youtube mac os x
use your voice
klein
[Music]
[Music]
[Music]
[Music]
bush
meanwhile
be top scorer of the team
in 2008
pizzi
They are topless and explained that following the
that
the connection with my eyes or
alone
bush
allocates only 28
8
bush
bush
the crisis
the country
[Music]
[Music]
[Music]
[Music]
[Music]
configure your important work
lot April
I am very good with an amount of
14 John River over its typical
[Music]
articles
canchi luis behind
physically active to side
their daughters
bush
[Music]
[Music]
laws
[Music]
[Music]
[Music]
although his death
[Music]
[Music]
[Music]
[Music]
[Music]
hear from officials
takes the stage and South smiley moore
[Music]
River deep in san felix
and I had no trouble
studying in France and where there are five
days later
[Music]
I saw right where forgiveness is the
project contains a foreword written
that more
the selection of topics
[Music]
[Music]
brightness or
[Music]
[Music]
It would not be good that's how we are
[Music]
do not
[Music]
efe
at last
It aims to increase
that can lead to his former partner in
crisis , but two of them
bad
or 810 photos and news at February 6 ay ccoo to
you your
santillán are taken without swing 6 x
saxophone solo pulled bank technician
66 66 agustín and which is now
the swing process and process courses
[Music]
while here
the aim is to put the walls
this is it true blood
I'm cycle as are their fans
bush
They are are connected to the alpha plus
laws
their uses
hello I'm Gavin and this is going to
work with my advisor Russell first and
let me introduce the well-known
orthogonal vectors problem we are given
a set of unfolding vectors of the
emergency and usually we consider the
American T is between logarithmic of and
unto the little of one and we want to
decide whether there exists vector U and
V such that their third product is 0 and
when this logarithmic of their simple
algorithm that runs in n square times
slogan and best algorithm bye-bye put
Williams and events in this amount of
time and the orthogonal vectors
conjecture says this problem with still
across logarithmic of the first time
unto the two- little of my of what that
means this problem cannot be solved in
sub quadratic time and this conjecture
is implied by strong eth here is the
graph of the reductions from the MP
complete word we have case in f that
problem and in the polynomial time warm
you have orthogonal vectors and here is
a here is a reduction from and the
casing access to always saying that if
the latter has a sub quadratic time
algorithm then Kasyanov set can be
solved the faster than 2 to the M and
there are a lot of reductions reducing
and for more solvent vectors to into
these problems including a distance LCS
for her distance sparse graph diameter
local alignment and longest common
substring our motivation is that the
casing offset is a complete problem in
in the mpu world and in
polynomial world we want to find us we
want to find a complete list of problems
so we showed that that there is a sparse
case of orthogonal vectors problem that
movie is a special case of this problem
and we can also show that on all these
three arrows and still hold true if we
replace orthogonal vector if we replace
parts orthogonal vectors here and we
also introduce a class of problems which
we call first-order graph properties and
it includes the problems like hitting
set click click and the K terminating
set and many other problems sports
orthogonal vectors is contained in this
class and we also show that this class
of problems are reduced to spar so V or
smart so V is complete in this class of
problems next we show was the first of
your problem means it is equivalent with
the two disjoint sets problem here we
give the input as a bipartite graph with
M edges and and on the left side artists
are the set and on the right side are
the elements if elements belong to a set
we create an edge between them and and
the input is the list of all the M edges
and output is to decide whether there
exists to the two sets that they are
disjoint here in here we consider as the
orthogonal vectors problem and to
disfranchise problems are equivalent if
we consider the orthogonal vectors of
problem in a smart way where we only
records the ones and don't post the
zeros in the input so so we say
orthogonal vectors is a special case of
two destroyers problem
the Parrot Heads of guava is very
unbalanced here the here's a website is
logarithmically small to the left side
and our question is to decide whether
the parasite graph satisfies this logic
formula where there exists set as one
exists at as to such that for all the
elements we have either when the element
is not contained in as one or is not
contained in as two similarly in we have
a sports cast of orthogonal vectors
conjecture where we replaced the time
and we replace the parameter and with
the parameter m next let me introduce
the first watercraft property by craft
we mean a bunch of binary relations plus
a bunch of urinary relations and we have
a fixed first order formula 5 with
cripples one quantifiers the quantifiers
are either there exists or for all the
input is a graph tree with n vertices
and edges notice that the input size is
the is M and we say the model checking
problem for formula Phi is to decide
whether G satisfies five here are some
examples the hidden side problem is to
decide whether there exists hitting CH
such that for all other set as their
existing elements and that is a common
element in the heating set and reset and
in the click click problem we want to
decide and if there exists k vertices
such as there is a pair of edge between
there is an edge between all pairs of
vertices and the K terminating set is to
decide whether there exists k vertices
suggest for all other vertices they're
very
at least and I drew from the dominating
set vertex to this vertex and the K
orthogonal vectors problem is to just
decide if there exists K vectors such
that for all indices at least for some
vector there isn't zero on this vertex
and on this index
by a simple recursive algorithm we can
show this the model checking for k plus
1 quantifier formula is so about in time
until the K minus 1 times m and we
conjecture it to request time I'm to
decay and this conjecture is implied by
strong age and the orthogonal vectors
conjecture and we can call it model
checking conjecture so we solve loss of
generality we can assume that the number
of input size the input size will reduce
the number of edges is near linear to
the number of nodes because otherwise we
can just use the simple an algorithm
above so we can we can always assume the
graph is sparse in this case and for the
dense graphs when Williams show that it
is decidable in time until the K minus 2
plus Omega where Omega is the matrix
multiplication exponent and when k is
greater than or equal to 9 it is
decidable in time unto the K and under
strong eth it requires time on to the
case of this bound is test where case at
least nine our main result is is we show
that the two or so the two disjoint sets
problem is hard for model hiking and
crafts under randomized reductions or we
can say the model checking conjecture is
just another name for sports orthogonal
vectors conjecture and our theorem
states that if two distances has the sub
quadratic time algorithms then the model
checking for k plus 1 qualifier formula
can be solved in time less than I'm to
the K and to prove this result we use
fine green reductions we say for problem
hi one with time t1 it is fine green
irreducible to problem hai to waste time
two if the reduction can preserve the
fine grain time complexity of problems
or we can say if ty 2 can be solved
substantially faster than t2 then PI 1
can be a source of substantially faster
than t1 we can show that for 4k plus 125
problems there is a penguin introduction
to okay 25 problems this can be done by
exhaustive search over the first
qualified variable so from to confirm
the k plus 1 quantifier problem we can
we can reduce it down to three qualifier
problems and we just need to show me the
three qualifier problems are reducible
to two disjoint sets and our steering
one says two teaspoons that is hard in
three qualified problems under
randomized fine groaning reductions
Steven Ruben is implied by theorem 2 and
0 mystery theorem 2 says there's a
reduction form for all exist for all
problems to exist exist for all problems
and the theorems recess there is a
reduction from exists exists for all
problems to two distances problem so
also possible quantifier structure our
list as are listed as follows for all
uses for all and its negation exists for
all exists by serum to can be reduced to
exist exist for all and education for
all for all exists and family rooms
three and they are reducible to two
different stats on the other hand and
this for qualified instructors are
considered as easy problems where we can
have algorithms running in time I'm to
the three halves
here are some examples of these
quantifier structures accreting set and
the Quaffle radius 2 is the first class
and the graph diameter to is the second
class and the three click and the three
independent sets are considered to be
the easy problems well the theorem 2 is
inspired by the respective is also
stated in the last talked in the
previous talk mmm that hitting set is
reducible to orthogonal vectors in the
tensile structure and we can generalize
this result to a sparse graphs and we
can show that the formulas and
quantified by for all exist for all
camping videos to exist exist for all
next we mainly show the proof of theorem
mystery we prove a theorem 3 when from
this following lemma these three
problems are equivalent under randomized
fine green reductions the first is to
destroy nests the second is set
containment and third is to set covering
and noticing this randomized the
reduction is only randomized reduction
other parts of our proof are always
deterministic here's the definitions of
the three problems which we call the
basic problem the first is to this one
says we have seen before and it is
represented by this logic formula and
this bipartite graph and the second
problem is set containment it is to
decide if there exists to stats as one
and as to such that as one is contained
in as to it can be represented by this
logic formula and we're free for all
elements if it's in as one and done is
in us too and we can we can represent
the implication
relation using the negation of you is in
as one or using us to and it can be
represented by this paragraph and to
start cover is similar East assess
whether there exists to assess which
covers the whole universe and it can be
represented by this large formula we can
observe that the first problem has to
negations and this problem has only one
negation and this problem has no
negations and this means if we could
find a way to complement all the sets
whereas to is the chosen form then we
can reduce from the first problem to the
second problem where we can can replace
the purple edges with the blue edges
then it becomes a set containment
instance similarly if we could
complement all the cells where s 1 and s
2 are chosen from then the two disjoint
sets are reducible to a to set cover
problem however we are not able to in
complement the set directly because we
are working on a sparse graph where the
number of edges is almost linear in the
number of nodes so to preserve the
sparsity of the input graph we have to
think about and other reductions our
reduction technique is to use universe
shrinking self reductions on the basic
problems for four basic problem problem
one we can use a randomized self
reduction to the same problem with a
smaller universe new prime and the
sucess as one and as to our map to two
sets as one Prime and s 2 prime then we
can complement the stairs on a small
University you so it becomes as one
prime and as the as 2 prime bar so we
can show complementing on this small
universe the country
the fun growing the complicity on the
sparse graphs here is the example of
reducing from set set containment to two
different states because the graph is
sparse there aren't too many large
degree vertices so for the large degree
vertices there are not many of them we
can do exhaustive search on them and see
if they are in the solutions and then we
remove them from the graph so the graph
has only small degree vertices then we
can stop reduced in this graph to a
smaller universe u prime and we can
complement other sites on the new Prime
and we can show because the degree is
small the error probability is small to
do the universe drinking we use a bloom
filter that randomized that randomly
maps each element of you to t elements
in you frame uniformly at random so last
hp's imprisonment Matthew function and
the funny for each test which is a
subset of you we can define a such as
form which is a subset of U prime for
the set containment problem we can see
if if as one is contained in as two then
at one prime is always contained in as
two prime but if as one is not contained
in as two and and all the elements of s
1 are unfortunately mapped into elements
inside as to frame then an error occurs
we show that the yes instance is always
knife to the yes instances and no
instances with high probability math to
no instances and the waste small
probability math too yes instances in
particular we show that when the size of
s 2 is small then the probability of pot
of false positive is exponentially small
the last step of this reduction is from
the general for from the general exists
exists for all problems to the basic
problems we define the hybrid problem
which connects the two problems in the
middle the hybrid problem is a
combination of basic problems where the
universe is partitioned to for
destroying the universes and we are
looking for a solution s1 and s2 net
simultaneous need satisfy the four
conditions the first is that on universe
you 0 they are destroying stats on
universe you one as one is contained in
s2 universe youtube as one contains as 2
and on universe you story as one and as
to work covers the universe usually we
show that we can construct an instance
of the hybrid problem of linear size in
linear time family is the conclusion we
we have we have Ruben approved the
relations showed in this graph and in
evil and if one day someone prove that
strong eth is false then the sparse
orthogonal vectors conjecture my still
hold true then we in the polynomial word
we can still have this relation finally
is open problems the first open problem
is to find a complete problem in the k
plus 1 qualifier problems we can we can
find a complete problems worse than
specific quantifier structures for
example k orthogonal vectors is complete
for in problems with que existe no
quantifier and one universal quantifier
but we don't know if this problem is
completed for general k plus 1
quantifier problems the second oven
problem is to find similar
results for dense graphs but dense
graphs are different from the sparse
graphs in the sense that if we magnify
the number of edges we can go through
all the k plus 1 variables using time
I'm to the K so we can save one variable
here but if we measure the number of
vertices intense graphs we won't have
such advantages so the design of
algorithms will be changed and the third
open problem is to extend this result to
the model checking in hierarchy
structures which we mean from web
properties to hyper graph properties the
last open problem is to find the
complete results for dynamic programming
problems the motivation is that we want
to clean up more messy in the reduction
graph so we found many strong eth hard
problems have dynamic programming
algorithms and many of them cannot do
batters and dynamic programming and we
want to know what what properties do
these problems they have in common and
we want to know if there is a complete
problem in this kind of problems
questions
versus Parsons it got much better person
yeah well in in internal turns case they
you already use the algorithms based on
matrix multiplication and in the sparse
graph most algorithms are not based on
it and in an hour example we we heavily
depend on because the graph is sparse
there are not many large degree vertices
and for some was and and forth and for
the large degree versus you can do
exhaustive search and the first multiple
we have some properties
so let's
Thanks hey guys I'm Josh and yeah i'll
be talking about radius and diameter in
sparse graphs so a brief outline of what
I'll be talking about today I'll begin
with an overview of the radius and
diameter problems just as a quick recap
next I'll talk about the various
hardness conjectures we'll be using to
prove lower bounds for these two
problems so in this work we talked about
both upper and lower bounds and we
managed to get some of them to match so
I'll talk about what hardness conductors
we need to prove the lower bound half of
that I'll be going more briefly over our
results for truly sub quadratic
approximation in other words like how
well can you approximate these two
problems in n to the two minus epsilon
time and then I'll spend more time
trying to talk at depth about our fixed
parameters sub quadratic algorithms and
we get to that section talk I'll kind of
explain what that term means all right
so radius and diameter so some quick
definitions to begin suppose we have
graph G equals V come e and then we'll
define the distance between two nodes U
and V to be the length of the shortest
path from u to V and we'll define the
East centricity of node denoted a of V
there as the maximum distance from V to
any other node and so the radius problem
just asks what's the minimum
eccentricity of any note inside the
entire graph and that note is called the
center so it says like which node can
reach all the notes quickly and how long
does it take to reach every other node
from the center and the diamond problem
is just the maximum eccentricity but
it's really just the longest path in the
longest shortest path in the graph it's
not too hard to see that if you can
solve all pair shortest paths or apps up
you can solve this problem but apps up
kind of outputs n squared things so
there's n squared time just for
outputting the information because it
outputs the information for all pairs
and there are n things so there n
squared pairs so even when the graph is
sparse apps up will require n squared
time but rate isn't a mateur because
they just sort of output like one number
the e centricity of the minimum is
interested in actually eccentricity
maybe we could possibly do better
and in particular in this paper not only
do we study the normal undirected
definitions of radius in diameter we
have the following natural extensions
for directed graphs so sources necessity
is kind of like the undirected version
but like you only care about the case
where you want to get from V to the
other nodes in the graph round-trip
eccentricity is when you want to get to
the other node in the graph and you want
to get back to your center node V for
sources for maximum eccentricity you
care about the larger of the two
distances and the course the natural
follow-up definition is min e centricity
where you care about the smaller of the
two distances and it turns out that's
actually a pretty hard pretty hard
problem so for all of these definitions
we're still interested in the minimum
eccentricity for radius in the maximum
14 diameter so for example the source
radius problem asks what is the smallest
source eccentricity intergraph and this
leads to 7 directed problems because two
of them are actually the same if you
think about it source diameter and max
diameter or just asking you the same
thing because it doesn't matter which
way you take it for diameter alright
next I'm going to go briefly over the
hardness conjectures that we'll be using
so as everyone's probably really
familiar with the strong exponential
time hypothesis by in parklands Oh Peter
I Zayn just says there's no 2 minus
epsilon to the n algorithm for k set and
it's been used yeah well throughout not
only throughout this workshop but
throughout this quarter to prove a
variety of interesting lower bounds and
many of which show that look this
algorithm we wouldn't we weren't sure
could be improved is actually tight
specifically we will be using some more
polynomial time conjectures so before I
can list a conjecture I'll define a
problem so for the first one is the
orthogonal vectors problem which you're
probably all familiar with as well so
you're just giving two lists of n
vectors in 0 1 to the C log n and the
question is is there a pair of the
orthogonal pairs so vector a from
capital a of vector B from capital B
such that the two are orthogonal and the
orthogonal vectors conjecture just says
there's no truly sub quadratic algorithm
for this problem
so the orthogonal vectors conjecture was
actually used by Radha T and Virginia
who I don't think it's here at the
moment to show that diameter is hard so
we'll be using a different problem to
show why radius is hard and how I will
explain after I show you the next
structure why we kind of need this sort
of change in conjecture to explain why
radius is hard so here's a very related
problem it's called the heating set
existence problem so almost the same
setup you have two lists and vectors
there are C log n coordinates and you
want to know is there a vector in the
first list that is not orthogonal to any
vector in the second list in other words
is there us is there is there a vector
enlist a that hits every vector in this
b and then there's the analogous hitting
set conjecture which matches the
orthogonal vectors conjecture but just
for the hitting set existence problem
instead and the reason i put these
quantifiers up here is to explain better
why we want to move to the hitting set
existence problem for radius so for the
orthogonal vectors problem it's an
existing cyst is there a vector a and is
there a vector and B such that this
property holds the hitting set existence
problem on the other hand says does
there exist the vector in a such that
for all vectors in B we have a certain
property and yeah so the next slide
explains is quite well so how do we get
from these hardness conjectures to lower
bounds for radius in diameter and so for
this slide I'll show you how you can
reduce the orthogonal vectors problem to
a diameter ish looking graph so I've
written the problem up there again but
the key idea is we'll make a set of
nodes on the left corresponding to list
a each note on the left will represent
one of the vectors from this day and
we'll do the same thing for this be on
the right hand side of this graph and
then finally the center of the graph
will have one node for each one of the C
log n coordinates and then we simply
hook up two nodes if the vector here has
this coordinate on so if it's on we'll
draw an edge and so what does it mean
for a pair to be orthogonal so if a pair
is not orthogonal then
they have a coordinate where they share
a one in other words so if if there's
some vector here and some vector here
and they actually are orthogonal
there'll be a two-hot path going between
the two and if they're not orthogonal
there won't be any two iPads because
they won't share any of the coordinates
in common and you can kind of get a
sense of why this leads to a diameter
looking graph so we aren't quite a damn
it yet we have to take a few more steps
to get to any diameter problem but the
key idea is that whether there's an
orthogonal pair or not kind of
determines whether the diameter when you
only consider endpoints and a and
endpoints in b is two or three in this
graph when we had a few widgets okay
cool so how are these conjectures
related and how believable is the
hitting set existence conjecture so like
I said orthogonal vectors we use it to
show our diameter balance and we use
hitting set show radius downs Ryan
showed in 2004 that Seth implies
orthogonal vectors and we actually show
the strange result that hitting set
existence implies orthogonal vectors
which is slightly counterintuitive so if
you call a hitting set was the exists
for all version of the problem and it
has an alternating quantifier so you
kind of expect the problem to be more
difficult than the exists exists version
but this reduction that we've shown our
paper actually proves that the other way
around is true the exists exists problem
is harder than this is for all and it
can be used to solve the exists for all
problem yeah okay so i will briefly go
over our results on how well the radius
in diameter can be approximated in truly
sub quadratic time oh and we say sparse
graphs in the title of this talk because
we care about the case where m equals n
as for most of these results relevant
because we're figuring out when n
squared is the right bound so I guess
the inspiring paper for this work was
the one by Roddy and Virginia in 2013
where they use orthogonal vectors to
show that three halves is the right
answer for undirected diameter and
X diameter and then so this is n to the
roughly n to the 1.5 in a sparse graph
so what we do is we extend these results
to weighted setting so like I said the
minimum eccentricity problems are rather
difficult one interesting case for them
is the case of DAGs so on a dag minimum
eccentricity is the only non trivial
definition of eccentricity for
everything else because the distances
between u and v are always at least
infinity in one of the two directions
all the problems become trivial somme in
diameter remains hard even for dags and
then for round trip diameter so when I
say metric for the algorithm that just
means that because because the the
centricity obeys the triangle inequality
if you just any if you just pick any
point it serves as a two approximation
for the diameter and we managed to prove
of three halves bound from orthogonal
vectors but there's still a bit of a gap
so our results various are all new
because we're introducing the hitting
set conjecture we managed to show that
the algorithm for undirected radius
actually turns out to be tight under
this new conjecture but surprisingly we
actually find that a lot of other
algorithms are tight and not even at
three halves these other items were
tight at two so for source radius we
present a two approximation algorithm
and this one's not trivial because if
you think about it source radius is not
a metric so just because there's a
center that can reach everything quickly
doesn't mean if I pick an arbitrary node
I can get to the center and then get to
any other node quickly so we we spend a
bit of work proving a two bound and then
we also show a to lower bound from the
hitting set conjecture max radius we
show that the met the knife metric using
algorithm is tight and min radius is
still quite difficult although we get
just a three-to-two gap on DAGs and then
we show that the metric case for round
trip radius is tight and if you're
interested in graph algorithms for
radius and diameter you should
definitely take a look at the paper to
see how we do these but like I said for
this talk I really want to focus on our
fixed parameters sub or attic algorithms
so what does this mean so this is a new
framework called fixed parameter
tractable and P and I guess the basic
idea is that typically when you think
about primaries complexity you want to
know what problems can be solved in F of
K times poly end time and then sample k
in this case are like what is the size
of your clique or what is the clause
size of your set instance or what is the
tree with of your graph and typically
this is applied to NP problems it
doesn't make sense for problems in P
because for any parameter there by
definition fpt you just use the same
algorithm and you ignore the parameter
but a more fine-grain approach would be
to pin down what the correct exponent on
the running times in which case pulling
out this F of K factor sometimes does
matter maybe an algorithm is you know in
this case like maybe you think it should
take n squared time but if you pull out
F of K for the right choice of parameter
K you can beat n squared time so what is
the right exponent and for our
particular problems the natural question
is can reduce damage we solved when we
focus on tree with in truly sub
quadratic time after pulling out true
with and what functions of K do we need
yeah so Jenna palomares eosin
Niedermayer also propose an approach
independently this approach in the same
year but their paper only shows upper
bounds on our work both shows upper
bound and upper bound and the lower
bound and so it kind of shows we have
all the tools we need to kind of pin
down the correct running times and this
is pretty exciting new field some recent
results that have followed foamin at all
study max flow parametrized by tree with
and many other problems and they show
roughly linear time algorithms for them
and then a mirror and some other
co-authors also study the subtree
isomorphism problem and in this case the
parameter is the depth of the trees so
these are very recent results that have
come after this work
well and so the parameter we use is true
with I won't go too much in detail
because it was already covered in the
boot camp but just a very very brief
review of the definition it's the
parameterization of under of undirected
graphs problems and the kind of tries to
capture when a graph is very close to a
tree so tree sub tree with one and the
complete graph has tree with n minus 1
so the fundamental take away from true
with is if your graph problem is easy on
trees maybe it's FP t with respect to
true with and it turns out radius and
diameter are very easy on trees although
it's not that obvious if you just try to
do it at first glance so the algorithm
for diameter is folklore but it goes as
follows you start at any vertex V inside
the graph and you choose the you find
the furthest vertex from it and we claim
that that has to be one of the end
points of the diameter and then you can
just search from the furthest vertex for
that for the other end point and you
will know the diameter of the graph and
kind of the reason why this is true is
if the diameter doesn't involve you as
one of the endpoints and then you can
find a longer path inside the graph
because inside the tree there's really
only one path between two notes and then
you get radius for free after you solve
that just from the observation that the
diameter has to fall pretty closely
between to our minus one and two are so
for example if the diameters five you
know the radius has to just be three at
the diameter is 6 the radius still has
to be three so like you can just convert
the diameter are going to radius
algorithm with this equation cool so
what do we do for true with well we
solve all versions in 22 the order k log
k plus a little more than linear time
and we also show that so we solve all of
the versions we presented exactly in
that time so including the direction
versions and by true with on directed
graphs we actually mean the true width
of the underlying undirected graph so if
you just undirected edge and you measure
the truth of that graph and we also show
that even approximating any better than
1.5 into the little of K and truly
sub-project
time woodruff you the appropriate
hardness conjecture so if you did for a
radius problem you would violate the
hitting site conjecture and if you did
it for a diameter problem you would
violate the orthogonal vectors
conjecture and I will go more into the
algorithms in this talk but the lower
bound idea comes basically we take the
constructions that we've been using to
prove our previous quadratic bounds and
we implement them with a graph where the
tree with is small and in this paper
it's actually pretty easy because as is
like I think it's pretty commonly known
if you remove one node then you lower
the truth by most one and as you can see
in the construction if you just remove
all the nodes in the center there's only
C log n of them so and then disconnected
graph will just have constant tree with
cool so I'll be covering how we get this
runtime next so what is our gimmick
strategy so we will solve both radius
and diameter and more by computing the
eccentricity of every single node in the
entire graph and the only way we will be
using tree with is by the reduction to
the following problem so instead of a
general graph will be using the fact
that it has small tree with in order to
kind of sub divide it into two problems
so they'll be notes that on the left a
note set on the right and the only way
to get between these two is the K nodes
in the center so this this goes with
truth concepts like portable or
separator and you can see the tree with
K determines the size of the nodes in
the middle that we have to deal with and
the other property of truth that this
the truth gives us is that the left hand
side and the right hand side are rather
balanced so we will be recursive on this
cool so like I said our strategy will be
recursive s to find all the e centricity
within s and will be recursing on the
right-hand side as well and then we'll
just straight out run Dijkstra from the
KA nodes and center because there's only
a constant number of them or K Avila and
then the only thing the only bit of
information really missing in order to
fully figure out all these centricity is
given a node on the left hand side so I
give you a note on the left hand side
what's the furthest node on the right
hand side I already already know the
furthest node on the left hand side
I via the recursive call and I already
know the furthest note in the middle
because i have a Dykstra call from the
middle nodes alright so we now have the
following three layered problem so like
I said feature on the left what's the
furthest note on the right and we know
it has to go through one of the nodes in
the middle so we can just write down our
Dykstra distances has waited edges and
all of the distances we care about are
only two hot pads inside of this graph
so in order to get from a inside capital
a to its furthest node here we have to
go through one of these K nodes in the
middle so I have the distances from k
both to and from every other node in the
graph so it's just a two-hot paths at
this point okay so I just want to make a
note right here that if k is equal to 1
which happens only when the graph is a
tree this problem is very simple so the
reason is the answer is the same for
every node a on the left you just look
at the furthest node see from the one
single solitary middle node B and that
will be the furthest note from every
node in a so if k equals 1 it's very
easy and then we'll kind of extend that
intuition for K equals 2 so what happens
when K equals 2 so there's two nodes in
the center of our graph there's be one
and there's be too and suppose we are
processing and we find some way and we
want to know the answer for that a so
all the a really gives us is the disc
its distance the to be one and its
distance to be too and sort of the
natural question is which nodes in see
is that the right answer to go through
b1 and out of all those nodes which one
is the farthest from b1 because that's
the one we actually want to use for a
going through be one and then by
symmetry we can do the same thing for b2
and we would have the correct
eccentricity for a sew in equation form
this is a b1 is the optimal portal node
for a node see only if this equation
holds so the path go
through be one to see is better than the
path going through be two to get to see
and then using a pretty standard
rearrangement trick when you're trying
to make comparisons like this the
left-hand side is only the dependent on
a and the right-hand side is essentially
only dependent on C so remember we're
given these at the moment but we could
pre process these and that's the key
idea of the algorithm so the right hand
side can be pre computed ahead of time
so for every node in C will associate
with it well we will associate it with a
point on a line and it will be located
at this coordinate then when we get an A
inside the left set we care about all
the points on the line that are to the
right of this coordinate and then out of
all those points we just want to know
the one that is farthest from b1 because
that's the one that will be used if we
use any node that goes through be one so
like I said in this interval which point
is the furthest from b1 and if you're
familiar with some data structures this
is just data structure problem you can
solve it for example with a binary index
tree but in general when we have a
larger k we will have K minus 1
coordinates and it becomes the K minus 1
dimensional orthogonal range searching
problem so you're essentially given the
K minus 1 dimensional analogue of an
axis aligned rectangle and you want to
know what's the biggest point inside of
this like hyper rectangle and existing
data structures can solve it in n to the
log K minus 2n so sort of one log faster
than you would expect and then log to
the K minus 2n query time but we want to
make inquiries because we want to query
every point in a so our runtime is just
so we will we will make one of these
data structures for every point in the
middle set B and then we'll make n
queries to each data structure and so
that will cost k n log K minus 2 to the
N and then we have to recurse on the
left and right
and the last thing was the Dykstra call
but that's dominated by the first term
so I haven't written it down and then if
you kind of solve this and you use the
fact that the two sides were actually
balanced it turns out you pick up the
factor K because they're not completely
balanced they can be off by a factor of
K is the way the proof goes and you pick
up the log from the current but you get
this runtime and then I guess the
homework exercise is use standard fixed
parameter tractable analysis to show
that this is dominated by the original
fbt time I wrote at the beginning cool
yes so some open questions so there was
a there was an approximation gap for
truly sub quadratic algorithms for
round-trip diameter and there was an
upper bound of two which is just a
metric case and we show three halves is
their way to close this gap and then
perhaps more interestingly for the fixed
parameter side there's quite an
interesting gap between this upper bound
and this lower bound so first of all
this term compared to the little o of K
term this gap is o of is it possible to
like narrow to improve this all the way
down to 0 of K and it might be necessary
there's also a gap between this and this
because this is almost one and that's
almost too so maybe you can improve this
if you increase this one a little bit
and the other no I want to make is our
upper bound was exact and this lower
bound even applies to anything up two or
three halves approximation yeah I think
that's actually a really interesting
question and then finally a a challenge
as most speakers have done is like what
happens if you take the FP T&amp;P framework
and you apply it to your favorite
problem in p can you say anything about
how much the running time improves when
you're allowed to characterized by a
parameter
any questions yes more like your
comments on quite sure that the
algorithm for that there is an algorithm
for diameter that runs in time 2 to the
K times look Delta with dell times the
diameter x m ok it's delta in the
exponent or ok i see a time for the
other sorry i can replace you lost a
bionic elta sometimes better i see if
the damage is constant I because then I
see ya that means that you'll know about
this types those cases in which the
diameter is 2 or 3 as it is in all your
lower buns Yeah right ok those cases the
lower bound is baby answer is so
restricted to of the regime where the
diameter is 2 or 3 or 15 or your
favorite constant 2 to the K times n is
the right answer which is kind of leader
ya understand the round trip diameter in
this regime so should the directive ramp
diameter for this are you talking about
the fut version I am I think this is the
same gap by because this algorithm also
holds and the lower bound for rent robe
diameter goes to tree with so we get
these two but there's still the gap yeah
we have our algorithm runs for all the
directed variants even min
well so two comes one as a I think it's
really nice what you did here I think
taking credit for being doing fpt on
phenomenal time problem i'm sure that
has been not i mean people apt have
always tried to reduce complexity in the
polynomial and all this kind of stuff
I'm just taking that this is I just
saying it's not a completely new thing I
thing to apply apt to poonam real time
that's all I think we actually didn't
know of any previous work that I've done
it but maybe it easily yeah it's okay
you can think of some sense
what's new is that where chidiya to
worry about enter the two minus epsilon
right yeah lowering actually the only
one time when we know that under some
plausible assumption and squared is the
best then our happens when you so this
thing otherwise
Johnson's algorithm is a way to find the
shortest paths between all pairs of
vertices in a sparse edge-weighted
directed graph it allows some of the
edge weights to be negative numbers but
no negative weight cycles may exist it
works by using the bellman-ford
algorithm to compute a transformation of
the input graph that removes all
negative weights allowing Dijkstra's
algorithm to be used on the transformed
graph it is named after Donald B Johnson
who first published the technique in
1977 a similar e weighting technique is
also used in sewer ball's algorithm for
finding two disjoint paths of minimum
total lengths between the same two
vertices in a graph with non-negative
edge weights algorithm description
Johnson's algorithm consists of the
following steps first a new node Q is
added to the graph connected by zero
weight edges to each of the other nodes
second the bellman-ford algorithm is
used starting from the new vertex Q to
find for each vertex V the minimum
weight H of a path from Q to V if this
step detects a negative cycle the
algorithm is terminated next the edges
of the original graph re-weighted using
the values computed by the bellman-ford
algorithm an edge from u to V having
length W is given the new length W plus
h minus h finally q is removed and
Dijkstra's algorithm is used to find the
shortest paths from each node s to every
other vertex in the rear weighted graph
example the first three stages of
Johnson's algorithm are depicted in the
illustration below the graph on the left
of the illustration has two negative
edges but no negative cycles at the
center is shown the new vertex Q a
shortest path tree is computed by the
bellman-ford algorithm with Q as
starting vertex and the values H
computed at each other node is the
length of the shortest path from Q to
that node note that these values are all
non positive because Q has
length zero edge to each vertex and the
shortest path can be no longer than that
edge on the right has shown the rear
weighted graph formed by replacing each
edge weight W by W plus h minus H in
this reweighed graph all edge weights
are non-negative but the shortest path
between any two nodes uses the same
sequence of edges as the shortest path
between the same two nodes in the
original graph the algorithm concludes
by applying Dijkstra's algorithm to each
of the four starting nodes in the roux
weighted graph correctness in the rear
weighted graph all paths between a pair
s and T of nodes have the same quantity
H minus H added to them the previous
statement can be proven as follows let P
be an ST path its weight W in the re way
to graph is given by the following
expression every is canceled by in the
previous bracketed expression therefore
we are left with the following
expression for W the bracketed
expression is the way to P in the
original waiting since the rear
weighting had the same amount of the
weight of every ST path a path is a
shortest path in the original weighting
if and only if it is a shortest path
after re-weighting the weight of edges
that belong to a shortest path from Q to
any node is 0 and therefore the lengths
of the shortest paths from Q to every
node become 0 in the ROO weighted graph
however they still remain shortest paths
therefore there can be no negative edges
if a GV had a negative weight after the
rewedding then the 0 length path from q
to you together with this edge would
form a negative length path from Q to be
contradicting the fact that all vertices
have 0 distance from Q the non existence
of negative edges ensures the optimality
of the path is found by Dijkstra's
algorithm the distances in the original
graph may be calculated from the
distances calculated by Dijkstra's
algorithm in the roux weighted graph by
reversing their re weighting
transformation analysis the time
complexity of this algorithm use
Fibonacci heaps in the implementation of
Dijkstra's algorithm is Oh
the algorithm uses Oh time for the
bellman-ford stage of the algorithm and
over each of the instantiation of
Dijkstra's algorithm thus when the graph
is sparse the total time can be faster
than the floyd-warshall algorithm which
solves the same problem in time Oh
come on and I'd like to thank refers own
for inviting me to coast today to give a
seminar so our today and we talked about
our a problem list my co-authors are
senior editor and AM Kazakh about
finding larger highly connected subgraph
when we know some sort of condition
about the sparseness of the host graph
okay so here's the question so if I have
a graph of order n on independence
number alpha then how large or K you
connected subgraph can I find in ng so a
lot in terms of order and K this is our
four text connectivity so if I know the
independence number so that kind of
somehow measures that how sparse or the
graph is so it's a very natural looking
question to ask so let's take our first
observation so if we just consider
taking alpha queex
each with which they're all about the
same the same size so either feeling of
flow n over alpha and then if n is at
most n alpha K then we cannot get this
chalk then we cannot get
we cannot get a ke connected subgraph at
your soul
this is the simple example so if n is at
most n alpha K then there's no K
connected subgraph in this example and
if we have more than allocate and the
the best that we can hope for is well
the biggest is end feeling n over alpha
so in general we cannot hope to get more
than feeling of n over alpha for the
answer and the problem is only
interesting them if n is greater than
alpha K and then some more observation
so K is 1 this is trivial because you
can just take the largest connect the
components in the graph there is only
our most alpha components and also K is
2 is not too difficult you know we can
prove that by considering the block
decomposition of the graph we can remove
an end block under apply induction on
alpha so while it's true if n is greater
than 2 alpha and this cannot be we can
say 2 alpha because we can take take the
paths up with to our vertices so that's
got independence number alpha and this
sir there's no two connected subgraph so
if I have to I can assume case on these
three
okay so here's some a first result so it
sets the fallen so with K and alpha of
these two and G has more than alpha
square K for the phase and independence
number alpha then an overall feeling is
indeed the correct answer for the
largest K you connected R sub graph so
let me give a rough sketch of this proof
so we kind of go by contradiction
throughout so supposed to yourself is
not K connected so then we have a we
have a cut three four four G like this
and our a1 a2 each one non-empty we
don't have any edges between them and
the book that this is up I'm both K
minus one and it's are not too hard to
show that that to independence numbers
how to come to alpha so if alpha 1 and
our two other independence numbers
numbers then they have to come to alpha
and also the the sizes of the two sub
graphs would be approximately the
proportion alpha I over alpha times the
times n yes so about the spec
right so
so and one of these will will be
typically they're having this condition
one of these two parts will will be
large enough so it cannot be K connected
so let's say our oneness of these two
well I'm yes sir then then it's too big
so we cannot be K connected so we can
continue to split up be set here so I
can say split up a 1 so here's another
cut so a 1 and I can move this cut over
to C so this is at most K minus 1 move
the cup over to C so they're like I get
a new thing here now is that moves to K
minus 1 I got 3 and then you can guess
what happens a 1 a 2 a 3 are the 3 some
to alpha and each one so it's at the
same proportion so just after we label
it so so we get that the next step and
then we can keep going up so there's no
edges so we can keep going until we get
to our four of these assuming along the
way we do not find the required K
connected subgraph so I can continue
this process and I end up with
I just continue here with alpha of these
sets
so hopeful okay so the some of them will
come to alpha and here I will have alpha
minus one so I'm going to call this
three price and now the some of the
independence numbers is alpha so each
one must be a creek so each in each unit
furnace number is 1 so we have this
situation now so we're at this stage
here there's no edges between any pair
are so this we can show somewhere along
the way this this is needed like a full
induction phone induction argument and
this is also user because they prove
this here so each Creek has more than K
for the phase and now notice that them
for any protections C Prime the first X
has to dominate one of the creeks
otherwise we have alpha plus 1
independent vs. right so let's say it
dominates a 1 and then it follows side
side we can find a partition of C prime
say D 1 after D alpha
let's I have right there where di every
vertex of the isolates a I so to that
one that
not one okay
and now the largest of the AI Union di
will be cake if k connector than its up
big enough at least n over alpha so oh
so that's the proof so I've missed our
few technical details here but this is
the rough idea or thought this result
alright so okay so where what does that
tell us now okay so with then seen
so we imagine these are the values of n
so then below of okay the problems
trivial that's nothing to to do the prom
server we don't get the K connected
subgraph
are more than our okay we do get we have
solves the problem we do get the K
connected subgraph on at least then over
alpha undone which we've seen already
the other it's not the shop answer so
the problem is also solved are on the
side so now the interesting question is
what happens between alpha K and alpha
is square K right so let's think about
if we're little more than of okay then
can we possibly in fact get get the N
over alpha for the K connected subgraph
so maybe not surprisingly are the answer
is no we can't cannot because otherwise
the problem will be completely the hope
that I with a Finn's right now so yes
that the algorithm okay maybe it's a
rather weak lower bound on N and can we
possibly drive the lower bound all the
way close to our case so for third I'm
going to tell you that it's not
we cannot go all the way down to alpha K
because I have a construction at this
lemma says if alpha 3 I need came more
than alpha then we can have a graph G on
about a little bit more than 2 times
alpha K with no K connected subgraph on
the N over alpha so fine let me just
dive through here I mentioned that so on
so I've been actually visiting professor
Boehm talked the last two weeks and
we've been discussing this problem and
so you actually found a better example
we I think we checked them within find
any mistakes and so you managed to
increase the 2 plus epsilon up to 9/4
and in fact we don't need the K more
than over here and construction fix
person so I will be showing you in a
moment and I still reckon this might not
be optimal but it's it obviously beats
this one so I have to make it double
check this but I think it's correct
so maybe I can mention a little bit
about this one in a moment but let me
just mention this one so here's an
example for alpha is 4 so what happens
here is we have alpha minus one of these
lines here so we have every circle with
a click and every thick line between two
circles means every edge is present and
these are all the sizes and let's look
at the sole so we optimize n subject to
B plus C is K minus 1 because we the
whole graph has to be not K connected so
we choose B plus C of the cut-set right
so we need this condition here and also
our the biggest cake and like that so
cough is this click over here so we need
to optimize subject to this thing less
than n over alpha so this here is the
sum of all these and then to a bit of
optimization and we find find out that n
comes to this expression for some
constant C alpha so for example if we
have 3 &amp; 4 we end up with 6
insects on 8.8 so a little bit between 6
&amp; 8 so 2 times 3 2 hands 4 respectively
ok so just comparing back to the ninth
force here so 4 3 for example it's
better we get 27 over 4 which is more
than six point six that's six point
seven five so this is better for three
and well in general it's better than
this okay so so let me now talk up or
thinking should I show you the other
construction now I couldn't make it do
that so roughly speaking because all
under construction yeah yes so this one
know the names of ever okay we and yes
with this number for the size yes
so it's like this it's actually on I
think that they could still be some
constructions that's more complicated
but this is how this one works
so what's it then website if alpha is
three we have a g3 where from with
roughly 27 over 4 K minus 1 for the safe
and no are this side independence number
is 3a no cake so upon minus 1 or
something so so one less than that's sad
that's value no K connected
subgraph on so far I will show you what
G 3 looks like in a moment so assume for
now we have such a such a graph G 3
satisfying all these and then
furthermore there is a cut-set or figure
well okay maybe our most this is a
couple of g3 so what she just simply
that was our two art and then roughly
five for here and then and how many of
these there are there alpha minus are
three of these so now that was her
construction in fact so assuming the g3
F is this will be G alpha right and the
independence number is alpha you can
maybe check out these are all quakes and
the biggest so this would be n you can
work that off fairly easily and there is
no cater like a sub graph on n over
alpha because
why is that because
I think oh it isn't
No
think it's true if you have a cake I'll
agree so that we can add and it's only
on one side I'm linking to it is yes or
commensalism
either in TP either in here or one of
these and from them so I think that it
can be verified okay so okay so let me
now go on and talk about alpha is 2 and
alpha is 3 I'm trying if I prove the
shop results so here's alpha is 2 here's
the result so now I assume K is at least
3 and if n is at least 4 times K minus 1
and alpha is 2 then we have the answer
so we have at least n over 2 and so
here's a kind of warm-up so proof so
similar to this here if we have G is not
K connected then we have a cut-set and
now the two parts must be complete
because alpha is 2
okay and then we divide into two cases
so if both of them both creeks have at
least K then one of those that's just
observe here in the cut-set we can
partition into two parts are similar to
this here with one part dolmen every
vertex dominates a one in one part and
here every vertex dominate a two so the
bigger one of these two would would be K
connected if R this is true otherwise if
let's say a one is too small then a two
with some calculation it's big enough
and it's a creek so so a two works right
so that's really simple and this results
is sharp because here's an example where
far n is just one less than what we had
four times K minus one and then you can
check that there's no so there are four
K minus 5 for the say Sun n over 2 is 2
K minus 2 but the biggest that we can
only get is D and E here which is 2 K
minus 3 so here so if we drop n just by
one I do that the previous result is no
longer true
okay so first our equals two settled
right so now the main name of my talk
today is to try and prove the case alpha
is 3 2 right so this is going to be my
challenge today so here's our result
so KL is 3 alpha is 3 and this n is at
least this 27 over 4 K minus 1 then we
have the conclusion so now n over 3
alright now I'm going to show you this
graph so hit business actually in my g3
this graph will be my g3 as here
undercut set this business g3 this whole
graph is g3 on this color here is C 1 F
and B so if you sum these three that's
the cut set okay
yes Kim K minus 1 sum of them and so
just maybe ignore the fact that these
might not be integers like maybe a think
of K is 3 more for in this picture
otherwise we put some ceilings and
floors are and we can sort of argue
similarly so this is the cassette and
and then we to get this we users we use
this cut set us here so that's how this
works then right so then this graph we
found out that's over is 3 n is 1 less
and the biggest K connected subgraph are
there are three of them in this triangle
here or this one or this one and then
all three cases are they stay or thumb
up one less than this n over three here
so you can maybe we have believe me on
this
so within come up this randomly so it
came out from now
are the proof that I will show you in a
moment so as you can see on when we get
to Alvarez 3 we already have a very
surprising construction like so this is
telling us that dealing with the
independence now number sometimes just
sort of really strange constructions so
this is what we found on say for example
if we go to alpha is 4 this is why I
kind of maybe not expecting this to be
optimal because it's kind of relatively
simple we just add two more parts onto
onto an existing graph so so just
outputs for we know we get this here and
there might be some other construction
which is better than just adding the two
to queex
okay so let me try and prove this
results yes
so I can say in our paper this is a
10-page proof so so my plan would be to
skip part of few claims that I will just
put up but show you how all the claims
fit together into a proof ok so let me
do the setup ok so
[Music]
so firstly G itself is more connected so
I have
I hope this is my G so I have a cut-set
let's call it s and then into two parts
that the independence numbers have to be
2 &amp; 1 so this Seidler creates one so
with the click let me call this a 2
right so I'm going to fill me necessary
right 1 or 1 1 1 1 would be trivial yeah
so 2 &amp; 1 obviously right so and then
this is true under it um so this is
inventions to make it would be K
collector so a 2 needs to be smaller
than n over 3 so here so uh so here will
be roughly at least 2/3 n so 2/3 n minus
K minus 1 so this will be would be too
big so it has we have to have a cut-set
of this here so like that so I'm going
to call this a 1 1 a 1 2 both must be
creeks I'm going to call this an S star
okay so we can get these facts fairly
easily doing around the size of Crete or
how do you know the a2 is small it's a
fairly large actually so so certainly a
so we're a 2 is our small gonna say that
then then our so in front of me let me
say this first so we can um as before we
can partition the estar into two parts
where let's say s1 here star as 2 star a
nephron term they tell us before right
so if we have if this happens then then
this whole thing would be at least some
what N minus 2 so this whole part here
would be at least this so the larger one
would be at least that and it should be
large enough or otherwise are so we can
either consider if one of them if let's
say a one one is small then then a tree
will be large enough otherwise if they
both are these K then one of them is at
least this bacon but that's true right
so that that's how we can we can
eliminate
eliminators here right and then to get
that how do we do that
and
yes so so to get that a to so because a
so a to we always have fun of this so to
get that second one so if so I can I
assume our I assume this also so if if a
1 1 is of this K then the larger one of
of this one and that's one would would
be what we want our using using the
system we get our two thirds minus K
minus 1 over 2 or something that should
be big enough right or not
from Google what for
oh okay no no I I jumped one step yet
not yet so to get the second one there
so what I actually do with this so I
will define an s3 where so what done
that so s3 is defined so that induced
sub graph K connected on the s3 is
maximum okay so I take a maximum set
within s such that when I complete
together with a two I get a case unlike
a sub graph and then I cannot be fooled
design have a partition of the West
where they must dominate like this right
so then now to get that if we assume
otherwise then well because this is K
connected so if a 1 1 a sub is K then R
then the maximum of so are let's max are
of these two so are a 1 1
well this is a moose n over 3 because
it's K connected so I've got 2/3 / - yes
so that that's too big and one of these
is a would be K connected so I certainly
must have that and then this one what -
like - just some calculation yes okay I
can do that so n - n - a let's do so n -
Fay - right so this is our comfort our
this is 3 and then that that would give
you
I will give you what you want using our
function on n okay right I can I can
show you that one so if we define Bo I
have to define a few things before go on
so okay so now what I'm going to do next
is I'm going to define a fess n a
certain which is upset off s 1 and s 1
star Union of them in such a way that
okay so get rivers
right n is defined to be and I'm also
going to define H so n is a subset of s
1 Union s 1 star such that the soft of H
on a 1 to s 2 star F 2 and n is K
connected on NS maximum okay and I'm
going to define L and L star to be these
two sets here
so s 1 minus n and s 1 star minus n okay
right so now and so now a 1 1 Union how
basic start that's our claim
yes so right so with more than n over 3
because because this part K connected on
H is calculated
so the remain so they take up almost 2/3
n so the root of the rest of them has to
be more than 1/3 and therefore I can
find the cut set for for this here and
I'm going to call this up B B prime not
a be B 1 B 2 and T prime and then what
can we say notice that T Prime so that's
what I've yes notice that T Prime has to
contain a 1 1 because a 1 1 contains
dominating for the same offer of this
here right so then therefore I must have
so I'm going to copy this song here so
so here this is B intersect L whoa okay
so so we're if B we have to be just two
parts here okay you hope you can reserve
T T Prime has to contain any 1 1 because
a 1 1 contains dominating furnaces or
far of the set so therefore that B 1 B 2
so that B has to be outside of a 1 1 so
it must be there and there right
and um this sob not too hard to show we
can yes sir just from this yes but
that's the media from this just take K
minus 1 away and so so B intersect a oh
is this here so in fact this is L star
this is B in personnel and so this thing
here is non empty it's large
all right so here's a very important
fact to notice so we're a one is
actually are different here and then a
one if this whole time here so I cannot
have two for this is an a one and a
third and a 1 Union s1 s2 with the three
of them independent so if I do it's a
that's very easy to three so let's say I
have I have that and what what goes
wrong right because a two is at least K
said as are non neighbor over here and
this is so we assume this is in the pen
so we've got four for the same so it's
not that's not allowed so three here is
a very important thing to keep in mind
right so on this one is also not hard to
show us so you kind of assume assume
that the contrary and you get a
contradiction to this so it's not
terribly hard to show this one okay I'm
not going to prove these three oh I'm
sorry I haven't even defined a and a
prime okay okay so eight prime and a and
Peter okay so
a 1 Union s 1 Union s to the fest is
also back so we must have be able to
have a cut-set so
right so as as usual here for both of
them
this is almost K minus 1 and R be
pleased to have dull edges with between
them under non empty
just the usual cuts right so so then
with this definition up this is not
terribly hard to show our okay so five
six and seven
I am NOT going to prove them but I will
just assume them up takes a small bit of
work but not difficult why so so in
saying that
so this is a good thing to also keep
keep in mind so a1 the sorry a by the
way we also assume we assume our purpose
is to abide by symmetry of our a a prime
and B 1 B 2 ok so up so this is a cut
for for this here
okay so this one here is start and then
P is the rest the things outside that
the a and a prime okay so who let me now
I'm going to define into two cases so
case one so we're going to look at in
here and we asked um if there are four
types of be tuned and this set here so
the case the first cases we assume yes
there is on the case one is start there
okay so let's see
if I can complete this part so a prime
is our most K minus 1 otherwise of a
prime is up these K then you can find 3
for the space independently that will
contradict dot the the thing earlier so
so we can take off what we do so X
like so we can take us to for the face
here and if a prime is on these K then I
can take a non neighbor off of Y so
right by assumption and then the then XY
set will be independent which
contradicts our culture decks are this
fact here okay so three is P connected
okay so a bit of work I know dear ah
okay so America running very behind so
on so let me I would just side complete
this case and then case to us but even
much much more complicated let's see
this case in full and unsee what happens
okay so so we've got this we got this
here our otherwise we have a
contradiction and let's see this here so
firstly right that's that this briefing
here's at least two two thousand on a
prime it's at most K minus one so the K
minus 1 K minus 1 so that is in fact I
put max in with three five words to sing
my song
right so we have this pot and let's see
that this thing is connected so firstly
I will claim that this is a creek so we
are talking about this here so obviously
we have a creek in here so this is a
quick I know so everything here
dominance it so we just have to check
two for the sides over here so let's
check that so if we have two independent
for the phase over here then our I can
just solve so a intersect L star so X OS
so if you can see them under independent
then I can just pick up that anywhere in
a a prime so it might be here for
example and and then they have to be
independent and read again contradiction
from from that earlier observation and
finally I go to add these on so once
again so I claim that everything here
dominates this part here so obviously
that's two and then if we are missing an
edge between something here and
something here I can take I can take a Y
and so we have this ability of fact here
a prime intersect a12 is not empty so I
can take a Y in that set so it would be
like over here
so if that's said this is why and then
once again in the pendant triangle
contradiction
okay so that starts okay let me try and
complete this one I don't know
I really don't have time now so so now
this thing empty so so I cleaned up this
is empty so I have to show that would be
to here it's empty so
and yes to
I'm going to prove this first so ever
if unwise are we can pick so we assumed
only if this was non-empty so we can
pick this with an pick this by a
function and we can pick this is non
empty from back here so and then they're
independent another contradiction so at
least two of them on a one we can see
that so X X's and L star so it's and Y
is in a 1/2 so it's also an yet x and y
are on a 1 so that's not again gives us
a contradiction so therefore we have
this here because yes because a so now
this is true so a does not meet be in to
say L star and neither does a prime
because a Primus so because L star is up
here
and a prime that's down here so it
doesn't meet L star so we have this here
it must be inside T and this
yes we use we use this fuck here with
two suitable for the face to get this
here so yes you can you can believe this
and then under we do a calculation here
so what's happening here so n is the
subset of t be in say a thousand
subtlety on a 1/2 union s 2 star
consists of only two parts are apart in
the second a prime and the part in the
second key so it doesn't meet a here and
so I can split this up into an a prime
part and a teapot which can be absorbed
with these two terms are absorbed into
this key here and using this here I get
the K minus 1 so I have this upper bound
and then this okay so I might have to do
the same one more picture so
and okay so let's see what's happening
right so I'm going to find what this
fence under sets are so all these 3s -
[Music]
it's this one and T prime unit yes t k
finest defense so without without these
two circles condom P intersects I'll
start one so that new thing is so start
there so right and also our let's find
what this is here on this red thing here
is our it's this this awesome here which
is almost 2 K minus 2 times K minus 1
right unnoticed starts
in fact our publicist actually contain
not any course so what what can we say
so therefore
therefore we have far locate with some
calculations we get this here are so
anyway so okay that's the contradiction
to to this facts here right this is
obviously an S and then that this being
started suppose are two times K minus 1
and 2 / Q right so that completes case
one case two well there's a lot more
facts to prove I may need another hour
to do what I think so uh
okay so case two is this so we assume
now B 2 does not meet here so this is
only B 1 here and B 2 has to be inside
here and then we find out all these
things and then put together an argument
I might need another house but ok open
problems ok I try what to say in the
bottom limit so the first one is some so
we saw that so we had on very basically
we have a some thing here and we know
what's going on here and here so this
first problem is just asking
at which point onwards to and we able to
get the cake and like the subgraph or
not least n over alpha so where is this
function f versus optimal function f of
valve okay
right so we know that it has to be more
than variable 9/4
K alpha K roughly and then for small n
so if we let's say less than nine over
four x over K minus one ah
and we know that we cannot get the end
of alpha but we nonetheless how large
can we get instead 4k connectors across
so there are some other nice questions
you can ask but here just like two
problems here were no bound on what how
large you can you know something I don't
know anything except that there cannot
be more than n over alpha so I'm not
sure what the answer is because I yeah
for example yes something's more than
let's say end of a constant alpha for
example so that would be an interesting
question to consider okay
so come to me that thank you
[Applause]
thank you some sick I can done okay so
it's great to be back here okay yeah so
this talk is completely unrelated to to
yesterday's talk it is something I've
also been working on thank you thank you
good thanks I think I've also been
working on in the last few years in PDE
alright so many equations many physical
systems in PDE are governed by the
parental equations called evolution
equation so evolution equations are
differential equations either ordinary
differential equations on involving one
variable time or partial differential
equations involving space and time which
involve yeah so but they evolve in time
so they will have to involve time so
this is one of the basic equations of
motion in here in physics
the simplest example may be of evolution
equation is the first order Oh II II
where so partial T of new time
derivative of U is f of u so U is the
unknown solution so this might be may be
the location the trajectory of some
particle for instance so for any given
time in some time interval let say from
0 to T your particle takes values in
compactive space RM may be three
dimension for instance and you move
around and your velocity is given by
some function of the solution and so f
is given to you and then you you you
evolve ok so this is a very simple
example of an evolution equation a more
realistic example I would be the partial
differential equation here it's called
the non linear wave equation N or W so
now a second order in time so but the u
depends on time and space so U is a
function of time and of G spatial
variables for example you might be three
for instance so if you live in three
dimensions also taking values in come
back to space
so this might be for example some
electric field electromagnetic field or
some other some of the field and this is
a nonlinear wave equation you take - the
second time derivative of U plus the
spatial laplacian of u you take the
double space derivative in every
direction as map ticular parking and
that will be some function of U if this
was 0 this would be the linear wave
equation and if you put a nonlinear
function of U here this is the nonlinear
wave equation and so f is some function
from RM to RM and so this is the
nonlinear wave equation a slight variant
is the nonlinear Schrodinger equation
shows up in various quantum mechanical
models so it's very similar so that your
solution your fuels now it takes values
in the complex number to the M rather
than the GU numbers to the M and instead
of taking double time derivative which
is what will give you the wave equation
you take I times the first time to have
a time derivative so could you
the only assuring equation so these are
several common examples of evolution
equations as Chilton physics there's a
lot of sub receives as to exactly what
these equations mean if your solution is
rough it has discontinuities or
singularities who but okay but I'm not
going to care about in this talk about
the subtleties I'm always going to
consider smooth solutions and solutions
which decay in space and finite energy
these sort of things
okay and when my favorite equations but
also what most difficult to deal with is
the navier-stokes equations which
describe incompressible fluids or
idealized incompressible viscous fluids
such as water
so here again the solution is this is a
system of equations involving an unknown
field you a function of time and space
this is the velocity field so any given
point in space and time this is the
water at some velocity that's a vector
and that's U of T X and then there's
also a pressure at any given point and
you can point in time space that there's
the the the water pressure that's a
scalar and then there's the true
a motion so this in every Stokes
equation this is a the continuum version
of F equals MA Newton's law so the
acceleration the the time derivative of
the velocity differentiated along the
velocity field so you also have to add
this transport term here you dot the
gradient of U is equal to negative the
gradient of the pressure as the force
coming from the water pressure plus the
viscosity term nu times for classroom
use that's the effect of the viscosity
of water it dissipates some of the
energy so new is just a positive
constant it measures how viscous your
fluid is and last noted for class here
but then you also require that U is
incompressible so us be divergence free
okay so the density of water never
changes in at least an ideal life
setting and so this system is the
navier-stokes equation sometimes we
consider the case when the viscosity is
zero inviscid fluid and then those are
called the Euler equations when you set
new to zero okay so these are examples
of evolution equations show up in
physics and we study them mathematically
and the natural problem we study so
there's several questions you can ask
about about these equations but the
natural problem to ask usually is the
initial value problem so what you do is
that for that initial time time ago zero
you specify what your solution is doing
and then you ask if you know what's
going on at time zero you ask what
happens at later time so for example if
you have a body of water and you know it
right now where what the velocity is and
pressure is at any given time at any
given point in space can you then
predict what happens you know an hour
from now a day from now what what
happens is the fluid in the future given
what happens at time zero so this is the
basic initial value problem now the
initial data you specify it anzio
depends a little bit on your equation so
for instance if you have a first-order
oh de like this over here DT equals f of
U what you do is that you specify just
initial position you just specify what
yours your particle is doing at time
zero and then that
that I told you what what happens at
later times if you had a non linear wave
equation this equation here turns value
is specified in the specifying initial
position is not enough it's not enough
to know what your way so so this sense
of equation describes for example the
the evolution of some membrane like the
surface of a drum well that's not
exactly this equation but you can think
of it similar but but it's not just a
position at initial time which is which
which is important you're thinking
specify an initial velocity so yes with
second-order equations in time you need
both position and velocity specify in
order to to control the evolution plate
at a time but in a last one since the
first order you need need to initial
position okay so what data means is
we're different from from equations
equation for the value for the fluid
equations memory soaks in Euler
equations what you specify the initial
velocity only so there are two unknowns
velocity and pressure but it turns out
that once you're given the velocity you
can actually solve for pressure you can
actually using if you take the
divergence of this equation you can
actually solve for the pressure in terms
of velocity so you just need so you
don't need to specify initial pressure
you just use the initial velocity but it
has to be incompressible athlete
diversions free because so but so
there's a compact compatibility
condition that you specify but other
than that is really no constraint okay
so all right so people happen so the
bureau PD has has been studying
evolution equations and the initial
value problem for four decades and to
you know oversimplifying quite a bit
what usually happens for these equations
is that if you specify the initial data
correctly you usually have local
existence so it is usually not too
difficult to establish a local existence
without so given any say nice smooth
solution within occurs in good enough
decay at time zero given it enough to
smooth enough data you can usually solve
the equation for a short period of time
there's some short period of time where
the
fluid is guaranteed to to revolve
properly without developing any way
singularity though your wave is not
going to blow up on you but what is
usually more difficult and often
actually unknown is the global distance
problem whether you can actually solve
your equation for all time and sometimes
you can sometimes you can't so this is
true even even with the simplest of
evolution equations for example this
first order OD e it's derivative of U
equals f of U so of course we had as you
learn as and your undergraduate PD class
okey class that that given any only like
this with a nice if with some initial
data u naught and some smooth non
linearity
there's the existence the uniqueness
theorem of Picard which tells us that
there's always you can always solve this
equation locally and so there's always
some times which are called T staff
which you can solve the solution Tokyo
equation and you can solve it on to at
some point and then you stop now
sometimes tooth out infinite which means
that you can solve the equation all the
way indefinitely but sometimes it's
finite so that you can fill the equation
up to some point and then at that point
the equation can no longer be sold and
and when and furthermore if if your
equation can only be so profound amount
of time then there must be a good
physical reason for doing so and the
good physical reason is that you have
blow up this solution let's actually go
to infinity so yeah so what you can
actually prove is that either you have
global existence the time of existence
its internet or your finite time of
existence and your solution is becoming
infinite at that time which is a which
would so vary so it is really physically
not existing
after that time okay so for example a
very simple example with the first
example everyone learns is the riccati
equation derivative derivative of U
equals u squared so for example if you
specify if you're so pious equation at
with initial data you goes one at time
zero you can solve the equations for
some time in fact for time one but you
blow up at time 1 because we this
equation has an exact solution the
solution is U of T is 1 over 1 minus T
ok so this makes sense for all times
less than 1 but time approaches 1 the
solution blows up ok so I mean well this
is the exact solution but one way you
can think about this so you can plot use
function time so so at time 0 you start
at at u.s. value 1 and so the equation
is that the time derivative derivative
the slope is supposed to be the square
ok so over here you have slope 1 so what
you expect is that some so U is
increasing at red one so it will and and
you start at 1 so you will soon reach
the point to you know and the time that
you take to reach the Bible you should
be roughly about 1 it gives with your
your slope is about 1 so it's actually
bit less than 1 but but roughly about 1
ok so you're doubling time the time it
takes for you to go from 1 to 2 is about
1 but once use of earth-sized to you
square the suffice for and if music is
so now this focus of size 4 and 2 double
from 2 to 4 if you if you're increasing
at at a slope of 4 then to increase by 8
by 2 as before the time it should take
you that should be about 2 over 4 so on
half so it takes one unit of time to
double from here to here but only half
no time to double from 2 to 4 I'm going
to go to 4 to 8 ok now the speed is 16
they should actually and should only
take one quarter of a unit of time to
double again and so what you find with
this little heuristic analysis that the
doubling times that the time it takes
for you to get twice as big as it as it
was before converged geometrically and
so and this is why your final time blow
up ok so that that that this series is
convergent and at the end you've doubled
any of the amount of time and you have
blown up ok so this equation the
solution here does not exist globally
because the doubling times are
converging geometrically to to 0 all
right ok so in this particular case you
blow up but in other cases are you don't
have blow up so many equations in the
evolution equations particularly those
that have physical meaning have come to
the conservation law that stops this
this sort of blow up this runaway to
solution happening so often there is
some conservation law such as an energy
conservation law that that keeps the
energy or some other quantity of obvious
solution finite or bounded TOEIC and
then you can't actually go to infinity
so and in particularly if you have some
conservation all that traps your
solution in some compact set then this
sort of scenario can't happen so a
simple example is that if you have a
particle in a potential well so you can
think of you know here's space ok and
yet some potential well like like a hill
when you have two marble rolling around
around this potential well so the
equations of motion for a particle in
potential wealth are given by by this
equation here F equals MA so the the
acceleration here a DT T of U is given
by the negative gradient of the
potential energy ok so this is the
equation of motion for particle in a
well and in this case there's a
conserved energy the conserve energy is
the kinetic energy is 1/2 the square
root of velocity or times mass so that's
normalized mass to be 1 plus the
potential energy
and so this this is the this is this is
conserved by the flow energy is
conserved so the kinetic and potential
energy can trade places with each other
but the total of energy is always
conserved so as long as your potential
is defocusing so this potential energy B
is said to be defocusing if it goes to
infinity and infinity so it goes up like
this okay so as long as as long as V
goes to plus infinity infinity then the
energy conservation law traps you so if
you have a fixed energy so if the total
energy of your particle is saya is e
then then you hope your your particle
can never escape this region here it
must always be trapped in that is this
interval and then it can't blow up
oopsie no what I want you to do okay
okay so should I be focusing on
non-linearity so in contrast if you have
a potential it goes to negative infinity
at infinity so this would be a da
focusing potential yeah then then you
can blow up if you have a marble here
then and if you have a potential that in
fact could any potential that goes to 0
faster than quadratic then then the more
you go out there faster you have you
know you become because of because your
presenter and you cause more more
negative and the situation becomes more
like this you could weaken you can you
can get find out and blow up and give a
negative potential let's say minus X
plus for something like that okay
so so for all these at least the basic
story is if you have a conservation ball
then which is defocusing in some sense
then you have you tend to have global
solutions nothing bad happens but if you
if you don't have this conservation law
or if your conservation law is focusing
in something then your Co solution can
can blow up okay so if you don't have
the so confining this focusing soda
focusing type of potential okay so
that's the story no de or these are
relatively well understood so you can
ask the same question for PD es if you
have an evolution equation of space and
time
and you ask okay all right so sometimes
you have global solutions known as you
don't but if you have a conserved energy
and the conserve energy is d focusing in
some sense that that the energy surface
looks more like this and like this then
okay is that good enough to prevent
finite and blow up and what's annoying
is that the answer here seems to be
really complicated it really depends and
we don't understand completely when it
happens when it doesn't happen okay so
so just to give you one example so so in
PD is even when even a country even
having a conservation law doesn't
necessarily stop bad things happening so
just an explicit example is okay so here
it's an equation code called the piece
of the focus in quintic it nonlinear
Schrodinger equation okay so it's in
question of one space in one time
dimension okay so it's i dt u plus 1/2
DX x u equals this nonlinear function -
absolute value of U for 4 times u so if
you use a complex valued field ok so
this is an evolution equation it turns
out to have a conserved mass ok so the
mass of this of this field is is defined
to be the integral of the square and
this is conserved for all types and this
is this is a positive quantity so it in
it traps you in a bounded subset oil -
at least but nevertheless this solution
there are solutions to this equation
that are completely smooth at a time
zero but then they become infinite at
some finite later time so there's an
exact solution which you shouldn't
really stare too hard but it's easier to
explain it to describe what it looks
like to sketch it
[Applause]
okay so the solution okay so at time
zero it looks like a looks connected
Gaussian it's all kind of gas unit it's
a 1 1 over the cosine square root but it
looks I look like a bump like this and
this pump has serve a width of 1 and a
height of 1 okay so it's a nice nice
bump but as time evolves what happens
okay here's one
as you approach one liked example if you
a little bit if you just a little bit
before on the bad time one what happens
here actually is that the solution in
solution won't walk or push inwards it
will be supported on a narrower interval
of size about 2 to minus n but it wasn't
come steeper with you okay it will also
become taller so so the closer you get
to 1 that means that then 2 to n because
the minus n goes to 0 you get narrower
and narrower but you also you also get
steeper and steeper okay so your wave is
focusing to a point is getting so and
then at the final time it just becomes a
mr. rock mass a little bit of Dirac
Delta mass it it becomes discontinuous
over here okay so your solution has is
concentrating or its mass into it into a
single point so any at any given time
the mass is this this mm of T is always
constant so because the mass is like the
the integral of U squared so if you have
height to the minus n 1 2 3 n over 2
you square it at a qpn you integrate on
intervals I 2 minus n that's not really
one so the mass is constant but but
still yeah but still you blow up okay
one way of thinking about this is that
in it when you work in infinite spaces
like L to the unit balls no longer
compact so so just because you're
bounded doesn't mean that your
compactness anymore
okay so that's an exact solution yeah so
okay I just settled it okay yeah so so
yes the solution looks looks to take the
shape okay okay all right so all right
so now let's take it to a slightly
different equation so this is this is
the non linear wave equation so a minus
TT u plus we'll pass in u equals so
usually P minus 1 times U and now we
have a scalar field to values and T is
some thick exponent it's measures of how
nonlinear this equation is okay so this
equation turns out to have a conserved
energy so much like the particle in a
well it's got a bit has energy which has
a kinetic energy component time
derivative there's a surface tension
component which is coming from space
derivatives and their potential energy
component like this and the total energy
is always conserved and basic question
is is that knowing the energy is
conserved is this enough to show that
that solutions here exist for time so
can you blow up okay if you know that
the energy is finite and the answer is
it depends so it depends on this
exponent P so it turns out that
something funny happens at P equals 5 so
for P less than 5 if you take this
equation for P less than 5 which is
write out the equation again
as long as key is less than five then
the energy conservation was strong
enough to to give you global solutions
nothing bad happens
that's an old rule of your games from
1961 that's got the sub critical case
and then for the critical case in P
equals five that was a lot harder so it
took 30 years after your guns before we
figured out that also for P equals five
the energy conservation is strong enough
to prevent a blow up and so again this
is a good equation which we figured
small time but therefore P bigger than
five we don't know this is this is a
okay
no don't take more than 30 years I think
shit to settle this question okay yeah
we do we do not know what happens for
people in five for this equation same
thing for the nonlinear Schrodinger
equation down here again for P less than
5 we have global solutions for smooth
data this was done again as far back as
1985 Geneva and Bello
when P equals 5 this was a lot harder
jumbled gain was able to prove it in the
radio case and then I managed to prove
it in 2008 with four other people Kali
under Q star Anita coca and P bigger
than five we don't know okay so right so
so whether it's five come from okay with
three dimensions well why why why five
so okay so so good a heuristic that you
can use to explain why some some PDS are
sort of as very easy to to deal with and
computers are really quite difficult
okay let me write a nice question okay
so yeah but actually dealing with
properly rigorously with functional
spaces and toppling the quite technical
but but you can you can get a handle on
how these equations work using
heuristics
I keep doing this okay all right so
please your equation so at any given
time you can pretend okay so any given
point in space and time your solution
should have some amplitude so it's a
wave okay so waves of amplitude and
frequency so at any given point in time
you you assume that you wait as a
certain amplitude so it oscillates at
sign up to do a and have some frequency
M which means that is wave looks like
one of the end okay
so let's assume that our solution looks
something like this at some region of
space and time of course we need a
different point in space and time the
absolute may change because you may
change but let's say let's take is
ignore that for time being and just look
at amplitude and frequency like this
okay
so you'll see if your solution has
attitude a and frequency n then when you
take the laplacian here you're
differentiating in space twice so you're
dividing by the wavelength twice so the
laplacian you would expect to be of size
n square times a and then over here this
expression here should be should behave
like like a de l'épée okay so that is
a u.s. ID a u P minus one times you
should have size by eight looking and so
the the heuristic is it so this equation
you can think of as a contest between
there are two terms here's the solution
thing is being pushed by two different
forces
there's the linear wave equation
dynamics coming from surface tension
which is trying to so dissipate the wave
and push all the oscillation our
community and that's going from the
linear term and then there's the
nonlinear term which is doing all the
bad stuff you know focusing your
solution together making it to
potentially do all kinds of dozen things
and that's that's given by the nonlinear
term and so depending on which of these
equations is bigger one of these two
terms should dominate so if some yeah so
if the nonlinear size is bigger than the
end than the linear side
you should expect you should expect on a
nonlinear behavior which is bad okay but
if this Nonya side is less than the
linear size you should just expect
linear behavior is this is a heuristic
okay okay I know how to do it out okay
back okay all right on the other hand if
your three dimensions like a turn in
three dimensions if you have a
wavelength of 1 over N then you must
your solution must spread out okay so it
could be localized it could be look like
on okay so your solution has a
wavelength 1 over n it could be
localized to a single ball or maybe to a
slightly larger region but the region
that you are localized to must always be
at least as big as as a ball or a ball
over a decision certainly principle from
harmonic analysis so the the size the
volume of your where your solution lives
should be at least like one of it in
cubed down here okay this is Jen 30
principle
okay this is not rigorous but my
character it's a good heuristic okay so
this this okay so you can use this to
identify the energy so remember the the
conserved energy here has got a a toughy
kinetic energy component a surface
tension component at a particular
potential energy component and you know
using these logistics you can even you
can start understanding how big this
energy is so for example this surface
tension component here should so
gradient of U is the size n times a you
square it and okay squared you've got a
volume of 10 to the minus 3 at least so
the amount of surface tension energy
should be at least n a squared a 10 to
the minus 3 and then you have a similar
you have a similar bound for the
potential energy and so you have a lower
bound for this energy so if the energy
is bounded energy conservation consumed
it provides a constraint between the
amplitude and the and the frequency
there is some limit as to how much
amplitude you can app will give in
frequency so you do this computation and
then what you what you find is that this
lower bound here so if the energy is
bounded then this quantity must be
bounded there must be some bound to
making an amplitude and frequency so yes
you have this down here and then you can
just do a little bit of high school
algebra and you find that that if you if
you have its amplitude and frequency are
constrained by this bound then you will
be necessarily in the nice linear
behavior regime as long as P is at most
5 okay okay so I'm not gonna do the
calculation here but it's just somehow
if you were to get from here to here and
you find out when Peterson 5 you can you
have it you can do this P equal to 5 you
just barely get from here to here and
when P is bigger than 5 this condition
does not imply this condition here okay
so that's that doesn't prove anything
but it does it does indicate why P
equals 5 is important and having similar
holds for showing it okay so
wanting this tell you one thing that
suggests this completion suggests is
that the the worst case the case that's
going to be the most nonlinear is if all
the energy of your solution is
concentrated at so if at each time the
energy is concentrated in a smaller
region as as is predicted by the
uncertainty principle so so at any given
time the solution may may be supported
in just a little ball sized one cube or
it could be spread out in a more
turbulent fashion in the largest larger
space but but if it's spread out in a
larger region then the amplitude will be
smaller because of all energy
conservation and then you're more likely
to be in the linear regime so the the
most nonlinear case scenario would be if
all your energy at any given time is
packed into in just one ball of size of
radius equal to the wavelength and if
your solution can do that then and if P
is bigger than five you're doing a super
critical regime then you would expect
well then conceivably blow-up can happen
and this is something we saw so in this
explicit example I gave earlier solved
the quintic n LS is exactly what happens
at any given time the solution is
concentrated in a smaller region as is
committed by the inserting principle and
so then you actually blow up okay so but
what we don't know is whether this
actually happens okay so so for P bigger
than five in the supercritical regime it
is potentially possible to blow up at
least this energy argue the scaling
argument is not prohibited but we don't
actually have a construction that does
this okay so so I started working on
these sort of questions in the last few
years and so rather frustrating that I
can't say anything about the actual
equations we care about like like this
scalar nonlinear wave equation but I can
ask these questions if you allow me if
you allow a cheat again when she
unfortunate is a big one you have to
change laws of physics so the so for
example for modeling wave equations we
don't yet know for this equation what
happens in P is between five pick and
five we do we have global existence or
do we have fun and blow up what happens
we don't know
leading up to a scalar equation but what
I can show is that if P is bigger than 5
then I don't know about the scalar
equation but if you take a vector valued
equation so you take value in some in
some are to the M well in RN then there
is at least one non linear wave equation
so with some potential energy but I get
to pick at the potential to exist the
potential energy such that which is
defocusing so the energy is positive and
it keeps the the energy it keeps all the
terms the energy are bounded but but
even though the energy states bounded
the solution still blows up at O and the
pinna transfer has the right size it so
it behaves like a usually plus one but
this equation does have solutions which
are which are really nice at time 0 is
smooth and compact is reported as nice
as you can imagine but they do blow up
because the solution does concentrate it
back to a point and blow them ok so so
the scalar equation we don't know yet
but but effective value equations of
this type do work as long as n is big
enough actually in my proof msu least 40
so as long as you have at least 40
degrees of freedom you can you can you
can force blow up okay and there
something similar for n LS which is a
bit more technical but I will not
mention it here okay so this is so it's
a somewhat artificial equation because I
most potential V that is not natural
it's something that that I choose to
make liquid to make me without work so I
would really love to say the same thing
for the scalar equation this is a but
but it doesn't directly say anything
about this equation but what it does do
is that it does provide a barrier to
proving so you know so there are many
techniques out there for proving global
regularity and one could hope that that
one of these techniques you would if you
apply them cleverly enough that you
might be able to to show global
existence for the scalar scalar equation
but but now that we have this vector
valued on any wave equation for which
blow up happens that tells us that no
method can work to prove global
regularity for this equation on
less they can somehow distinguish the
scalar wave equation from the vector
value wave equation and most of the
techniques that we know about if they
work for the scalar equation they also
work for the vector valued equation so
those techniques are not cannot possibly
solve the scalar equation yeah yes if
you want to solve if you want to put
global regularity for this equation you
must somehow use some special property
of a scalar equation which is not shared
by the vector valued equation so it
shows that global existence is going to
if it's true it's gonna be very hard to
prove it's going to need a new technique
in fact I don't believe global exists is
true anymore for these equations okay so
I want to mention some so it's now case
some of the ingredients in the proof all
right so one nice property of this
equation is has finite speed propagation
that the solutions into this equation is
this exactly a relativistic equation
which solutions cannot propagate faster
than speed one if you would have seen
squared here they would not propagate
faster than C but so the speed of light
comes in you know but we normalize this
utilized in one so solutions cannot
propagate faster than the speed of light
so because of this you can actually just
work in a in what's go backwards like
comb you you take time in space and you
take a backwards light tone like this
and you can just specify initially you
can just build a solution only in this
light tone you you can find some nice
smooth solution at say time minus one
which just concentrated which blows up
okay so it's nice and smooth here but it
comes more and more infinite as you
approach at this point here and you
don't bother solving the equation
outside here you only solve the
equations inside this cone if you can
make the solution blow up in this cone
then you can extend the solution to to
the rest of space you just extend your
smooth data however you like outside of
this region and you solve the rest of
the equation but but because the
assertion already propagated theta why
it doesn't matter what you what you do
over here you won't affect what happens
inside this cone so you can you can you
can almost compactify the problem by
working only in this
in this cone this basically
collectivized space and doesn't quite
come back by time because of this blower
point here okay so these are the
construct solutions in this in the back
was like oh now there's a standard way
to try to contract blow-up solutions
which blow up at a point like this which
in sequence ocular self-similar solution
so there isn't there's a certain natural
notion of a self-similar solution which
which looks like this so what it means
is that the solution any given time it's
just a rescaled version for you solution
here so the solution is doing something
over here it is you do the same things
are scaled you know made smaller that
also might all up okay
and it should squash tail point so you
can try just so a very standard thing to
do is to try just constructing a
self-similar solution focus equation but
terms unfortunately solutions don't
exist there's there's a certain
integration by part of miracle which i
won't i won't talk about here but but
you can show that that such solutions
will have to do energy after lots and
lots of integration area part but why
you can um what i'd turns out to be
possible to do is you can construct a
solution which buts go discretely
self-similar so it's not always the same
shape but it keeps returning to the same
shape at a pure x at a time so i let's
say at a time minus 1 it does something
and then it changes shape but then at a
time minus 1/2 it comes back to the same
shape but we scale okay so if it does
something
it's a these are solutions that have
some is good that could breathe the
solutions they breathe in and breathe
out
associations don't leave we are
preserving the grease in and it comes
like to to to where was before but
smaller and and and control up okay and
then it reads out again in the reason
again at time minus 1/4
you again have to come back to its
original shape but again we are narrower
and taller than what's before and it
keeps doing this and then by the time
you reach time 0 you would have a knobby
country
the sauna on the set of waves are zero
and input amplitude and you get blow up
okay so the reason why this is a good
type of solution to consider so one way
of thinking about it is that you are
quotient again so you have this cone
this non-compact cone so you take the
cone you remove the origin so there's a
non compact region but you are you're
imposing a symmetry and the symmetry is
the dilation group by two so you use
either by 2 by 4 by 8 by PI over 2 and
you are you're imposing a symmetry of
the solution respective to this group so
if I call this cone your gamma what
you're really doing is that you are
quotient in this cone by by by the
symmetry group and you're really solving
now as a new PDE on the quotient space
now the quotient space looks looks like
it'll come back to torus it's what it is
it's the is a truncated cone you take
the comb from to the minus 1/2 to the
minus 1/2 truncated cone and then you
take this in here and you glue it to
this and over here you take this end of
the cone you squish it down and you
identified okay so you get I go looks
like a Taurus
but the donuts if you wish okay but but
the point is its compact now you move
all the or the non-combatants where you
problem and so you're now trying to
solve a certain PDE on a compact domain
okay so all right so how do you actually
solve this equation so it's a weird so
normally in PD is what happens is that
your first given the equation so you you
you have your equation of motion and
your PDA and then you solve it okay to
get your solution you okay so you take
your your the PD maybe get some initial
data you'll have initial data okay and
then you sober ticket to get your
solution you and then once you have your
solution you can start computing various
features of the solution like like
energy momentum various physical
features of alpha solution it turns of a
wave equations most of the physical
properties of your solution are caprica
fun I captured by chunk of the chess
engine stress energy tensor
which is a certain quadratic function of
your of your plasma nonlinear term okay
it's a certain combination of
derivatives of your solution yeah so
yeah it captures both both energy and
momentum and something called stress yes
so for example T naught naught is the
energy density for instance yeah this
thing that we saw before time
Canoga and you picked up our surface
tension and potential energy again and
then there are other components we'll
take them they measure momentum density
and so forth okay so this is an
important statistic of your solution
okay and it obeys certain property so
energy is conserved and momentum is
conserved and this leads to the stress
energy conservation mode turns out that
this tensor is - is divergence free in
particular
okay and it has some other property it's
positive definite and as it has other
some other properties okay so this is a
tensor which captures many other
physical features of the solution yeah
so so normally you start your equation
you find your solution and then you
compute things like the stress energy
tensor so to cuss are to blow up we do
everything in Reverse what we do is that
we first write down a stress energy
tensor so we write down a solution we
write down a tensor which we believe to
capture the energy and momentum curves
of dynamics of your of your solution we
first write down a tensor stress-energy
tensor as to obey certain conditions
like the word this conservation law
starting once you choose your ticket
once you guess what your tensor is you
then find a solution you that has that
stress energy tensor so you give you
have to solve this equation here where T
is the data and use your note and then
once you have yeah so okay so the
equation I'm trying to solve is
the equations here okay so finally once
you have your solution then you okay
whatever it is a time one minus one is
your initial data and once you've a
solution you then find your PDE
okay so once you've seen you you find
the percent for the potential for which
this equation is true so so everything
everything goes back in reverse but okay
but this is how you can actually
construct solutions that blow up and
it's because I need to choose it's
because I choose my equation last that's
why I don't get to pick my equation and
redress
so it is unfortunate that so one catch
with this method yeah okay so maybe so
it's the technical so but maybe I just
mention one cool thing so getting from
the energy transfer back to the solution
you yeah so it's yeah so the question is
given a sense of T alpha beta can you
find at you which solve this equation
all right so that's a very complicated
system of equations but it is a variant
of a much better studied equation so
there's a much more famous system of
equations for the isometric embedding
problem where if you start with ten
sergeyeva beta and on some compact
manifold M if one of if you want to find
some some fueled users that the the
opportunity value in a product that beta
tutorial of you it's always equal to G
of a beta so this system of nonlinear
quadratic PD this is called a symmetric
embedding problem because so finding
such a new is equivalent to finding an
isometric embedding of M into u
Euclidean space and it turns out that
this equation this was famously sold by
John Nash in the 50s so so he proved the
national bedding theorem that given any
smooth compact manifold of certain
dimension if you specify if you make
this dimension M big enough you can
always embed any six dimensional
manifold into RM isometrically
but the dimension is you pretty big and
so this is why I need I need 40
dimensions so actually so this is a four
door manifold that I need to embed
isometrically
into a certain space and and yeah so I
think nationally is like 16 or 17
dimensions to to embed a 440 manifold
for the small complicated problem I need
40 40 dimensions but yeah so very
conveniently for me that you know this
problem is already solved because this
national bearing theorem and that let me
get some here back up to here getting
from the solution you've back up to the
equation V to find V this is actually
not too hard this uses classical
analysis in particular that the cheat
extensions the organism is the main tool
yeah so the the the most tricky thing is
exactly to find a good stress energy
tensor which everything else works yeah
so the stretch in tensor has to obey a
certain normal conditions but it turns
out that in three dimensions at least
you can make a lot of symmetry
assumptions you assume that your
certainty tensor is rotation invariant
and dilation invariant and after making
a lot of symmetry reductions the
conditions that you need boil down to
solving to the whole bunch of a small
number of Audis and you can actually
solve it all these in by hand explicitly
and and you can find a syringes template
that does what you want actually what
happens is that this is the energy tends
to actually concentrates very near the
boundary of this cone so it's the
solution comes like an imploding shell
it's a shell of energy that includes to
a point okay
all right so that's all I'll say about
the wave equation let's see what I have
move ok class I think I'll skip Euler to
talk navier-stokes because ok so yeah so
the equation I most care about which I
that I can't I can't result no one can
result is the navier-stokes equations
again so we go back to the ok so the
fluid equation equations for waters so
this is an equation for setting fluid
with both viscosity and pressure
pressure is a bit annoying so it turns
out that you can you can eliminate
pressure from this from the system
there's a there's a single integral
called it the way protection it's a
fuckin on projection to divergence free
vector fields this is the gradient
ratings are always orthogonal to
divergence free vector field so if you
projects to direct view vector fields
you can eliminate the pressure and you
get instead you can turn that the
navier-stokes equation into a nonlinear
heat equation so you can simplify the
system it's just dt u equals viscosity
opacity without just be the heat
equation plus this annoying by linear
non-linearity which is this somewhat
complicated by the in a multiplier here
okay so this this biting multiplier has
a certain property there's a certain
cancellation this buu is always a fog no
to you
the term times this you can check by any
good integration by parts and what that
means it's equivalent to energy being
conserved like an energy being
dissipated so this equation it doesn't
quite conserve energy because look this
casa t viscosity will do a drain away
energy turned the kinetic energy into
heat but what you find is that the
kinetic energy of your selves of your
fluid is always decreasing okay and it's
always dissipated as heat okay so so in
particular this this that this energy is
always bounded in time and in one in two
dimensions that's enough to get global
regularity so the navigate occlusion is
one day and critical in 2d and that's
good enough to global regularity in
these dimensions but the dimension we
care most about is three dimensions and
in three dimensions unfortunately this
is a super critical equation the any
conservation art does not prohibit the
solution from from being in the
nonlinear region where the amplitude is
is so big so the applicant can be so big
potentially that the normally behavior
happens so we do it does not seem that
the energy density is strong enough to
prevent law and the big question is is
it doesn't have any secretion actually
blow up all right there's a
million-dollar prize anyone who can
prove that or disprove that so I can't
do it no one s is it but what I was able
to show about two years ago with a
similar sort of cheat to before if you
change the laws of physics then you
cancel the question so if you take this
equation that you replace the actual
non-linearity buu which captures with
the physical features of the Navy
physical equation by what I call an
average version of bu something a little
bit smaller so it is possible to make
the nonlinear term a little bit smaller
by averaging it against a sector certain
symmetries but let me not talk exactly
what averaging means is big complicated
then the average there exists an
averaged equation which is still as good
energy property still obey the ng
inequality so energy so dissipated quest
but now there are solutions which start
off smooth and that very very nice you
know Schwartz class but but which blow
up in finite time which which which
concentrate to a point much like these
are the solutions here they're not quite
so similar but it's their closest of
some of them and so I wasn't the first
to do things like this so this previous
work of Montgomery Smith Sherman
Gallagher Pike you release and I but but
the previous equations that
constructions did not have energy
conservation so the enemy was was
blowing up together the solution but
this is this is an example with the
energy state considered the energy is
still bounded but the solution blows up
so what this tells you is that you
cannot solve the navier-stokes equation
regularity
just using the energy conservation law
and standard methods you know you would
need to use some technique that
distinguishes the true navier-stokes
equation from this sort of faking idea
so this equation here okay yeah okay
all right so I just maybe say a little
bit about running out of time a little
bit about what we do here so it's it's a
similar sort of yeah so I I don't
construct these self-similar solutions
in fact I don't even know if they exist
and Navya Stokes yeah but the idea again
is to create something like it
discreetly self-similar solution so it's
inclusion we given time as a certain is
localized to a certain region of space
and as a certain frequency as an
amplitude and then it does something it
reads it goes in and out and does
something okay
but after this it amount of time you
wanted to concentrate into an even
smaller region of space and with a high
amplitude and then it will then breed
out and weave in again but but faster
now okay so that may be in half the time
it will kill or concentrate who will
rescale to it a smaller version itself
and then do that again and again and
again until until you blow up so you can
still manually choose a non-linearity to
sort of push you can kind of push your
solution into smaller and smaller scales
but when we and I wasn't the first to
try doing this but but when when you do
this when you try this in the most naive
fashion you encounter a phenomenon known
as Komarov turbulence so you know when
you actually make actual foods foods
don't do this okay you know if you if
you make a little eddy of water here the
world does any what does not spin into a
smaller idea water and then spin it into
an even smaller idea of water and so
forth what happens is that the only
spreads out so you know maybe maybe some
of the energy will move to to to this
smaller scale but then but but then this
is the energy here method while you are
you trying to squash this energy from
from this scale to this scale the energy
at at this scale itself pushing energy
into into an even smaller scale and so
on and so forth and what will soon
happen is that your your your energy of
your stew of your fluid will spread out
in a power spectrum there'll be some
portion energy at low frequencies and
medium frequencies and high frequencies
and once you have enough energy
spreading out then the amplitude drops
to a tube to a point where the the you
know the the linear term dominate
dominates again so you cannot stay
attenti you don't stay in the nonlinear
domination regime if if the energy can
spread out and so the hard part of this
office work whether we I had to somehow
program is nonlinear IQs the
non-linearity to prevent the the coma
grab turbulence affect him happening so
what I had to do was it was I had to
keep the solution look like that and one
region space one one frequency and you
keep it there for a certain period of
time and then you very suddenly squash
it to it the next higher so the solution
service has been done very slowly at a
certain frequency and then it suddenly
collapses to something of half the size
but then yes maybe if I make a stop
again yes so it
you then have to put a delay in your
equation so it you have to wait for all
this all the energy from from the
previous mode to to drain it into your
new mode before you start the next phase
of the cycle if you have all these some
energy transfer modes going on
simultaneously you get this - you get
this turbulence effect so yes I sort of
program your your your equation almost
like like making a circuit actually like
designing a circuit and put my mind my
actually I thought I'd give my wife
actually who was trained as a lexical
engineer and you know so you know
engineering some of the opposite of
mathematics income so so you know in
mathematical PD you're given the
equation and you start solving it but
engineering you have a desired outcome a
solution and you want to design you know
your circuits design your equation that
will produce the Goodman outputs of the
opposite mentality so yeah an engineer's
design circuits you know out of simple
objects in like maybe capacitors and
resistors and so forth and you take
these simple objects and put them
together and so I had to make to make
these please
so I built this this non-linearity which
is really complicated
smaller simpler nonlinearities that
which behave kind of like a key like
logic gates and in all gates and and I
had to program a simple clock so let the
solution only moves one frequency to
another after a certain time delay but
anyway it is possible to do this after
quite a lot of work and you can create a
lot of weird equation which which which
behaves like navier-stokes and pulls up
yeah so I say what this tells you is
that this rules out so many of the
existing techniques to prove clover
gravity will not work to soak
navier-stokes because they cannot
distinguish the navier-stokes equation
from this artificial average
navier-stokes equation which does have
an assembler so if you want to solve the
global gravity from the navier-stokes
which I don't believe is this is true
actually but if you want to do it you
must use some property of Abia Stokes
which is not true for this average
equation now there are such things okay
so this this result doesn't doesn't
exclude every single possible way to
prove regularity for an area so for
example you can take the curl of the
navier-stokes equation and there's an
extra equation which is very important
for navier-stokes which which is cynical
the vorticity equation so this import of
the vorticity which is the curl of the
velocity two images so how much your
your your fluid spin and this an
additional equation in the episode per
my but this average net worth of Stokes
equation does not obey an equation was
form so one possible loophole I see is
that if you want to prove globalclarity
navier-stokes you would somehow you need
to use this equation now I do believe
that there is some modification of my
construction there should be some way to
construct in a visionary surface
equation that but also obeys the
vorticity equation and which has finite
envelope which would close this loophole
I can't do that for navier-stokes I can
do it for the Euler equations but I
think in the interest of time I will not
discuss that those results and I think
I'll just say you skip with that and
yeah
okay
okay so yes i'll talk about graph
parsimony utd's only the most valiant
relationship okay if you remember this
one so this is joint work with emily fox
at university of washington so that
there has been a tremendous interest
where recent years in in the study the
understanding and the modeling of
complex networks so so i'd be interested
in in so in different types of networks
for example so networks were a set of
nodes of vertices with connection
between them that maybe the right kiddo
and the right kids for example you got
directed multi graphs sonos may
correspond to two people and an edge
nature from one node to another node may
correspond for example to an email from
one person to another persons who have
also examples for example citations or
the world world wide web we also
interested in simple graphs so now you
just have nodes and undirected
connection between two nodes so the most
popular example is a social network so a
node may correspond to an individual and
and an edge to a connection between
three DVD also here to repress on the
network so the size of the node is
proportional to the degree of that note
or to the number of connection of that
of that node and actually so this this
network is a is the network of
collaborations between oxford and well
week academies so you've got oxford in
blue and well week in rates while jim
mentioned so we expect to have much much
more edges in in the near future so i
didn't want it to put names on the
under gravity because usually a please
one person with the largest node and
then I upset everyone is so so I'd be
also interested in in the last type of
network so bipartite graphs where now
you've got two different types of of
nodes so nodes of type A and nodes of
type B and you only allow connections
between those of different types so for
example here scientist authoring papers
readers or in books internet users
posting messages on forearms or if you
think about recommender systems so
customers buying items so while i'm
interested in am experiencing in
developing a statistical network model
and so why do I want to do that so I may
be interested in basically observing a
network and try to find interpretable
structure in the network so by building
the statistical model I've got some
parameter that I'm able to interpret it
and tell me something about the networks
for example some block structure awesome
Layton's parameter that may explain the
connectivity of that graph may be
interested more in in predictions so for
example I will be interested in
predicting missing edges in the network
or may be interested in predicting the
connection of new nodes so if you think
about the customer buying items so I've
observed so some subset of the customer
having bought some sort of items and
then I've got a new customer and we want
to predict the east set of connection so
we so we've got now some massive
networks which has on massive size of
just put like latest estimated thing
from from various companies for example
linkedin so three hundred millions user
facebook claims more than a billion
Twitter to 300 million in the world well
well to a billion soap so we've got very
very large network and three
we've got sort of to hem so first so we
want models that are able to capture the
large-scale properties of network and so
I'll discuss that in a minute and we
also want once we've defined this this
model we want also a scalable in France
algorithm to learn to learn the
parameters of all those owners and so in
terms of of property in these talks I
will focus on the on the on the
following properties which is a total
poverty or network which is a spa cities
what is the definition of sparsity so if
I write so an e so so think about from
now thing about a simple graph n is the
number of edges in the in the network
and n is the number of nodes so we we
call a graph dance if the number of
edges scales quadratically with a number
of nodes so it grows basically as the
same rate as the maximum number of
connection and and what I we call sparse
so there may be different definition of
a sparse graph at wellick I will call a
sparse graph is basically a graph which
is not dense okay so where the number of
edges grows sub quadratically with a
number or not so it is regularly so
admitted that most of the real world
network are actually sparse and now are
not dance okay so this is not because
the size of Facebook double that the
number of your friends will double at
the same time and so what comes with
sparsity what is related is it's also a
very heavy tell degree distribution so
the degree of a node is the number of
connections of that node and for many of
the real world network so the degree
distribution are heavy tail so whether
or not its power low it's actually a
power distribution is some there's some
matter of debate but actually it's
anyway it's very heavy 10 and what you
expect also in real world network so is
having some sort of a late
structures or some block structure were
you more likely to a more connection
between nodes of a certain type than
between nodes of of different types so
so I would like so to be able to to
build a statistical network model that
can capture those those properties okay
so just as as an illustration so here's
an example of a bipartite graph so two
types of nodes so one set of nodes is
the readers so this is from the book
crossing community network so we've got
five thousand readers represented at the
top and I've got so the second type of
nodes books so I've got three thirty six
thousand books represented at the bottom
and so again like as the as the previous
picture so the size of the node is
proportional to the degree of that node
so on the top the larger nodes
correspond to readers who have read a
lot of books and at the bottom so larger
nodes correspond to books right by a lot
of Raiders so what you see here so I
don't know if you know some of the books
while animals da Vinci Code so those are
the most popular books in in the data
set so what you see obviously so that
the graph is quite sparse with only a
fraction of the potential possible
connections and there's a high variance
in the degree of the of the node so for
both the leaders and end the books so
some of the videos have a very very high
degree but most of the readers actually
have only read one or two books and the
same for the books and so if you if you
represent the degree distribution so the
distribution of the connection for both
so for the readers and for the books in
your present it on the log-log scale
okay so here I've got the proportion of
reader having read one book two books
etc and near the same sub proportion of
books
right by one-way dirt orders etc so
you've got this sort of a linear trend
so indicating some sort of heavy tail
power low degree distribution so I would
like to be able so to to to build a
model that can capture such such
behavior okay so I so I took now but so
network modeling and the classical
approach route network modeling and I'll
focus now on simple graphs okay so I
don't have any direction so so
undirected graph without self loops and
so typically so we're interesting in
defining the model for the adjacency
matrix so we're with entries binary
entries xij where xij indicate if
there's a connection between load I and
no Jay and so a reasonable assumption
which which is made is an assumption of
of joint exchangeability so intuitively
just means that when you define your
statistical model the order the the way
you order the nose does not have any any
sort of importance so mathematically
what you assume that the distribution of
this infinite matrix xij is invariant
over permutation joint tran permutation
of the hose and the column ok so if you
apply any permutation of the node you
obtain the same the same distribution so
interestingly so there's there's some
representation theorem by I'll dues and
n Hoover so it takes the following form
for for exchangeable binary matrices so
if you make this exchangeability
assumption then you've got this
representation for the matrix so you got
xij given some uniform unenviable you
are you J and some function W from the
unit square 201 so xij just
from a Bernoulli distribution with the
probability of connection w of you I you
J ok so there's some function call it a
literature the grifone and so xij given
this function and give a uniform
variable it's just been we distributed
with probability of connection given by
W of uiuj and so so as noted in a recent
review by all bonds and hoy so you've
got several network models of asian
version of the network model that fit
into these frameworks for example the
erdos-renyi stochastic block models mix
membership stochastic block models by
Jean nonparametric version of stochastic
block model etc so that's fine so does
those model can capture a lot of
structure within the network okay so
however so that there's that there's
only one limitation which is that if you
make this exchangeability assumption
then you've got one consequence which is
that graphs that are represented by an
exchangeable matrix they are either so
trivially Mt all they are tens okay so
either you've got the empty graph or you
obtain necessarily dense graph and so to
quote so the the recency of a paper of
all bonds and also made this this this
remark so the theory clarify the
limitation of exchangeable models so it
shows for example that most asian models
of network data are in army specified so
so me specified in the sense that they
necessarily lead to dance graph and we
believe that most of the real world Rafa
actually as possible so how can we enter
these them so so some people what do
people in the literature do so of course
so you can still work with you
with your adjacency matrix and just give
up infinity exchangeability because if
you have infinite actually beauty then
you've got densities for example you can
work with non-exchangeable generative
models such that the bar Abbas I'll bear
preferential attachment models which is
some sort of a rain falls young process
you you can also what people do so in
the frequentist literature is to to use
so to consider sequences of finitely
exchangeable models xij of n so maybe
the most common is to use some sort of a
specification of the of the glovephone
so you now assume that for sample size n
your xij of n are drawn from a Bernoulli
distribution so where the probability of
connection is this graph on the value of
uiuj times some scaling parameter ho n
where this ho n goes to 0 as n goes to
infinity and so by doing that you obtain
sequences of graph which are actually
sparse so the limitation of using that
so now you've got sequences of model so
so those model are finitely exchangeable
but they they are not consistent under
marginalization okay so it does not
define a proper generative network model
so if you interested in making
prediction what is the distribution of a
connection of a new node given the
previous one so you cannot it doesn't
really make sense to use to use this so
the approach we propose is liz is
different so we we we don't use this we
don't define a model on this district
structures on the matrix so now well we
will do so we use a different
representation for the graphs we will
represent the graph as a marked point
process on the plane and so so what is
interesting that we we we've got sort of
the similar property
as in the in the algebra framework so in
particular there's also representation
theorem in this for this point process
there's a representation by Karen before
jointly exchangeable pawn processes on
the plane and our construction fits in
this in this framework so we've got some
construction based on on completely
under measures and with the following
property so we've got a change ability
in this point process setting we can
have depending on the values of the
parameter we can generate graphs or data
sparse or dance with every 10 degree
distribution and an interesting point is
that so the model we define so we can
derive scalable in France algorithms so
how do we represent the graph so our
represented as a point process on on the
plane ok so I assume that each node has
some location theta I ok theta I in our
place and there's a point at location
theta I theta J either the connection
between ing ok so here this represent a
set of connection with this node theta I
connected to this node theta G and so
you so for the moment so you don't need
to care about the location actually sort
of the the the connectivity will be
actually independent of the location of
the object and I won't try to learn
those this location so what is the
definition of exchange ability for this
fall for this process so basically if I
take any regular grid on the plane of
length H basically and I count the
number of points within each box so it
gives me an infinite dimensional match
eggs then these infinite dimensional
matrix has to be exchangeable in the in
the previous as in the previous
definition ok so if I do any joint
permutation of the walls and the colon I
need to have the the same the same
distribution it has to be invariant and
this has to be valid for any value of
little H so as I said before so if I
make this assumption is that if I've got
this assumption of John tech gbg are
also have some more presentations around
you to calendar ok so now I'll describe
so this is how the process would be
represented so i will have these
properties and so now we describe our
what is the the network model how do i
generate a network so as I've said so we
will assume that the nodes they are
embedded at some location theta I in our
place so don't worry bother but those
location we won't try to learn them and
they do not tune the connectivity and so
I will assume that each known as some
sociability parameter WI so the larger
this parameter the more likely to notice
to connect to to other nodes and the way
I will I will generate so those WI tita
I so I will assume that those WI fita I
they they arise from a completely on the
measure so basically you've got a pure
jump here point process so where the
levy measure new of DWD teachers who is
decomposes as some lady measure that
will tune the here the germs and another
part that will just tune the location of
the of the nodes so here lambda is just
the lebesgue measure ok so the important
part is 0 here that will tune the
distribution here on the on those
sociability parameters and they are so
too important
case is to distinguish so the first one
is when the integral from 0 to infinity
of Worf DW is infinite then so we set an
infinite activity completely on DiMaggio
so we in any interval you will have an
infinite number of germs and if this is
bounded then you will have almost
release or finite number of germs in any
interval 0 T so if I if I think about my
network model so i will have basically
two cases so i observe a graph a set of
connections ok and so in the first case
basically I I have an infinite
population of any 3d population everyone
as his own sociability parameter but
only a finite number of them have a
sociability parameter that is large
enough so that they actually connect to
anyone ok so so I have only a finite
number of people who have connected to
someone and the an infinite number of
the others of no connections at all an
English in the second case I will have a
finite population so and so what we will
see that actually this difference
between a finite and infinite activity
will translate into a sparsity and
density of the of the royalty network ok
so this is the so each node has some
locations heater is on sociability
parameter WI and the way I construct the
network is just the following so the
probability that there is a connection
between node I noj is just so if I is
different from j is 1 minus exponential
minus 2w I WJ ok so the larger WI or WJ
the larger probability of connection
between two nodes and so I've
represented here the process so
graphically so here are represented so W
ok so with the germs that represent the
suitability of the different nodes and
so those two
individuals that we will connect with 5
t 1 minus exponential minus two so WI x
WJ and I've got a connection at this
point okay so so remember if my
complexion measure is infinite activity
i may have here an infinite number of
germs over any interval okay so so this
is the the model so now I'm interested
in in the in the properties of that of
that model and so so i defined the model
on the plane and so how will I grow the
size of the network so basically if you
look at any box here at 0 1 Phi square
you have almost surely a finite number
of connections okay even if you have an
infinite number of germs so basically
the size of the network increases when I
increase the size of this window 0 alpha
square and so I can have represented so
at the bottom this is my point process
represented the set of connection and so
here I've represented so the n alpha the
number of nodes depending on alpha and
an alpha e the number of edges okay so
an alpha and an alpha he so as I
increase the size here of this window so
I increase the value of the number of
nodes and number of edges and so I've
got here to counting processes and so
now so I'm interested in what is the
what is the behavior as alpha tends to
infinity of n alpha e of the number of
edges corresponding to the number of
nodes and so so interestingly so if you
if you assume that soho that shows the
distribution of other germs is non zero
and and you have got some basically that
the the expected value of the sum of the
sociability parameter over any interval
is is bounded then you've got the follow
aggressors so that the number of edges
scale quadratically with the number of
nodes if I've got a finite activity
completely on a measure and its scale
sub quadratically so I've got a sparse
graph if w is infinite activity okay so
in a sense if you use a sort of a
Beijing non parametric model where
you've got an underlay infinite number
of parameter you're in the space regime
otherwise you're in the UN the dentary
and so so to make it slightly more
precise oh so if you I can take so what
I use for the experiment is the
generalized gamma process where you got
two parameters parameters Sigma and the
parameter to hear so this is the levy
intensity and this parameter Sigma in
particular is the most important so you
can take the range between minus
infinity and one and this process so is
infinite activity for Sigma greater or
equal to 0 and so you obtain basically
the oil range of behaviors for Sigma
greater or equal to 0 you obtain sparse
graph and and the graph becomes parser
Arcilla increases and you obtained ends
graph when Sigma is is below zero so
interesting for these changes
interestingly for this class of
processes so you can sample exactly the
graph via some young process and you
obtain also a power law degree
distribution for for Sigma strictly
positive so here are just some
simulation from from the models on the
top left is the just for comparison
elder chronograph with 1000 clothes and
probability of connection of 0.05 so
where you obtained some sort of a dense
graph and here's a simulation from the
model so here for gamma process so when
Sigma is equal to 0 and here you you
increase the value of Sigma Sigma is
equal to 0.5
0.8 so as you increase the value of
Sigma you obtain grabs at our spacer and
spacer and with a view of you the same
degree distribution okay and so and if
you look at now at the degree
distribution for Sigma strictly positive
you obtain those poverty redistribution
so you can look at those three curves
here the the red curves i correspond to
a generals game a process with sigma is
equal to 0.2 0.5 0.8 so this is the same
type of picture as I've shown before so
on the log log plot the degree
distribution and so basically the value
of symbol will tune the the exponent of
the of the polar degree distribution so
that the exponent be higher as you
increase the value of of Sigma and the
parameter 2 is also interpretability
will tune to the cutoff in the tears of
the degree distribution okay and so this
is a simulation from the model so as you
so we stop from the upper left corner
and so here I'm just generating from the
model on the left and on the right so
our represent the associated graph okay
so here I've got the sociability
parameters the set of connectivity and
so there will be a hub appearing at some
point with a hike on purgatory so here
goes along
okay so this is so generated from with
the generalized gamma process with Sigma
which is a strictly positive so you
obtain let's pass sparsely connected
graph okay so it was for the father
model and and and and its property so
now what do we want to do with that so
if you want to make in France so what do
I observe so I just observe obviously
the set of connections between
individual I don't observe location and
what I want to what I want to learn so I
want to learn the sociability of each of
the individual for which have observed
at least one connection and so I may as
I've said I may have an infinite
population of individual with no
connection at all and so I want to learn
w star which is the sum of their sochi
abilities okay so i want to learn the
stability of the person for which
absorbs self connection sociability of
all the rice and some set of parameters
so in permit in particular so alpha
which tunes roughly the size of the
network sigma which will tune the
sparsity and this parameter to that some
exponential cut off and so the way we we
derive the sampler so we introduce a set
of a latent count variable so one for
each edge and we derive so three step
Markov chain Monte Carlo sampler so
we're in the first steps we update the
weights the sociability weights given
the rice using a Hamiltonian Monte Carlo
then we update the total mass so this is
just a scalar ND I parameters using
metropolises things and then we update
the latent counts given the rice is just
using your tongue cated possible
so the two bottlenecks are one and and
three and actually those steps can be
implemented very efficiently so here
you've got the gradient is very cheap to
compute and so here are some
illustration so I i simulated a graph
with this set of parameters and I
obtained and network with 14,000 nodes
and 76,000 edges and so I run mcmc some
players with three chains and 40,000
iteration and just to say that ad it
scares relatively well so here it takes
just 10 minutes on on standard desktop
with with matlab and with not a lot of
engineering and so here are the some
sort of trace plot of the photo
parameter alpha parameter Sigma 2 and
the sum of the sociability parameters
for all the other nodes or showing that
we managed to recover quite well the
unknown parameters and so okay and so
here so some posterior credible interval
for the for the nodes on the left it's
the node with the top top 50 with the
highest degrees okay and on the right
the nodes with the lowest degree so
because the graph is actually power low
all of those nodes are optically one and
so here's on the log scale so just to
show that we managed to to get quite
accurately those those credible
intervals so both for the I degree nodes
and for the load agreed on okay so we we
use this model to to try to assess the
sparsity of real-world network so what
we do so we we use our generalized gamma
process and we tried we learn this
parameter Sigma and we report so the
probability that this parameter Sigma is
greater or equal to 0 so based on a set
of absurd connection and we do that so
for 12 different network so where the
is the number of nodes or ranges from a
thousand to three hundred thousand nodes
and up to a million edges so here just
here examples I don't think I'm good so
it is the largest is the set of a world
wide web connection you got internet
Hooters the animal email data set etc so
there's a large variety of networks so
just to say that in terms of time so so
it's very very quick for the for the
lowest for the small network so it takes
12 up to 10 minutes 44 30 year thousand
nodes and here two hours for for the
largest network and so we report here
the probability that Sigma that the
sparse is graph under the model and some
confidence callable intervals here for
four Sigma so what we are 95 so we are
on tify sparsity for so we we identify
that some of the networks are out then
so for example this one is a small
social network so you expect that it's
quite densely connected this one is a
network of political blogs in the US so
again you expect that there are a lot of
connections there's four others so you
obtain clears path cities for example
for the a neuron data set and here for
the world wide web data set so we've got
quite surprising results for example for
the for this the internet network where
we obtain a sigma between minus 0.2
minus zero point 17 so here clearly so
the internet network is believed to be
sparse oh there there there must be some
misspecified in our model that you that
and we cannot really capture this this
network okay so I've present in that so
a new class of statistical network model
that builds on exchangeable Honda
measure so that can capture so sparsity
power properties for which we derive
scalable inference
algorithms so at the moment the mother
is very in a sense is very simple it
only captures or sparsity because you
only have one parameters that Thun the
sociability of a node so what we what I
currently want to do is to thwack Stan
so within this framework to extend to
most structure models so using some sort
of no- factorization block model version
of this class of model introducing
cavalia dynamic networks etc so here is
the plot of a preliminary work on now
introducing you don't have a single
parameter legends of stability but you
would have a set of three parameters or
presenting maybe your interest in three
different categories etc and so you
could model some sort of a block
structure in the network so the end the
code is a available online if you want
to to try it thank you okay we're
running a bit late has anybody got any
any questions here I mean one thing that
I personally was sort of be interested
in it was the fact that you way that
you're setting this up do you have this
redundant of theta there is really quite
an interesting parameter in fact it's
got some sort of metric there you can
sort of start scaling up so if you've
got the units being you know if I get a
secretary to do an order for me most of
the time so so there will be a sort of
you could build lean as in into these
systems something which which which
actually had another process which was
associated with with clustering the
Poisson type process stuff that you've
got there which has a completely
different interpret
in terms of scaling up which might I
mean okay will be quite interesting okay
I didn't look at if it would be too much
yeah the frame that you've got ya 1 1
direction won't eat well is to introduce
covariate would be to to make now the
sociability dependent on the location so
the location would be a cetera yeah
covariance
hello friends my name is Tushar and
today we're going to talk about how to
find all simple cycles in a directed
graph using Johnson's algorithm so what
is the cycle a cycle is a path in a
graph such that the first and the last
word existed what is the simple cycle a
simple cycle is a cycle in which no
vertex is repeated twice except the
first and last vertex so in this graph
here we have bunch of simple cycles
example h98 or 1 2 3 1 so the idea is to
find all such simple cycles using
Johnson's algorithm so there are five or
six other algorithms to find simple
cycle but I found out that Johnson's
algorithm is the fastest of them also
for Johnson's algorithm I expect my
beavers to know how strongly connected
components work in another video I
already talked about how to find
strongly connected confident using cosas
Rajas algorithm and I am also going to
create another video about how to find
strongly connected component using
Tartans algorithm but just for revising
it quickly so a strongly connected
component is a subset of vertex in a
graph such that all the vertices are
reachable from each other so in this
graph here we have three strongly
connected component one is H 9 because 8
&amp; 9 are both reachable from each other
another one is 7 and the third one is
consisting of the remaining vertices
because all these vertices are reachable
from each other also for the purposes of
Johnson's algorithm I have numbered
these vertices going from 1 up up all
the way to 9 it is not strictly required
but this is how the algorithm is
described in the paper so this is how I
am going to explain it in this video so
next let's see how a Johnson's algorithm
works first thing we do is we divide
this original graph into sub graphs
where each sub graph consists of
vertices and edges which are for a
strongly connected component since we
had three strongly connected component
there we have three sub graphs here so
why we do that because a cycle is always
going to be restricted with a strongly
connected component there is never going
to be
cycle it stands for multiple strongly
connected component so for example this
edge here is never going to be part of
any second why because if it was it
means that either from 8 or 9 there is a
way back to 1 and which which would have
matched that 8 and 9 would be the parts
a strongly connected component as the
rest of this vertex and since they are
two different strongly connected
components which tells us that there is
no way back to one which means that
which means that edges like this this
and this would never be part of any
cycle so once you have done divided into
three sub graphs then we look for the
least number H least number what X so
the least number what X here is one so
first we are just going to work on this
sub graph this is this strongly
connected components of graph also I
have stack blocks set and block map map
data structure and as we run through the
algorithm I'll show you how this data
structures are you so what started what
X will be 1 which was the least numbered
vertex in this strongly connected
component so basically we are looking
for all the cycles which start and end
at 1 so then we do a DFS starting at 1
so we add 1 to the stack and 1 to the
block set and now you're going to
explore neighbors of 1 so 1 neighbor of
1 is true who is not say the start
vertex which means that we have not
found a cycle 2 is not part of the block
set which means that we can explore 2 so
we put 2 into the stack and 2 into the
block set then be able to explode
neighbors of 2 so 1 neighbor of 2 is 3
so again 3 is not same as start vertex
which means that we have not yet form a
cycle and 3 is also not part of block
set so we can explore 3 so we put 3 into
the stack and 3 into the block set then
they explode any course of 3 1 neighbor
of 3 is 1 and that table is same as
start vertex which means that we form 1
cycle and the content
the cycle is the contents of this tab so
let's record this cycle so the cycle
will be 1 2 3 1 then we come back to
three and then we explore other
neighbors of C 3 and see if we can find
more cycle so one neighbor of 3 is 2 but
2 is already part of this blocked set
and since we are just looking for a
simple cycle anything other than 1
cannot be repeated so we cannot continue
in this path of 2 so we come back to 3
and then explore another neighbor of 3
so one more neighbor of 3 is 4 so 4 is
not going to start vertex 4 is not part
of block set so we are going to add 4 in
woods tag and block set and explore
neighbors are for
one neighbor of 4 is 5 so 5 is not same
I start vertex and 5 is not in block set
so we will add 5 into stat and block set
and then explode anywhere is a 5-1
neighbor 5 is 2 not 2 is not same a star
vertex but 2 is in this block set which
means that we again hit a road black
road block and we cannot proceed in that
direction because to gain on what X 2
cannot be repeated again in the same
cycle so we're going to come back to 5
and explore other neighbors of 5 but 5
have just 1 over 2 so what that means is
all the way are going to recur start
from 5 I am NOT going to remove fry from
the block set why because the way things
says stack is set up correctly there is
no way in the current DFS traversal that
you could find a cycle which starts
which which has 2 in it and also which
goes to 5 because 5 is going to h2 but I
am going to record that if we ever
unblocked to then I am going to unlock 5
so basically if we look for
neighbors of five and add five to their
unlock to the block map so if two ever
gets unblocked at that point of time
island block five so as you see I left
five in the block set then from five we
come back to four four has no other
neighbors and we did not find a cycle in
this current path so similarly just like
before we leave for in the block set
will remove it from the stack and once
you find the neighbors of 4 which is 5
and add 4 to the blocked map so just
saying like if five ever got unblocked
then be able to also unlock 4 and then 4
we come back to three now three has one
more neighbor 6 so be able to traverse
there 6 is not same a start vertex and
sticks is not same as block side so we
add 6 to both of them now 6 has one
neighbor for now 4 is not saying a start
Ward expert 4 is times this block set
which means that we should not explore 4
and this is how we do optimization since
we did not remove 4 from this block set
which means that we save this acceptable
cell where we go from 4 to 5 and 4 5 to
2 and then realize the 2 is already in
the current stack and then come back so
by not removing 4 and 5 from the block
set we provided this optimization so
anyways but we have to note that if
forever gets unblocked then we should
also have lock sets so 6 has nobody else
will we go back to 3 so we remove 6 from
the stack but not from the block set now
equal to 3 so 3 is done exploring all
its neighbors and it did find a cycle in
one of its neighbors so 3 be able to
block 3 because there is a possibility
that there could be a future cycle going
from 3 to 1 since 3 found attached to 1
it means that there could be some other
path which could lead from 3 to 1 so we
have to
three so I had removed three from the
stack and I'm also going to unblock
three and then I'm going to check this
dog Mac to see if by blocking three do I
have to block any other vertices and
none because there is three is not part
of any key so then from three we go back
to two because that's where it came from
now two has no more neighbors so now
since we did find a cycle from 2 all the
way to 1 which means that we have to
unlock two to open up the possibility
for future cycle so we will go to unlock
two and since now that we've unlocked -
we have to go and check is anyone else
waiting to be unlocked because of two so
five was so we go to 5 and see if anyone
else was waiting to be unblocked because
of 5 and 4 was to recursively go to 4
and if it was anyone else ready to be a
block because of 4 6 was and no one was
waiting for 6 so first we remove 6 from
here unlock 6 and remove this particular
entry then you go to 4 remove flow from
this unblocked block set and then delete
this particular entry and then we go to
2 and 2 is already removed from the
block set but we also got to remove 5
and then remove this particular entry so
as you can see by the time we were done
with 2 and then also going to move two
from the stand so by the time we were
done by 2 now we have to unblock 4 5 6
all this other vertices why because now
that 2 is no one in this tab there is a
possibility of a cycle going through 6 5
and 4 and leading to 1 so now we come
back to 1 and 1 has another vertex 5 so
we are going to add and find it's not
part of this block set and why because
since to garden not removed 5 which
means that we open up a possibility for
another cycle going through 5 and now
that 5 is not part of block set we can
explore
five so we add fire to the stack five to
the block set and then you have an
exploding where's a five so neighbor of
five is two and now we'll repeat the
process like before so neighborhood five
is two and two is not Paris attack and
true is not same and start word X we add
to here to here and then here of 2 is 3
so 3 + 3 + 1 aber of 3 is 1 it means
that we found a cycle and and the
contents of the cycle will be this 1 5 -
3 1 so we found cycle here so now we
explore other neighbors of 3 1 neighbor
of 3 is 4 &amp; 4 is not exploding 4 is not
saying start node X so we add 4 here
another neighbor of one neighbor of 4 is
5 but 5 is already colored blocks set
which means that we cannot exploring the
direction of 5 because 5 is already part
of this time and it's a simple cycle we
are looking for so we go back to 4 leave
for in the block set but mark that if 5
ever gets unlocked then we will unlock 4
then 4 for big and then we go for from
this tack but not from the block set
from four we go to 3 3 has another
neighbor 6 we add to the stack and to
the block set 6 has another neighbor for
but 4 is already the block set so we did
not find a cycle from 6 all the way to 1
so but we are going to mark that if
forever gets unlocked then we will
unblock 6 so leave it says in the block
set and remove 6 from this tag and go
back to three now three has explore 3
has another neighbor 2 but 2 is already
caught a block set so we come back to 3
so now 3 has explored all its neighbors
and it did find a cycle going so it did
find a cycle leading to 1 so we have to
unlock 3 if you never find a cycle so
ever find a path leading to the start
vertex yes that block that vertex to
open at the possibility for future work
future cycles so we remove 3 and you can
got 3 and then we check doesn't doesn't
one else needed blocking because of
three no one else does so we go to two
now since we found a cycle in this path
from two so we able to unblock to remove
two from the stack and see does anyone
else need Sun blocking and no one does
so from two we go back to five and we
unblocked five and remove fry from the
stack and also see if anyone else needs
unlocking because of five yes we do
because for was dependent at fight then
locked which means that we will remove
four from here and then we will see who
is dependent in four and six was so we
will remove six from here and we'll also
remove these entries so finally we come
back to one and now one has no more
neighbors to be explored so basically
after all this we found all the cycles
which starts and ends at one and so
there are two cycles with start and ends
with one so after we are done dealing
with one the next thing we do is we
remove one from my original graph as if
one never exists existed before so we
remove one we move all the outgoing and
incoming edges from one and just delete
one and acts like one ever existed and
then repeat the entire process again so
let's do that quickly now that one is
not part of the graph we still have
three strongly connected component we
again look for the list numbered vertex
which is two so where new start vertex
will be two and will repeat the entire
process again so let's do that very
quickly
so two into the stack two into the block
set neighbors of two will be explored so
definitely three three will be go here
will be going here and three will go to
the block set neighbors are three will
be explode one neighbor is 2 so it is
say the start vertex so we found one
cycle which will be two three and two
then you go back to three and then
explore another neighbor of three which
will be four so we add flow to the stack
and four to the block set and explore
equal to 4 which will be 5 so we add
five stack and
and then fry the end up at two so again
you find another cycle and the contents
that cycle will be two three four five
two and then be records back from five
since you found the cycle from five
there's a possibility of future cycles
if we remove from the block set and
stack and then you go back to four since
we found a sec on this path we remove
four from both blocks set and block and
stack and then we go back to three three
has another three has another vertex
another neighbor 6 so we explored that
so we add 6 to here and 6 to here and
from 6 we go to 4 and 4 is not a block
set and now you can see why the moving
forward was so critical because if you
had not removed four from the block set
we would have never found this this
cycle going wire 6 so we remove the S 4
is not X is not a block set to be high
it we add it then for from for a good 5
it's not there so we added and then 5s
neighbor is 2 so we found one more cycle
so the content of that cycle will be 2 3
6 4 5 &amp; 2 &amp; then makers back so we found
a cycle so we unblocked 5 for the
possibility of future cycle then we go
to 4 then block 4 for the possibility of
future cycle then we go to 6 we unlock 6
for the possibility of future cycle then
we go to 3 so at this point of time we
have explored all the neighbors of 3 so
we're and since we found cycles be able
to unblock we haven't unlocks 3 and we
will remove from the stack and then we
end up at 2 and now to has no more
neighbors so again we are done finding
all the cycles which starts with 2 which
is 2 3 2 and this one and this one so
now again we go back to this
you know original graph act like to
doesn't exist and then repeat the entire
process again let's do that very quickly
with two gone these are the remaining a
strongly connected components here so
there is no reason to pick a sub graph
with just one vertex because there is
the possibility of the cycle because
we're assuming there are no self loops
in this graph so then next we pick the
least number what X which has at least
two vertices in the sub graph and there
will be eight so with eight we'll repeat
the same process so it goes into the
stack and blockset and explode neighbors
of it which is 9 so 9 goes here and here
and then explode any person mine which
is one of them is 8 and it is also
starting vertex so we found another
cycle and the second will be 8 9 8 and
then and then we removed them from the
stack as where occurs back and then we
are done with it
so once you're done with it will will
act like it doesn't exist in this graph
so then we will just be left with then
we'll just be left with a strongly
connected components or with the sub
graphs whose size is just one at this
point of time we are done with the
algorithm so in this graph here we found
six different simple cycles let's
analyze the time complexity so the time
complexity is o of e plus v into c plus
1 where c is the total number of cycles
in the tour number of cycles in the
graph so we can find strongly connected
component in a plus we time which is why
we can guarantee that between every
cycle the worst time which will pass
will be e plus we but remember we can
have exponential number of cycles which
means that the total time complexity can
still be exponential so but the only
guide is that
recycle the worst time which will pass
will be II trust V and the space
complexity also will be e plus we so
next let's look at the code for this
let's first look at all the data
structures used in this algorithm one of
them is block set which is a set of
vertices which when blocked will not be
explored another one is blocked map
which is a map of vertex to the set of
vertices so when this vertex in the key
is unlocked then all the vertices in the
values should also be unblocked and then
stack holds the current DFS tag and then
all cycles is a list of list of vertices
so in this inner in this inner list is
is going to is representing one cycle
and then the outer list is the list of
all the cycles so basically all cycle is
going to store my final result the main
function is simple cycles it takes in a
directed graph and then first we are
going to do is initialize all the data
structures and then our start index will
be one and you remember or the index
indices or the vertices are valued
numbered from 1 all the way till Hauser
many vertex we have then we are going to
use stargell algorithm to find strongly
connected component
so while start index is less than the
total number of vertices we have which
initially will be true first we do is we
create a sub graph from the original
graph such that it only consists of
vertices which are numbered greater than
or equal to start index so anything less
than start index will be ignored so
initially start index is 1 so we will
include the entire graph then in this
sub graph we are going to find all the
strongly connected components using
target's algorithm then we are going to
pass this SCC s and the sub graph to
least index SCC so basically finding for
the least index vertex among all the
strongly connected components so this
method here first creates a graph for
each of the strongly connected
components and then among them looks for
the least index vertex also it ignores
any graph which has just one vertex in
it because it will never have any cycle
so this might or might not return a
this might or might not be present so if
the least index vertex is not present
then we break and then we are done with
algorithm and return all cycles
otherwise if it is present then we get
the least index vertex clear the block
set and block map because they may be
populated from the previous iterations
and then find all the cycles in this
particular in this particular strongly
connected component graph which list
index least vertex is part of so this is
the start vertex and then this is the
current vertex and then finally we look
at this method in a bit and then finally
we increase our start index to be at
least least index plus one so when we go
back here and then when we create a sub
graph next time we are going to ignore
everything which is less than start
index and this is also we discussed
previously in the video so let's look at
find cycles in s e-g which is the main
part of the algorithm so here the first
is the start vertex and second is the
current vertex so first we do is we do a
DFS here phone cycle is false we push we
push the current vertex into the stack
and then we also push the current vertex
into blocked set then we are going to
explore all the neighbors of current
vertex so we get one neighbor at a time
if this neighbor is same as start vertex
it means that we found the cycle so we
are going to push the contents of the
stack into this list cycle and add this
list cycle into all cycles because all
cycles is storing our final result and
then we are going to mark the found
cycle as true because we found a cycle
in this particular path by going wire
this current vertex
otherwise if neighbor is not same as
start vertex and if also if neighbor is
not in the blocked set then we can
explore this neighbor so we again go
into the recursion with fine cycle in se
G keeping the start vertex s same and
for the current vertex passing neighbor
and then we and then we what and then it
repeats the same process and then if we
did find a cycle in this
then we then we sent the found cycle to
be true so basically if for a current
works a current vertex if any of its
neighbors finds a cycle in its path then
found cycle will be true for the current
vertex so once we are done with the for
loop and all the recursion
if found cycle is true then we unblock
the current vertex and this unblocking
is not just an drawing the current
vertex but it is also checking the
blocked map and recursively unblocking
all the other vertices which are waiting
on the current vertex so let's look at
unblock very quickly so we remove vertex
u from the block set and then records
and then getting all the vertices from
the block map which are waiting on you
and then going through one each of them
and removing and recursively and
blocking them as well so as we discussed
in the video this is recursively
unlocking all the vertices so once we
are done and blocking if then we are
done unlocking and if the found cycle is
not found so basically if there is no
cycle in this path then we are not going
to end block this vertex this current
vertex but we are going to add we are
going to find all the neighbors of the
current vertex and add this current
vertex to the blocked map so this is
because if they ever got unblocked
then we are going to unblock the current
vertex as well and this is also we
discussed in the video before and after
this we are just going to remove the we
are just going to remove the current
vertex from the stack and then if cycle
was sawn divisionary will turn to and if
cycle was not found then widget Falls
and this happens recursively
so finally fine cycle comes here and
then we come back to here and then the
increment over start vertex and all and
then all cycles will have the final
result let's quickly run this code so
this is the same graph as we discussed
previously in the video so let's look at
the result of this graph so here this is
the list of all the cycles found in this
graph so I can totally endure
and this video is little in this code is
little involved so I would recommend is
you copy this code and run it and see
how the entire code is working so
finally the time complexity for this
algorithm is o of e plus we into c plus
1 where c is the total number of cycles
the reason we have a strong bound here
is 2 because 1 we can find strongly
connected component in o e e+ we time
and second because we are doing all the
blocking of the vertex and unblocking it
only when required
we are saving on the useless fruitless
and needless searches and also the space
complexity is o of e plus v so this is
all i have to talk about johnson's
algorithm please like this video share
this video comment on this video check
out my facebook page facebook.com to
sharra 25 and check out my github link 8
sub-commission peace interview wiki
thanks again for watching this video
oh it's a great honor to be invited here
to decay my second tank in Korea but
first time I'd only insult or swimming I
see the rest of the country okay so I'll
be talking about a problem in common
talks the ER dish discovery problem that
I worked on a few years ago and actually
managed to solve and it's it the year
the solution ended up being rather
interesting I thought it is a problem in
combat works but to solve the problem we
needed to introduce ideas from number
theory from probability even information
theory so it was many ways that I find
this person but more interesting than
the problem actually some ways oops that
pause not what I wanted to do okay say
good ok so yes I just go be like a good
ok so the earliest description II
problem was one of the big open problems
in discrepancy theory so discrepancy
theory is the study of how balanced you
can make sequences of various types of
so this is the simplest type of type of
discrepancy theory concerns just comes
the assign patterns so for me a sign
pattern which is a sequence of sines
plus or minus 1 ok so here is a sign
that plus 1 minus 1 minus 1 plus 1 plus
1 ok so a fine pattern if you like such
a function from the natural numbers 1 2
3 4 5 - the science minus 1 plus 1 ok so
of course the lobster most design
patterns and most of them look kind of
random if you write down the sign pads
no Plus Ones and minus ones you expect
about half as many plus one so yeah half
the time you should have plus one half
times u minus one and so your sequence
should be very equally to you're evenly
distributed in practice so more
precisely what I would mean is that
uniform sums like I for example if you
sum the first n elements obvious sign
that electrical you take the first seven
elements of a sequence and you sum them
up you get three plus ones and four
minus one so you get a fairly small
number you get minus one and that's
because this sequence is failing
balance it has fairly low discrepancy on
this on this interval of n7 so that when
you sum up all these when you sum up
beat n signs together you get something
a lot less than n because there's a lot
of cancellation okay so we call the
discrepancy of the sequence how much
love it is I mean there's different
types of discrepancy but we roughly
speaking the discrepancy of a sequence
it sort of measures how much
cancellation there is in the sequence
what you do that you add up all the
elements equal in some sets such as this
initial segment of a sequence and you
see how large the sums are and and these
sums week we call the discrepancy and
when the basic questions is how low
discrepancy can you make a sequence like
how balanced can you make a sequence
like works as small as you can make
something like this and and this is the
topic of discrepancy theory it has some
applications if you want to do like
numerical integration Monte Carlo
integration you also want to pick
sequences that have that have very low
discrepancy to minimise errors but I
won't be focusing on applications in
this talk of the pure math talk okay
so alright so there are many types of
discrepancy and many discovery questions
here's a simple one so suppose you have
them some sequence I say a finite
sequence let's say n different signs
plus 1 minus 1 minus 1 plus 1 and you
look at all the partial sums you use
some of the sequence up to f1 f2 up to
some m and you can all the partial sums
and you want to make all the partial
sums small at the same time so you look
at all the partial sums you look at the
absolute value of the partial sums and
you take the biggest partial sum and you
want to make this quantity as small as
possible so we want your like all the
sums small and how small can you do this
so the second easy question to answer so
first of all so one thing we know now in
in comatose
ever since early introduced the probably
sig method in 50s 1950s is that if you
want to understand what's going on for
say as a sequence of signs or a graph or
something you should first understand
what happens for random objects like
random sign questions random graphs
whatever
gives you a lot of insight so one thing
you can try is you just pick the signs
random that you flip coins you know if
its head you pick plus one minus one to
the tip through flip you get a random
sequence of all coins upside and then
you can compute this this this sum and
in fact not hard to compute you can use
something in poverty equal to turn off
inequality or the hosting inequality
it's lots of inequalities you can use
and the inner qualities from probability
theory which will tell you that if you
flip n random coins and you get n random
signs normally at least this discrepancy
these partial sums websites about square
root of n okay so the basic facts are
for political sinner this is usually
scored square root cancellation that
when you sum n random things you often
get something else eyes square root of n
so that's what you you do that's what
happens you have a random sequence of
discrepancies ruffling root n times a
constant but in this particular problem
right the random sequence is not the
best sequence there are much lower
discrepancy sequences and the simplest
thing to do is likely just taking out
the ending sequence plus 1 minus 1 plus
1 minus 1 so that's not random and this
has much better discrepancy so if you
take the partial sum of this sequence
like a first partial sum which is 1 and
then 1 minus 1 that's 0 and then 1 again
and then 1 minus 1 plus 1 minus 1 is 0
so all the partial sums are just 1 and 0
and so the discrepancy here is just 1
and that's clearly the best you can do
these is there's no way you can make a
sequence like this which is obviously
better than 1 so this is the best
sequence you can do for this problem and
this is a very simple example of what
was it some random sequence the sequence
which it behaves better than a random
sequence is more balanced than a random
sequence so random sequence is often
very good for getting reasonably low
discrepancy but sometimes sub random
sequences do even better and big and sub
random sequences are very useful in
numerical algorithms because again I'm
not talking about applications in this
talk ok so that's a very simple problem
now let's look at a slightly different
problem so again we start with a sine
pattern of n different signs plus 1
minus 1 plus or minus 1 now previously
we some partial sums we only sound
some partial sums from f1 up to up to
some FM but now we consider now we ask
for a much stronger notion of low
discrepancy so not only do we some
partial sums along initial segments of
the sequence but that's sum over every
single arithmetic progression that we
can find in a student so if you take the
sequence
you pick any arithmetic progression
inside its length example F of 3 F of 5
F of 7 you look at the sums on every
single progression and you ask can I
make every single progression small so
can I make f3 plus f5 is ff7 small while
also making you know f10 + have 2530
small and so on but make every single
you try to make your European your sign
pattern as balanced as you can on every
single ethnic progression at the same
time ok so again if you try a random
sequence plus the minus ones at random
for the coins you'll find that this
discrepancy is still about square root N
in size so okay but is this the best you
can do is square root of n the best
conductor so if you try the previous
example plus 1 minus 1 plus 1 minus 1 so
that sequence was very good very low
discrepancy if you only take partial
sums but now if you allow a thank for
questions that's a very bad sequence
because if you take for example the
progression F of 2 F of 4 F of 6 8 and
so put all those numbers R minus 1 and
you get a very big partial sum so that
sequence doesn't work so ok and it is
hard actually to make a sequence where
all these sums are small and there's a
good reason for that there's a few in
the stops you
oops do that again sorry
it's the wrong button control ok ok now
in over that Nikita ok I won't need you
next time we do that all right
ok so there's a famous theorem in an
area convert or exclude all sizes for
the other date ok so there's an area of
mathematics court called Ramsey theory
and one of the first fumes in Ramsey
theories and got van der Vaart Mysterium
from 1927
and what the theorem says is that if you
take the natural numbers one two three
four five six seven and you paint them
different colors so if you pick some
colors see red blue and green or and you
color some of the natural numbers read
some of Elmo's green ISM and and so
forth
then among these different color classes
one of these classes must contain a lot
of arithmetic progressions in fact
radical progressions are of any length
so that's a basic theorem in Comal
Toric's and one of the consequences is
that so you look at a sign pattern X
equal to plus or minus 1
that's like coloring the metronomes in
two different colors the plus one color
and the minus one color and so Van
Nevadans theorem tells you that no
matter how you color the natural numbers
either the plus 1 color or the minus 1
color it will contain as many questions
of any length which means that this
discrepancy here when you sum your
sequence over ethnical questions this
these sums must become unbounded you
must be able to make some that
arbitrarily large okay so the larger end
gets the the larger this expression must
get and in fact we know how exactly how
much be summed are going to be that
there's a there's a fairly famous room
of Ross so what's the same for many
things you want the field Medal for book
well ok for several theorems and but one
of them is just a discrepancy theorem
how in which he actually computed how
big this discovery can be and he said
that actually it must be as large about
into on quarters times a constant so
random sequences give you n to 1/2 and
you can do better than that but you
can't do any better than to the 1/4
use now which uses technique which we
now call the second moment method and
there's a much later as there's an
example of Matus a concern Spencer that
shows that in fact this is the best
possible that you can you can actually
construct a sub random sequence which I
said this this supremum is a constant
multiple of n 1/4 and so this is the
right answer
okay so so what are we found out so if
you make a discrepancy using only
initial segments you can get bounded
discrepancy fairly easily but if you
measure this company along all a
thematic regressions you can unbound a
discrepancy so in the 50s original in
the 30s so I polish asked what happens
if you consider an intermediate problem
suppose you study science hasn't and you
again major discrepancy but now you only
consider what I got homogeneous as a
catatonic progression so progressions
where the starting point a is the same
as the as a step size so like 10 20 30
40 is a homogeneous arithmetic
progression but 11 21 31 41 that's an
elephant progression but it's not a
homogeneous Atlantic regression so he
asked what happens if I can only look at
homogeneous as may progressions what is
what is the best discrepancy you can
make here so I think he was motivated by
questions in number theory because
because sounds like is do show up in in
another number theory so I thought these
sort of sums include the partial sums
with it so they but that but they don't
include all the ethnic progressions so
there's all halfway between the two
problems that I already mentioned sorry
okay all right so the yeah so this is
the problem here
yeah so you only have an initial segment
so you can make this this distance is
bounded if you use all Asshai
progressions that description it must be
unbounded but if you want to use
homogeneous as microaggressions what
happens here do you have founded this or
unboundedness of this discrepancy and so
they thought opposed the problem and he
believes that that this discrepancy must
be unbounded but as you make the sign
tight and longer and longer you must be
able to make these these sums a bigger
and bigger you cannot keep this all the
some gears small simultaneously okay so
it's a simple problem good actually it
was really hard to make progress on the
problem yeah so another way of phrasing
the problem is that for any constant C
you must be able to make any constancy
you must be able to make this these
discovery larger than C as soon as your
sequence is long enough so the month
exists from n such that for every sign
pattern of F and this discovers these
bigger than C that's that's another way
of phrasing the discrepancy problem okay
so you can start studying the problem
for very small values of C so for
example can you make this to premium
bigger than it can you force the supreme
entry bigger than one so that's very
easy the moment you have just one side
plus or minus one you can just take that
number every one and that number of must
have size one and so your discrepancy
must always be at least one the moment
you have even just one term in your
sequence okay now what about two
okay so suppose okay what what does it
how long do you need to make your
sequence before the discrepancy has to
be at least two so it takes a short
while okay so forth so here's a sequence
of eleven signs plus 1 minus 1 minus 1
plus 1 and so forth and one can check
it's a bit tedious but this sequence
actually has discovers you won it so if
you take any event any homogeneous
progression they've example a 3 &amp; 6 so 3
&amp; 6 C minus 1 and plus 1 add up to 0 so
minus 1 alone it gives you minus 1 minus
1 plus 1 gives you 0 minus 1 plus 1 and
minus 1 give you minus 1 again you never
get plus 2 a minus 2 if you look if you
compute all the partial sums on the
sequence on any homogeneous progression
you'll find you only get 0 1 and minus 1
so this is the sequence of length 11
where the discrepancy is always only
only 1 and this turns out to be the
longest that you can do so at the moment
you have a 12th sequence of length 12
this discovery must be at least 2 so
maybe I'll actually show this to you the
fun little argument it's a lot like
solving a Sudoku puzzle if you actually
so yeah if there
identical sequence and through the 12
sign patent this one and the answer's no
so we can see it's just okay so let me
do a low table here okay so what is a
sign patent you have you have 12 numbers
are right in our space barely and you
have you want to fill in you know these
boxes are like filling it the boxes of
Sudoku puzzle and the rules are for
every arithmetic progression for example
two four and six the sum of the numbers
in down here can only be minus 1 plus 1
or 0 it cannot be plus 2 a minus 2 or
anything worse than that so now just by
filling in a sequence ok so one it can
be plus 1 or minus 1 actually without
loss of generality alcohol plus 1 if
it's not past when you slip all the
signs so let's let's call it plus 1 ok
now what about F of 2 well F of 1 plus F
of 2 cannot be two it just comes to the
only one so this copy plus 1 it must be
minus 1 ok now 3 we can't say legacy
right now because it could be either one
so it's ok but for we can do something
with because F of 2 plus F of 4 can't be
minus 2 so the four has B plus 1 ok
alright and then similarly for an H give
you minus a minus 1 over here
okay now now we can say some glass 3 c1
f1 f2 f3 f4 cannot add up to 2 so this
can't be plus 1 it's mostly minus 1 so
this is minus 1 and then 3 &amp; 6 that
gives you a pop on over here now 1
through 6 can only add up to a Kannada 2
plus 2 so 1 minus 1 minus 1 plus 1 this
has been minus 1 because otherwise this
without of titute ok 5 and 10 I give you
plus 1
okay seven okay so if you okay so 1
minus 1 minus 1 1 minus 1 1 - coming - 1
but this would have - two - two - it
will be plus 1 similarly this one has be
minus 1 otherwise this town would be too
big so let's say 6 is plus 1 which would
make 12 minus 1 and now I think should
be done okay 51
[Music]
Thank You the - to ever done okay
calculator I haven't got a contradiction
yes Donna okay I have a Q minima fixing
it but 3 6 9 is that the book thank you
yes yes okay thank you very much yes
okay so yeah so this sequence is a
progression whether just companies - -
okay and so that's that that gives you
that gives you your contradiction okay
so it is you know it should remind you
of sort of so solving one of these two
door coupe puzzles
okay so that was kind of fun so you can
try what about three
okay so what would it take to mix
descriptiveness three so it was only in
2014 that we know they figured out the
answer so conditional incisions in 2014
first of all they discovered a sequence
of them while 1160 so that I'm not going
to show it here but there's a signal
self 1160 signs for which this discovery
sees only two then note that that it's
known which is very very sub random
actually but more amazingly as they show
that was the best possible that at the
moment you have one one six one assigned
somewhere in that in that sequence you
have to the discovery somewhere must be
it must be two must be three actually
they kind of just be two so in a sense
what they did was it was a giant version
of this was pseudo goo gang except that
it's a lot harder so this is kind of
like a two set problem you know what was
that it's mystical like a three-step
problem yes oh so what happened here is
that often once you want you use some of
them solve your sign patent you could
did you could fix just one choice for
before the other for another sign patent
for another sign but he and I you often
get two choices and you have to do a lot
of branching so they actually made a
massive computer calculation using a
specialized prehaps over to show that in
fact the discrepancy for all of us any
sequence of m416 one must have learnt at
least discovery at least three okay so
actually for a short time this sequence
this this proof they had was had the
world record of being the world's
largest proof because the certificate
for the proofs that showed that there's
pre-tied calculation was correct with
was 13 gigabytes big long which which at
the time was what was the what the
largest we haven't ever made although
they later simplified it to only eight
hundred fifty megabytes and then later
on though the completely different
calculation which was the 200 terabytes
so okay so they don't have the world
record anymore but yeah but but until
they did this it was not known whether
you could have a sequence of this
currency to or not that yeah all right
so let's discover two three
so description must be eventually at
least three or about four so kind of the
sit has tried to compute what happens
before so they were able to constitute
to construct a sequence of fairly long
13900 signs but they're just going to
always say at mostly all the partial
comes with always between minus 3 and
plus 3 but they did not know if it's
what the largest sequence so now the for
that problem
it's really not not and this size is
really not beyond not within the venture
called modern computers and so it was
not known actually whether the sequence
actor discovers a at least four
eventually ok so this is what I managed
to do actually about two years ago so I
actually did solve the problem that that
have a discovery gel discoveries are
always unbounded that you take any
sequence of natural numbers or sign
patents eventually the ecology news
comes must be bigger in magnitude than
any number you wish so in for instance
for okay so one of the reasons why this
problem is hard is that it's it's sort
of the answer is almost yes I mean it's
an accident almost note that there are
actually lots of sequences that are
almost founded discrepancy right so okay
so not not the random sequence okay if
you pick random signs plus minus one we
pick a sequence of length n random
random sequences have discovered to you
about root n is another one this journal
of inequality or law of large numbers
type calculations but you can actually
do a lot better than log n so then they
root n that there are sequences that
have much better discovery than than
than root n so you know couple is this
length 12 sequence already studies of
random when we just go and see 11 see
consistency 1 or this length 1 1 6 0
sequence of discrepancy 2
so what are some non random sequences of
low discrepancy so here's one example
which actually it isn't sub random I
think we don't believe it is so random
it's it's about the same size as the
random function but this is a very
important function later
so one example to consider comes from
number theory so there's a very
important number theory function which
is a sign patent which is what's called
the Lubell function lambda it's a
function defined to be plus 1 when n is
the product of an even number of primes
and minus 1 when you're the product of
an odd number of primes so 4 is 2 times
2 that's 2 Prime's so lambda 4 is plus 1
30 is the product of 3 prime to 5 in 2 3
&amp; 5 so lambda 30 is minus 1 okay so it's
a sign - plus 1 minus 1 minus 1 plus 1
so it has a special property
it was completely multiplicative so the
special property of this function is
that when you take lambda of any product
ok so it's still easy to see in the
definition and so one consequence of
that is that if you try to sum this
function on on any progression you can
you can pull out lambda of a so this is
lambda papers I'm a to a tool and ok a
this is you can just pull out an active
round of a from the Sun and this
expression is just a plus or minus just
a partial first partial sum of all of of
lambda yes sir because this formula
and of a and to a something under ka
just act in fact Design Group A plus
number one okay so if you want to
understand the discrepancy of the
sequence on how machines have a
questions you just need to understand
partials up so the questions so the
basic question is how fast do the
partial sums of the Louisville function
grow okay how about this is going K so
this turns out to actually be a major
open problem so of course the first case
the first case arms they I think they
grow by the can't go faster than K
certainly and we do know that they go a
little bit slower than K because of
basically in number theory called part
number theorem
it's believed that the secret behaves
randomly so that this this this these
partial sums should go like square root
of K just just like they do for a random
sequence give or take a log or something
that's really hard that doesn't have to
be equivalent to the Riemann hypothesis
okay the fact of the partial sums that
we believe the partial sums go like root
K but we don't know that we conjecture
that as a reminder purposes but what we
do know that we can show that that they
must go at least as fast as root k so it
is known that that there are infinitely
many K for which this sum is actually
bigger than the square root okay for
those number of us in the audience that
that's because the Riemann zeta function
is at least one non-trivial 0 and we
don't know on the whole new life I look
on the critical line we think all of
them are non critical line but we don't
know that but we know at least one of
them is and that's enough to just give
you the lower bound okay so this little
function is not a low description
sequence but only because there's a view
of the zeta function on the critical
line so that's kind of weird because you
know this question is a comment or
question and this is number theoretic
fact somehow shows up at least for this
function
okay so that is not a counter example to
the discrepancy problem because thanks
to this 0 of zeta function but there's
another example which which is which is
better which IBM Monaco d'Orsay
characters which are also important in
number theory so identify character say
Chi is a sine pattern or more generally
you can take complex numbers as well but
let's say plus minus 1 which is
completely multi-coated just like the
Beluga function but it's also periodic
so it also repeats itself but it doesn't
necessarily take varies minus 1 plus 1
it is allowed to vanish and in
particular it is supposed to vanish if
you if for numbers that are co-prime to
this sorry numbers with a not co-prime
of the title to is clear it ok so I'm a
good example of deviously characters is
this guy called Chi 3 this is the
sequence which is plus 1 when n is 1 mod
3-1 win is to mark 3 and 0 when you're
zero mark 3 so this character looks like
this
[Music]
plus 1 minus 1 0 plus 121 0 plus 1 okay
so this is a directed character and
these guys have very low discrepancy so
like if you take any partial sum of this
of the sequence you always get plus 1 or
0 or plus 1 or 0 you only ever get plus
1 in Europe and you're completely
multiplicative so so once you know the
partial sums also on any aspect or any
homogeneous as a progression these these
characters also have a no discrepancy so
yeah so every single partial sum here is
either 0 1 so this is a very low
discrepancy sequence but it doesn't it
doesn't give you a counter example to
the discovery problem because because
they're zeroes here okay so so the the
right see it is discussed above only
considered sequences where you want to
allow plus 1 or minus 1 but the sequence
is 0 one third of the time ok so it's
sort of 2/3 of a counterexample to the
dispute of scripts we problem you know
2/3 of the time is plus or minus 1 and
such as currency is on 0 it's 2/3
because this is a character of you it's
3 you pick a character a period 11 it
would be like 10 over 11 of a
counterexample and koval you so using
digital characters you can make
sequences you know what I say you know
so 99% counter examples to to your
problem but they are plus minus 1 most
of the time but the discrepancy is
bounded and this is this is really your
first hint that this problem is going to
be quite difficult because so you're
going to have to somehow deal with with
these almost counter examples at some
point in your argument you can modify
this example a little bit so you can try
to replace the zeros by something by
plus or minus ones so this was
considered in 2010 by for wine short joy
in Koontz
so so
they considered a sign pattern which is
like kite three except so instead of
looking at the last digit the the last
digit of your basically expansion which
is what
Kairi does it looks at the last non-zero
digit of the basically expansion so if
you define T plus 1 if you are one more
three times the power of three and minus
one if you are too much three times the
power 3 so it's the same sequence at the
daresay character but you go place to
zero so this is 3 times 1 plus 8 plus 1
is 3 times 2 so 2 minus 1 this is 3
squared times 1 so plus 1 this is 3
times 4 and for one more 3 is also plus
1 and so forth so this is a sequence so
now with no zeros you've just filled it
your place with zeroes bye-bye sign and
the partial sums are now pretty is
pretty small so there's actually very
cute calculation if you compute the
partial sum if you sum of the first
three terms in this in this sequence
that's always equal to the number of one
in the basically expansion of your ok so
for example if you sum these three
numbers 1 minus 1 1 you get 1 and 3 is 1
0 in basically it has 1 1 and so this
you get a 1 here 4 is 1 1 in basically
so yeah and the partial sum is 2 and
then so forth so so there's a very cute
formula for the partial sum so the the
psalmist you go to how many ones you
have in the basically expansion ok and
that's unbounded okay as you make a
bigger you can have more and more one in
your basic expansion but it goes very
slowly the number of ones you can have
there's only like log K log basically
okay that's the most you can have so
that's as much that's very sub Landon
remember random sequences this reason
can go like root K squared okay is doing
okay so you can find sequences that are
almost bounded almost under discrepancy
they only go log logarithmically fast
and these examples all the examples we
know that are bad they all come from
dear sir characters usually characters
are the enemy in this problem so somehow
yet you you need to somehow isolate
these characters and somehow get rid of
them okay so it really suggests that
number theory has to be involved to
solve this problem because it is a very
number three of the cupcakes
okay so people have worked on this
problem for many decades so the first
big breakthrough really was about ten
years ago seven years ago in 2010 so
Timothy Gao is good friend of mine he
organized one of these things put
polymath projects a polymath project so
a polymath project is when you get many
many mathematicians all over the world
to collaborate on solving a single
problem over the Internet so you use
maybe a blog or maybe a wiki something
like Wikipedia and you get all these
people talking you know trying all these
different things and and and so not you
know even sewing are things that they
don't think are going to work because
maybe because even though they can't see
how to finish the Paul maybe somebody
else will be able to fix it and so forth
so so there were he had at some point of
a dozen or so people including myself
actually working on this problem for
several years so we advanced about two
or three years and it didn't actually
solve the problem completely
for example they found the sequence of
limbs like 1000 which is capacity to to
perfect shortly before kind of misses
the sitter and they had another very
nice observation so but one of the
things that they discovered roughly
speaking so this is this is simplifying
things a little bit but they managed to
show that to show some how the world
number theory that is Eiffel is problem
the most important sequences in this
problem were the completely inoperative
sequences like the daresay characters
and the lugal function that if you
wanted to solve the discrepancy problem
roughly speaking it suffice to check to
to solve it only for sequences that are
complete completely multiplicative if
you can solve it in that case then you
can solve it in general there's some of
this yeah here to prove something a
little bit stronger for these functions
but but let me not discuss these
technical detail okay so this leads to a
simplified version of the problem so
rather they take an arbitrary sequence
just like a multiplicative sequence and
ask for any multiplicative sequence
other of plus and minus 1 are the
partial
of a particular sequence always
unbounded so that's an easier version of
the problem now how you do this reaction
if you don't show how much time I have
maybe I will skip the description of the
program let me just say there it uses
tools and Fourier analysis
multiplicative functions you can think
of as like characters on the natural
numbers in some sense and Fourier
analysis lets you decompose arbitrary
sequences into characters and so you
basically break up an arbitrary sequence
into linear combinations of these
multiplicative functions and you do some
averaging but I think I will skip the
actual reduction okay so okay so the
model hoping now that you take a
multiplicative function such as the
Lubell function and the basic question
is you know can you show for example
that the sums of the partial sums of the
Lua function are unbounded so that that
is a basic model problem now in this
particular case as we saw before the
answer is we can because the Lua
function is - turns out to be connected
to the Riemann zeta function and you can
use zeroes of the Riemann zeta function
to answer this question but that's a
very special property of this one
particular function global function most
molecular functions have nothing to do
the zeta function and so you would like
a way to answer this question that
doesn't use facts about the geodata
function so this is this is a model
problem okay so you can take a public
view point so if so we believe that the
louver function behaves like a random
sequence that's one the basic beliefs in
number theory analytic number theory now
as I said before they take a purely
random sequence of 4-1 the partial sums
the first K sums should go like square
root okay and this is this is a very
standard computation in probability and
if you use tools in probability
in particular Senecal the second blowing
method to compute means and variances of
all small properties some what you can
find is that even if your function is
non random you give a sign factor which
is not random you can still show that
these sums grow maybe not as fast as
root K but you can show that they are
unbounded at least as long as you can
control covariances as long as you can
control these are the correlations so
you use some things like ever then times
a shift of F of n F of n so these are
correlations or called covariances if
you can control if you can make these
small if all these covariances are small
then you can you can use tools and
probability to show that this sum must
be big basically you can show that of
that somehow the variance of alpha of
certain randomly chosen sub zones of the
Sun a big so actually the real problem
is to understand these covariances so
did so then you are interested in
studying sums like that you take
Louisville function of N and Google can
say n plus 3 or n plus 2 and you want to
understand V so correlations now this
unfortunately is a very hard problem so
there's a conjecture of chawla in number
theory that says that these are these
sort of correlations should be small
that if you sum P sort of sum up to X
there should be little of X I mean it
that we divide by X is equal to 0 X goes
infinity so this is a this particular
claim is a conjecture chawla if this was
true this would imply at least the lumo
function that the partial these terms
are bounded
unfortunately this this this conjecture
is open it looks about as hard as
between prime conjecture so this this
conjecture is about asking whether NFS
age both have the same parity of prime
factors like both of even number of
prime factors of the honduran factors
and that that should happen about 50% of
time so what what the child conjecture
says and that's very close to the twin
prime conjecture which is asking whether
two numbers like n an N plus h with a
how often they both sometimes we prime
and that's open
so we can't prove that conjecture yet
but what I managed to do about two years
ago was I managed to to prove a weaker
version of this conjecture so if you
don't sum this conjecture you don't sum
these numbers directly from 1 to X but
you insert this weight you use
somebody's with quality lambda in that M
plus page when you divide by n then the
trivial balance of the sum is the sum of
1 over m the harmonic series which is of
size log X and how they will show that
that this there is a little bit of
cancellation in this sum and so this sum
is a little bit smaller than log X so
those some cancellation when you sum in
this way and that to that is still be
good enough ok if you can prove this
then you can show that the partial sums
on the lure function are unbounded and I
could do this for every every completely
modelkit function had it done like this
except the dear side characters a
difference.i characters do not do this
like if you look at your Chi 3 of n
times n plus 1 maybe some like this ok
this sum is actually about this sum
exactly grows like log X basically
because whenever this is plus one that's
already minus 1 right there's some
correlation between between these two
quantities because of the periodicity
but this is a non periodic function and
this doesn't have that periodicity so
some of the diversity characters were
the only characters for which you didn't
have this property but you can do a
literary character separately by a
separate argument so somehow they're the
worst enemy but everything other than
the digital characters they're okay but
they're the only characters I really
caused the most problems
ok so Louisville comes to example does
not cause any problems ok so I was able
to prove this this number theoretic
estimate which was like the key
ingredient in solving being of the year
the irish problem
okay okay so why this funny sub so you
know it's more natural to some numbers
mod X about anyways so why do you put is
1 over N wait so roughly speaking that's
one of it anyway it's like
multiplicative power measure so you know
when you integrate so often when you do
analysis and you want it you know
interview indicating a function say
since 0 to X it's awesome very natural
to insert and integrate a function using
teaching over T rather than GX is
multiplicative power measure rather than
additive are measure because this this
is ever gonna go has good probably
properties of Zagreb multiplication and
you know of course it's standard haar
measure has good probably specific to to
to translation but but this is good
probably some circulation and these sort
of questions actually like the more
model fricative than they are additive
so it's actually better to use them
something that looks like mor
applicative high measure that's sort of
roughly where this is one of the end
comes from okay so now he's gonna get
the technical part of this whole book to
explain how to prove this is just doing
it yeah so this is a once you decide to
divide by the sum by this 1 over N you
get this funny
so it's discrete so you know now for for
integrals you have a very nice change of
variables
okay so if you if you integrate the
station 0 to 2 2 to X F of T TT you can
always change variables you can dilate
by any factor Q and ok you can always
change this integral but you can scale
it by Q and you get this integral you
just take the changeable T prime good QT
ok so when you use multiplicative hard
measure you have this nice dilation
invariance now it doesn't quite work for
summation because when you take the
natural numbers motor X in mop up IQ you
don't get all the natural numbers 1 to X
you want to get the knobs of Q from 1 to
X so you get a slightly worse formula so
when you when you sum the numbers from F
of n over m into x you can dilate you
can event of a cube over N but you also
multiplied by this weird factor here you
could you only restrict to Mobile's of Q
and then there's a normalization factor
Q that shows up but apart from is this
unpleasant factor here this will
arithmetic factor here you have a
dilation invariant which is which is
sort of the analog of office continues
and variance here so you have this
violation invariance which combines very
well with the multiplicative properties
of the liberal function so the liberal
function has particular property because
its multiplicative lambda of T more
paren by any prime P lambda P n is
always minus lambda n because lambda
feminism is minus 1 to a number of prime
factors of n so we multiply P add one
more prime factor so you can use this
you can completely can combine these
these two expressions together and you
can use them to make this sum better so
the final thing about Alice's citizens
and analysis problem is that is that we
when you want to make us sound better
you make new usually what you have is
two definite uglier you have to make a
like this is only one summation but if
you can you can transform the sum to
have multiple sums in here well it's
just one something you have more
averaging and you can do more things so
you can start using this sort of
invariance to make this some other year
size of it but but but more tractable
analytically so what you can start doing
is that you can dilate okay so so lambda
n is negative lambda of n P and lambda n
plus a let's say we could use with
studying this on the end of n lambda n
plus 1 over N so you can dilate this Phi
P SS Phi P okay and you have to get this
and then you can do a change of
variables okay you can change n to 2 P
times n and what you find that you can
transform this song which correlation
involving one shift by one to a
coalition involving a shift by some
other shifts and polish FP for some
prime P times it's weird factor where
you have to restrict tube to multiples
of P but but let's ignore the second for
time being so this correlation involving
shift by one can be related to
correlation involving just by P for any
prime P and then you can average over
all those ships at once and what you can
find is that you can control the single
average to now a double average where
you only normally do average n from 1 to
X but you also have it over some prime P
average over Prime's P in some set and
you can pick what set you want I an
exactly important that you get to pick
words that you want so you can you can
change this sum into this stuff which
looks a lot worse okay it's a little
more complicated but there's more
averaging going on here and there's more
chances that you can make something work
when you're in analysis when you when
you have more averaging going on okay so
now we have to understand this double
sum now as it says the the problem is
that this is weird weight here which
depends on what n is doing what P and
that weight is is unpleasant but on
average this way is 1 if you pick a
random number n then it will be
divisible by P 1 over P of the time and
so this is this this weight here is you
can think of a random weight which on
average is all size what
now if we could replace this by one this
would be much simpler average so if
instead of this average we just had this
average here this turns out to be
something that we can understand this
double sum can be attacked by methods of
editing number theory including going to
something called the circle method and
to make a long story short there was a
big breakthrough by last year by marshal
mark in module two number theorists who
are able to show that these sums were
all small basically that so using a lot
of high-tech tools from from number
theory because you know sounds like this
in double sums were actually quite small
small enough that that for what we need
for this application okay so this sum
was was fine but the only problem that
we had this extra weird factor here
which involved the mod the virtue class
in most areas Prime's okay so the yeah
so the hard part is to understand what
the role of obvious of this extra weight
is so if you pick an randomly so the
Chinese remainder theorem tells you that
so if you can pick a number anyway at
random then n mod P will be distributed
at random uniformly between the right
two classes one p and you pick another
time cube the virtue classes in mod Q
also be distributed uniformly and the
Chinese remainder theorem tells you that
these these sort of residue classes
behave independently of each other so
these guys behave like independent
variables and now unfortunately at n
very these guys very very as well these
times the result but if they didn't very
if these are our fixed coefficients and
you only vary the and the only variation
came from these to be signed
then what you want you'll be doing that
you be something independent random
variable and this is going to get
published
note I understand very well and so if
you can use you can use tools from
probability so that say that if if these
signs will somehow fixed that they
didn't then on end over just some fixed
signs then this expression would
we usually be very small and there's a
specific toolkit happenings and politic
which you can use to favor that since
it's almost always very small that it's
only an especially small probability
there's only an extremely small set of
residue classes which will cause so in
which the some would would not be people
resources doubles on here now of course
these these sign patents are do depend
on n and that's very annoying but if you
look if you use the something in
equality what you can show is that this
this type of sum will usually be small
unless there is some weird times weird
correlation between this factor here and
this factor these sectors come out
conspire to do not cancel each other out
excited to cancel each other out so if
there's some sort of conspiracy that
that between actually between them so
what you can do is about any end you
look at all this you look at all the
values of Lua function near ends you
look at the sign pattern year end so you
take like H different sides to come H
like some like square root X or
something and you look at the residue
classes and mod p and these are two
different functions of N and these two
functions have to be highly correlated
in order for this this sum to be large
if these sort of to statistics behaved
independently you can use hoppings
inequality to show that this expression
has cancellation so the only only
problem is if there's a lot of
correlation and it turns out that the
correct way to measure correlation comes
from information theory in terms of
surprise
so there's a confident information we've
got mutual information so you can pick
any randomly and these two random
variables on one hand you'll assign
packing and on the other hand you these
read two classes and if there's a lot of
mutual information in a sense of Shannon
entropy between between these two random
variables then the sum can be large but
if they have low mutual information you
control some is small so the only
problem the only enemy
remaining if these sign patterns share a
lot of information that they're somehow
very biased depending on what the
greater costs of MRP are now we can't
stop that from happening unfortunately
it is possible that these signs are not
very sensitive to n mod P like maybe
when
is one mod mod mod xi these signs are
biased to be plus plug one and by an
accomplice one no three someone take the
same pattern but within the different
residue class maybe the bias to be a
different time patent there could
actually be some a lot of shared in
information but the last trick is that
these private peek drawn from some set
of primes which I quote script P and you
get to pick which set of primes to use
in your argument and it turns out that
each set of primes sort of behaves
independently and they and it is
possible that this sign patent could
share information one set of primes but
that uses up some of this entropy and
then there isn't enough it may not be
enough interview labs to also share
information with a different set of
primes and you can formalize all this
using what I call the Shannon entropy
inequalities that that each tips set of
prices is its own different sort of
carries it's also a separate amount of
information and and there's not enough
it's not enough entropy in the same
patent to to correlate with to share
information of all of these different
educators at once and by using actually
the pigeonhole principle as the final
thing you can find a set of fries for
which there is not much information
shared between the Stein patent and
because these were two classes and then
soap but he's using finally the original
principle you can get lower correlation
that this that the tools from
probability hot things and quality can
start helping you click it cancellation
and then that is finally what lets you
itself is not this problem so it's a
really I mean you know it really
surprised me when I was working on this
that more and more tools and different
areas of mathematics kept showing up to
solve the problem but yeah so so finally
we we actually able to to do it is to
finish off with this this feature so I
think up gimmick I talk
you
hello everyone my name is pleasure yes
Fraulein today I'm going to explain this
Johnson's algorithm for all play pairs
shortest paths so first of all we will
let us look into this definition then
representation so what is a graph or
graph is basically a set of vertices and
edges as you all know an undirected
graph is a pair of V and E V is what
this is at ease the edge and where V is
fine a set of points call what this has
an e is a finite set of edges so an edge
e belongs to e capital e is an unordered
pair so pair of you and we you and we
are both belongs to the set of it so we
have one point as U and one point as V
and the edge which are joining this both
the points R is known as e so E is a
belongs to edge family in directed graph
what happens H E is an ordered pair of U
and V so it is nothing but a vector edge
and edge UV is incident from victor left
for vertex V U and is incident to a
vertex V so an edge which is moving from
u to V is basically a directed graph
basically a directed edge and that graph
is known as directed graph a path from
vertex V to a vertex u is a sequence of
v-0 v1 v2 and all dot V caves so path is
basically a when we move from one vertex
to another so how fast is our sequence
of the vertices vertex of where v-0 is V
and VK is equals to u so V 0 is the
initial point in VK is the final corner
so VJ a VI comma VI plus 1 belongs to e
is a set of H belongs to e for l equals
to 0 to K minus 1 the length of the arc
is defined as the number of edges in the
path so we can define the length of the
path as the number of edges in that path
so the next definition and
representation so how we represent a
directed and undirected graph so
interacting graph is basically
represented with the outer word
vectored edge so the edge is simple and
it is not defined or it is not directing
to any other vertex so a directed is
basically opposite to that other so it
it is directed to some other u to V so
it is incident to you as well as V so
introduction first we will introduce all
pair shortest form like any
communication link like in the word
network or one of the most natural
algorithm question is how we determine
the shortest path from one point to
another so in this project we basically
deal with one of the most fundamental
problems of graph theory so how we wanna
solve it so basically there are three
distinct Sangam to face this problem
which is like the floyd-warshall
algorithm and all pair shortest paths we
have multiplication matrix
multiplication so we gonna means write
all the cost of the edges and then we
used to do some matrix multiplication to
solve that and the Johnsons algorithm is
basically what we are going to deal with
minimum spanning tree is so first we
define the minimum spanning tree a
spanning tree of undirected graph is a
sub graph of G that is a tree containing
all the vertices of them so in the
spanning tree what we do is like we have
all the vertices vertex and and a single
edge which is minimum as well and which
connects to all the vertex so innovating
graph the weight of the sub graph is the
sum of the weight of the edges in the
sub graph a minimum spanning tree is
like a for a weighted undirected is a
spanning tree with a minimum weight so a
vertex are there and the edges are
connected to all the vertex and and the
minimum one is the minimum spanning tree
so next are we going to use this one
minimum spanning tree so like we can see
yeah there are basically 1 2 3 4 5
vertex and each and every vertex is
connected to each other with edge and
each edge is a weighted edge so in order
to find the minimum spanning tree you
basically takes the minimum
H which is connected between the two
vertex so in order to find that we use
several algorithms like this truss and
others so finding the shortest path and
the problem divides into two related
categories and the basically one is
single source shortest path problem and
second one is all pair shortest path
problem so we here in this video we are
dealing with all pair shortest path from
single source shortest path problem is
directed graph consists of determining
the shortest path from fixed one source
vertex to all other vertices so in this
one these are directed path and we
determine the shortest paths from a
fixed vertex to others and all pair
shortest path is this is the distance
problem is that of finding the shortest
path between all pair of Firdous says oh
a graph so in next one all pair shortest
path a PSP SV its acronym given to all
pair shortest path is given her weight
and directed graph G when G contains a
set of vertex and edges and with a
weight function W weight is here's the
cost given to each and every edge that
map's edge to real value weight we wish
to find for every pair of vertices so
this is basically how to find we gonna
find it all pair shortest paths
so given mate graph G we Iike why W so W
is nothing but an integer variable in
decent value which is given to each and
which is the like the cost of each and
every edge the all pair shortest path
problem is to find the shortest path
between all the pair of vertices a
number of algorithms are known for
solving this phone so in Johnson's
algorithm is one of the basic algorithm
which is used for solving this one so it
finds a shortest path between all pairs
and in Big O of v square log V Plus ve v
square is the vertex and E is the edge
set of all the edges and V is the set of
all the vertex house fast graph it is as
simple e better than either repeated
squaring of mattis's or the
floyd-warshall cycle so it is partially
better than this in sparse graph in
floyd-warshall algorithm ITER returns a
matrix of shortest path weight for
of disses or reports at the Indies a
graph contains a negative weight cycle
so in this one what do you basically
implement two different algorithms side
by side like if in here we will
basically see a edge which contains some
negative values so in order so we cannot
directly apply this ra's al ghul so in
order to apply this curve we have to
change it to positive side so we first
apply bellman-ford algorithm then the
edges values will change to positive
then we apply this draws algorithm find
the shortest path so thank you for
watching this video michael eager will
continue with this one thank you hello
everyone my name is Vikram and I am
going to be continuing this presentation
on the johnson's algorithm and before we
get started let's talk about what the
johnson's algorithm is not about the
johnson's algorithm is an algorithm to
find the shortest path between all pairs
inner-directed weighted sparse graph
having a time complexity of B spare log
V Plus ve where V stands for the number
of vertices and E stands for the number
of edges for sparse graphs it is
asymptotically better than either the
repeated squaring of matrices technique
or the floodwall algorithm this
algorithm either returns a matrix of the
shortest path weights for all pairs of
vertices or it reports that the input
graph contains a negative weight cycle
Johnson's algorithm uses two algorithms
as its subroutines they are the Dijkstra
algorithm and the bellman-ford algorithm
now let us take a look at the steps
involved in this algorithm the first
step node Q is added to the graph
connected by zero weight edge to each of
the other nodes in the second step the
bellman-ford algorithm is used starting
from the new vertex tube to find for
each other vertex the least weight of a
path from u to V if this disturb takes a
negative cycle then this algorithm is
terminated next the edges of the origin
or graph re-weighted using the values
computed by the bellman-ford algorithm
and edge from u to V having length W of
UV is given the new length W of UV plus
H of U minus H of V finally for each
knot s the Dijkstra algorithm is used to
find the shortest path from s to each
other vertex in the riveter graph now
let us consider this following example
the first step we add a source s and add
edges from s to all of the worst reese's
of the original graph in the diagram
there s stands for for the second step
we calculate the shortest distances from
four to all of the vertices using the
bellman-ford algorithm the shortest
distances from 4 to 0 1 2 &amp; 3 are 0-5
minus 1 and 0 respectively as shown in
the figure above once we get the
distances we remove the source vertex 4
and we wet the edges using the given
formula that is W of U V is equal to W
of UV plus H of U minus H of V now since
all these weights are positive now now
we can implement the Dijkstra algorithm
to find the shortest path between all
pairs of nodes now we can now in to
check the time complexity of this
algorithm in different graphs is as
follows for dense graphs the algorithm
is ve log V when sparse graphs it is v
square log V which as can be seen is
much better than the floyd-warshall
algorithm in case of sparse graphs and
in case of the Dijkstra algorithm it has
a time complexity of L of V but when the
graph is passed
there is no e and it becomes v square
thank you for watching this presentation
now we'll look for at Dijkstra's
algorithm for finding shortest paths in
graphs with non-negative edge weights
Dijkstra is a famous figure in computer
science yeah and you can amuse yourself
by reading some of the things that he
wrote he was a very prolific writer and
kept notebooks and here really some of
his famous quotes now you need to maybe
know something about old languages to
appreciate some of these for example
COBOL was one of the first business
oriented languages programming languages
and it tried to make it so that programs
kind of read like English language
sentences but purists in computer
science such as Dijkstra really didn't
like that language as you can tell the
use of COBOL cripples of the mind it's
teaching should therefore be regarded as
a criminal offense actually when I first
came to Princeton was setting up the
department here one of there was a big
donor who actually I wanted to have me
fired and the Department closed down
because we wouldn't teach COBOL I didn't
know about Dijkstra's quota at that time
I wish I had and there's some other
opinions that Dijkstra had you can amuse
yourself on the web reading some of his
writings but he was here's another one
that's actually pretty relevant today
object-oriented programming is an
exceptionally bad idea which only could
have originated in California Dijkstra
worked in Texas University of Texas for
a while and of course came from the
Netherlands okay but Dijkstra did invent
a very very important algorithm that's
very widely used for solving the
shortest paths problem it's the one
that's in your Maps app and in your car
and many many other applications so
let's take a look at how that algorithm
works it's a very simple algorithm we
consider what we're going to do is
consider the vertices in increasing
order of their distance from the source
so in the way we're going to do that is
take the next vertex add it to the tree
and relax all the edges that point from
that vertex so let's talk about it in
terms of a demo and then we'll look at
the code so our mandate is to consider
vertices in increasing order of distance
from the source so our source is 0 in
this case
so what vertices are closest to the
source well we can look at the edges
that point from that vertex well start
out with vertex is 0 is that this and 0
from the source so we pick it then we
add that vertex to the shortest path
tree and relax all its edges and
relaxing all the edges pointing from 0
in this case is just going to update the
this to an edge two entries 4 5 8 &amp; 9 so
in this case this says that the shortest
path from 0 to 1 is to go along 0 1 and
its distance 5 from 0 to 4 its go along
0 4 and that's just since 9 and 0 to 7
let's go take a look 0 7 that's distance
8 and the edge weights are positive
we're not going to find a shorter path
to any one of what the sorry the edge
weights are positive and those are the
shortest paths that we know so far to
those vertices now the first key point
of the algorithm is take the closest one
if you take the closest one in this case
it's 1 then we know we're not going to
find a shorter shorter path to 1 the
only other way to go out of 0 is to take
one of these other edges and
they're longer so and all the edge
weights are positive so we're not going
to find a shorter way from 0 to 1 then
to take the edge there I want so that
means that edge 0 ones on the shortest
path tree that's what the algorithm says
take the next closest vertex to the
source and in this case it's 1 and then
we're going to add that edge that vertex
to the tree and relax all the edges that
point from that so now both 0 and 1 are
on the tree so now let's look at the
edges pointing to that and we have to
relax each one of those so in this case
if you go from 1 to 4 then we look at 4
we know a path of length 9 and 1/2 4
doesn't give us a shorter path so we
leave that edge alone
1 2 3 gives us a shorter path to 3 which
is going to be 20 because we went from 0
to 1 and then went to 3 is of length 20
and 1 to 2 also we didn't know what way
to tube and now we know now we know 1 or
better 1 is 17 so that completes the
relaxation of all the edges pointing
from 1 so now we have 0 and 1 on the
tree and we would consider next the next
closest vertex to the source so what we
have in this 2 is the shortest path to
all the vertexes vertices that we've
seen so far
and this one says that the and we've
been to 0 and 1 so that our next closest
one is 7 which is distance 8 so we're
going to choose a vertex 7 and again
that's the shortest path we've seen so
far we're not going to find a shorter
one because to get to every other vertex
is longer and so we know it's on the
shortest path tree and now we're going
to relax all the edges pointing from 7
in this case both of them the one from 7
to 2 gives us a shorter way to 2 then
what
new before and the one from 7:00 to 5:00
gives us a shorter way to 5:00 than we
knew before well we hadn't been to five
before so that relaxes seven and now
sevens on the shortest path tree so now
not it forced the next closest path
vertex to the source from the edge zero
four which is of length nine so that's
the one that we're going to pick next
for the tree we're not going to find a
shorter path there and we're lack we
relax along all its edges in this case
we find a shorter path to five than we
knew before and a shorter path to six
while we visit six for the first time
okay so that's four so now we just have
to worry about two three five and six
and five somewhere there so we select
vertex v relax along its edges in this
case those edges both give us better
paths than we knew before so now we're
left with three vertices and two is the
winner this is the two from the source
is 14 to 3 it's 20 into six is twenty
six we relax its edges and it gives us
again new shorter paths to both three
and six and then the last is next to
last step is to pick three and that does
not give us a new way to 6 and then
finally we pick 6 and then we now know
the shortest paths to all the vertices
from vertex zero if we just take the
edge two edges that's from two one you
take zero to five to take two and so
forth you get the shortest path tree and
that gives the shortest way to get from
the source to every vertex that's a
simulation of Dijkstra's algorithm
here's a demo of Dijkstra's algorithm
running in the large digraph so the
source is the vertex in the center with
the open circle and you can see it
considers the vertices in the graph in
order of their distance from the source
now you can see this is maybe like a
lake in the middle of the graph and so
it's not going to move on from those
vertices it's going to take a little
while to get around the lake and again
if this visualization is produced by our
code and it gives a very natural feeling
for the way that the algorithm works
this in principle and I think in fact in
many cases is what your car does when
you turn the map system on computes the
shortest path to everywhere and then
it's already when you ask for a certain
place know how to get there
so here's just starting from another
point in the graph you have if you
happen to live at a corner of the earth
and it's going to be a slightly longer
to get to the other corner and a nice
feeling for how the algorithm gets its
job done again when it gets into those
blank areas it's it takes a little while
to get over to the other side and of
course if we had Islands if we had
little roads in the middle there that
were not connected there'd be no way to
get to them from the source and we
wouldn't see them and that's fine not
for the way our algorithm works we just
leave that out of the demo and the
proofs to avoid adding an extra comment
about that for every algorithm
that's a visualization of Dijkstra's
algorithm on a large graph so so how do
we prove it's correct well essentially
we prove that it's an instance of the
generic algorithm so first thing is that
every edge is relaxed exactly once we
every time we put a vertex onto the tree
we relax all the edges that come from
that vertex and we never reconsider the
vertex and what does relaxation do
relaxation ensures that after the
relaxation either way that was before
afterwards you have the distance the W
is less than or equal to the distance to
V plus e to the plus the weight of the
edge either it's equal because we just
made it that way or it's less because it
was before and the edge is not relevant
and this inequality is going to hold for
every entry and for every edge
corresponding to every edge for this two
entries corresponding to every edge
because number one the this two values
are always increasing
we never I'm sorry are always decreasing
the only thing we ever change only
reason we ever change just two w is to
make it smaller if if we found an edge
it would make it bigger we ignore that
edge so it's always decreasing so when
we change this to W we're not going to
make that inequality false now we're
just going to make it better and this 2v
is not going to change at all once we
relax an edge from a vertex we're done
with that vertex we're not going to
process it at all so then when we're
done
we have the optimality conditions hold
that exactly is the optimality condition
and not only that we have a path from
the source to every vertex so that's a
correctness proof for Dijkstra's
algorithm
based on the optimality conditions
here's the code it's similar to code
that we've seen before
we're gonna use the indexed priority
queue that allows us to implement the
decrease key operation and we have our
edge two in just two arrays that are
part of the shortest paths computation
and the goal of the shortest path
computation so we initialized the
constructor initializes those arrays
including the index minimum PQ and we
start out with all the distances
infinity except for the distance to the
source is zero we put this source on the
priority queue and then what we're going
to do is take the edge that's closest to
the source off the priority and that's
on the priority queue off and then relax
all the edges that are adjacent to that
so using our standard iterator to get
all the edges that emanate from that
vertex and relax them and then the
relaxed code is just like the code that
we showed when describing relaxation
except that it also updates the priority
queue if the vertex that that edge goes
to is on the priority queue it gives a
new shorter way to get to that so we
have to decrease the key on the priority
queue if it's not a priority queue we
insert it and that's it that's a
complete implementation of Dijkstra's
algorithm using modern data structures
now this algorithm might seem very
familiar if and if you've been paying
attention it's essentially the same
algorithm as friends algorithm the
difference is that in both cases we're
building what's called a spanning tree
of the graph but in prims algorithm we
take a vertex that that's not on the
tree using the rule of let's take the
vertex that's closest to the tree
anywhere on the
three closest to some vertex on the tree
for Dijkstra's algorithm we take next
the vertex that's closest to the source
through a path that goes through the
tree and then into a non-tree vertex
that's the difference and that the
differences in the code have to do with
the fact that prims algorithm is for an
undirected graph Dijkstra's algorithm
for a directed graph but essentially
they're the same algorithm and actually
several of the algorithms that we've
talked about are in this same family
they compute a spanning tree I have you
you have a tree that takes care of where
that records where you've been in the
graph from every vertex back to where
you started and they use different rules
for choosing which vertex to add next
for a breadth-first search you use a cue
for depth-first search you use something
like a stack and then you just have to
decide what to do if you encounter
vertex you've been to before
but many graph algorithms use this same
basic idea so in particular when we're
talking about what the running time of
Dijkstra's algorithm it depends on what
priority queue the implementation we use
and it's the same considerations as for
prims algorithm we have v insert
operations every vertex goes on to the
priority queue V delete min every vertex
comes off the priority queue and for
every edge in the worst case we could
compute a decrease key operation
so the original implementations of
Dijkstra's algorithm used an unordered
array which would mean that it would
take time proportional to V to find the
minimum to find the vertex closest to
the source so the total running time be
proportional to V squared that's not
adequate for the huge sparse graphs that
we see in practice today like the map in
your car so the binary heap data
structure
makes the album makes it feasible to run
this algorithm and that's where all the
operations take time proportional to log
V we have to use the indexing trick that
we talked about last time to support
decrease key but still we get a total
running time of elog V which makes it
feasible and just as with prims
algorithm by using a implementation of
the priority queue that can do a faster
decrease key you can get a faster
algorithm and in practice something like
a 4-way heap is going to give quite a
fast algorithm that's a more expensive
to in certain to insert but much faster
to a delete min and decrease key and
again there's a theoretical data
structure that's not useful in practice
that gets the running time down to e
plus V log V of course if your graph is
dense and again the examples I use
they're not the array implementation is
optimal you can't do any better you have
to go through all the edges you might as
well find the minimum at the same time
but in in practice people use binary
heaps for sparse graphs I may be going
to four-way if the performance is really
critical bottom line is we have
extremely efficient implementations for
the huge graphs that arise in practice
nowadays
In the lecture, we continue our discussion
on Sequencing and Scheduling. In the previous
lecture, we had seen the assumptions, the
objectives.
and different types of sequencing and scheduling problems. We will now address each one of
the problems in detail, and see how we can
optimize or minimize the chosen objective.
So, the first problem that we will be looking
at is a single machine sequencing problem.
We take a numerical example, to explain few ideas in single machine sequencing. So, we
consider situation with 4 jobs, which we call J 1, J 2, J 3, and J 4. So let the processing
times be 6 10 8 and 6 units of time, these
units of time can be minutes, can be hours
and so on, so it is customary to keep them
as minutes.
Now, we want to try and sequence them send them on a single machine, where the single
machine is here and we want to send them,
for processing on the single machine, so that
we try and optimize or minimize a chosen objective. Now, there are 4 jobs J 1 J 2 J 3 and J 4
and these 4 jobs can be arranged in 4 factorial ways, in general, if we have n jobs,
they can be arranged in n factorial ways. Now, depending on the objective, one or more of
these n factorial sequences will turn out
to be optimum.
Now let us look at minimizing makespan.
Now, first let us explain the computation
of completion times and then we try and look at each of the objectives, now if this is
a machine and we try and send them in the
given order say J 1 J 2 J 3 and J 4,
now J1 enters at time equal to 0. So, the completion time for J 1 J 2 J 3 and J 4, J 1 will enter
at time equal to 0 and come out, at time equal to 6, so J 1 comes out at time equal to 6.
One of the assumptions is that the machine can process only one job at a time. So, the
machine becomes available at time equal to 6 and then J 2 goes and comes out at 6 plus
10 equal to 16 time units J 2 comes out.
Now at time equal to 16 the machine is free.
So, J 3 enters, so 16 plus 8, 24 is the time
J 3 comes out, so at time equal to 24 the
machine is again free J 4 enters takes another 6 units of time comes out at time equal to 30.
So, if we consider the sequence 1 2 3
4, which is the order, in which the jobs are
going inside, now the jobs come out at time 6, 16, 24 and 30. Now, as I said there are
two important assumptions one is the machine is not idle at all and a machine can process
only one job at a time. So, we also make an assumption that the machine
is not idle or the machine is not kept idle,
when there are jobs waiting in front of it.
Now if we look at Makespan as the objective Makespan is the time, at which the last of
the jobs is completed. So, Makespan for this sequence is 30, for a given sequence J 1 J
2 J 3 J 4, the sum of completion time is 6
plus 16 plus 24 plus 30, which is 22 plus
426 46 plus 30 76. So, sum of completion times equal to 76 and mean completion time equal
to 76 divided by 4, which is 19 is the mean
completion time.
So, given a sequence J 1 J 2 J 3 J 4, we can now calculate the Makespan, we can calculate
the mean completion time, let us assume that, the due dates for these 4 jobs are 10 15 22 and 30.
So, if we follow the sequence J 1
J 2 J 3 and J 4, the completion times are 6 16
24 and 30, we will assume that the due dates are 10 15 22 and 20 for these four jobs.
So, the completion times are 6 16 24 and 30, now job one comes out at time equal to 6,
it is due date or time, at which its due is
10, therefore it is early, job 2 comes out
at equal to 16, it is the due date is 15.
So, the tardiness is one here the completion
time is more than the due date, so tardiness is one here the tardiness is 2 and here the
tardiness is 10, so total tardiness is equal
to 13, for this So, what we have right now
done is for a given sequence, we have explained, how we calculate the 3 objectives, once the
completion times are calculated all the objectives now depend on the completion times, some of
them depend on the due date.
So, the given process times, you would call
this as p j process time of job, we have now
given the processing time p j of job j,
we have now computed here the completion time of job j, once the completion time and due
dates are known, we can calculate the objectives. Now, your optimization problem is what is
the sequence that minimizes Makespan? What is the sequence that minimizes, mean completion
time or sum of completion times? What is the sequence that minimizes total tardiness and
so on, we will now address these objectives in a little more detail.
Now, as long as we do not keep the machine idle and as long as the machine can process
only one job at a time, whatever order we
sent these jobs, the last of the completion
times is going to be the sum of the processing times, which is 30. Whether we send it in
the order 1 2 3 4 or whether we send it in
the order 2 1 3 4 or any one of the 4 factorial
ways by which, we can send this job, the Makespan is going to remain the same.
In fact, the assumption that the machine will not be kept idle, when there is a job waiting
in front of it, would actually help the Makespan or alternately, if we are looking at Makespan
as the objective then does not makes sense for us to keep the machine idle, when there
are jobs waiting in front of it. So, as soon
as the jobs, all the jobs are available at
time equal to 0, so the machine starts processing 1 job after another and in whatever order,
we send these jobs, the total Makespan is
going to be 30, so Makespan is equal to 30.
So, minimizing Makespan on a single machine is not a very difficult optimization problem,
any order any one of the n factorial sequences will give us the same Makespan, which is equal
to sum of the processing times. Then we move to the total completion time or mean completion
time, now we ask ourselves the question, if
we send the jobs in this order 1 2 3 4, the
sum of completion time is 76 and the mean completion time is 19, what is the order,
in which we have to send these jobs, such
that sum of completion times is minimized.
Mean completion time is sum of completion times divided by the number of jobs, so whether
we minimize sigma C T or whether we minimize M C T, it means the same, because you are
dividing by n and in this example, we are
dividing by 4. So, what we want do is if we
want to minimize this 76, now let us go and
find out, how we got this 76, 76 was got by
the sum of 6 plus 16 plus 24 plus 30.
Now how did we get this 6 16 24 and 30,
now the moment, we chose an order 1 2 3 4, 6 is the processing time of the first job 16 became
processing sum of the processing times of
these two, 24 becomes sum of the processing
time of these three and 30 becomes sum of the processing times of these four. So, essentially
we want to for the order sequence, such that sum of these numbers are minimized,
now when we want to minimize the sum of these 4 numbers,
it only makes sense to minimize all the individual
numbers. Because the first job that we pick, which
is, which has processing time 6 finds, it
is place in here, here, here and here, so
whatever job, we fix as the first job, it
is processing time is going to contribute
to all the 4 completion times, whatever we
pick as the second job in this example 10.
Now, it is contribution is going to be from
here and here, whatever we pick as the 3rd
job, it is contribution will be from here
and here, whatever is the 4 job will contribute
here. So, if we want to minimize the sum of completion
times, we wish to minimize all these 4 of
them and whatever, we are going to chose here
is going to contribute 4 times, whatever we
are going to fix here is going to contribute
3 times and so on. Therefore it is only correct that, we use the smallest of them first, which
contributes in all the 4 completion times,
then next smallest comes second, which contributes
in 3 of the completion times and so on.
Therefore, we arrange the jobs in increasing
order of processing times and in general non decreasing order of processing time
and this is called shortest processing time also called as S P T. So, arrange the jobs in the increasing
or non decreasing order of processing times to get the sequence, now the processing times
are 6 10 8 and 6. So, we arrange them in the order J 1 J 4 J 3 and J 2, now the moment,
we order them in J 1 J 4 J 3 and J 2, the
completion times are C 1 has a processing
time of 6. So, this is complete at time equal to 6, at
time equal to 6, the machine takes up J 4
takes another 6 units of time do finishes
at 12, at 12 it takes up J 3 takes another
8 units of time finishes at 20 and at 20,
it takes up the last job, which is J 2, which
has a processing time of 10 and this will
finish at time equal to 30. Now, sum of completion times is 6 plus 12 18, 18 plus 20 is 38, 38
plus 30, 68 and mean completion time is 17, in this example, the optimum value of mean
completion time is 17. So, the shortest processing time rule minimizes
sigma C T or M C T mean completion time, this is an extremely important result, which finds
it is way into other sequencing and scheduling problems. More importantly this is introduced
us to the rule, which is called shortest processing time rule, where we try and arrange the job
to form a sequence, such that the processing times are in increasing order or non decreasing
order as in this example. Now, we look at the third objective,
which is minimize
sigma T j, we have already defined T j, T j is defined as the tardiness associated
with the completion times and due dates, so tardiness T j of job j, so T j is maximum
over 0 coma C j minus D j. Now, C j is the
completion time or time, at which job j is
completed, D j is the due date associated
with job j, now if the job completes later
than the due date, then it is tardy in such
case C j minus D j will be positive.
So, maximum of 0 comma C j minus D j will
be positive and tardiness will be positive,
if on the other hand, the job completes ahead of the due date, then C j minus D j will be
negative, because C j the completion time
will be smaller than the due date.
So, when the job is early, which means C j is smaller than D j, then C j minus D j becomes negative
on the tardiness associated with job j becomes 0.
So, tardiness of job j is maximum of 0 comma C j minus D j and total tardiness is given here,
now the tardiness has two terms, which is the completion time term, as well as the
due date term. Since we want to minimize the total tardiness, we would like to begin with
we would like to arrange or sequence the jobs, in non decreasing or increasing due dates
and hope that such a sequence will turn out to be optimum.
So, first evaluate that, so the rule here
that, we try to use, this called E D D,
which is call earliest due date,
where jobs are arranged in the order of increasing or non
decreasing value of the due date. Now, let
us apply the earliest due date rule to the
example problem and first compute the total tardiness.
So, now, based on earliest due date rule,
we first sort the jobs according to the due
dates, so the dues dates given are 10 15 22
and 20, so job 1 comes first job 2 comes 2nd
job 4 comes, 3rd and job 3 comes 4th. So,
based on the earliest due date rule the order
will be J first 1 J 2 2nd J 4 3rd and J 3
will be the 4th. Now, the moment, we find
out this order the completion times are J
1 starts at time equal to 0 and finishes at
times equal to 6, J 2 starts at time equal
to 6 finishes at time equal to 16, J 4 starts
at 16 and finishes at 22 and J 3 finishes
at 30 the last of the jobs always finishes
at the Makespan. Now, due dates that we have here, so the due
dates are for job 1 the due date is 10, so
due date is 10 15 20 and 22, now let us find
out the tardiness associated with job j, now
here, the completion time is 6, the due date
is 10, therefore it is early. Due date is
15 completion time is 16, so tardiness is 1,
due date is 20 completion time is 22, so
tardiness is 2 and due date is 22 completion
time is 30, so tardiness is 8, so sigma T
j for this example is 11, so total tardiness is 11.
So, it is very customary to use the earliest
due date rule to try and minimize the total
tardiness, for example if we had used, we
showed some calculation here. So, when we use the rule J 1 J 2 J 3 and J 4, we found
that the total tardiness was 13, the earliest
due date rule gives us a total tardiness equal
to 11. Now, only difficulty here is that while,
here we can say confidently that the shortest
processing time will always give the minimum value of sigma C T or M C T we cannot say
that E D D rule is optimal to minimize the
total tardiness, so E D D is not optimum always.
Now, there are examples, through which we
can show that the E D D rule is not always
optimum, though in our example, we have calculated a total tardiness of 11, which actually turns
out to be optimum for this particular illustration. But, E D D need not give us the optimum solution
always, then we get into the next question
of, can we get to the optimum solution always
and if, so how do we get to the optimum solution always.
Because, the 2 earlier objectives of Makespan and total completion time, we have said that
for the Makespan any sequence is optimum, so I could possibly write here and say any
sequence or every one of the sequences is
optimal, which will gives us a Makespan of 30.
As far as minimizing sum of completion times, we could say that shortest processing
time rule minimizes sum of completion times, earliest due date rule tries to minimize the
total tardiness, but it need not give us the
optimum solution always.
Now, in this case, if we need a method to
find out the optimum solution always then
that method would either explicitly or implicitly, it has to evaluate all the 4 factorial or
in general n factorial sequences that are
possible and then only it can gives an optimum solution.
Sometimes, it evaluates explicitly,
sometimes, it evaluates implicitly, because
n factorial becomes very large, when n becomes large for example, if we look at n equal to
20, the number of computations is extremely large, when we want to enumerate n factorial.
So, we would look at methods that would implicitly evaluates a part of n factorial, but tell
us there, it has a evaluated that and then
by evaluating fewer than n factorial, it could
give us the optimum solution. So, several
branch and bound algorithms, try to do that
by implicitly evaluating some of the solutions, but n factorial is very large and these sequencing
and scheduling problems become hard problems. Particularly, when the problem size increases,
when we, so far we have not found an algorithm that runs in other than n factorial explicitly
or implicitly, because the computational effort to find out n factorial becomes exponential
and very large as n increases. Therefore in
these class of problems, it is customary to
look at thumb rules or what are called dispatching rules, to try and get the best solution.
Earliest due date is one such popular dispatching rule, though earliest due date does not guarantee
the optimum solution, in the case of the single machine tardiness minimization. Now, we have
seen three objectives with respect to single
machine problems and at the end of it two
very important rules have come out, one is
called the S P T rule the shortest processing
time rule, the other is called E D D rule
the earliest due date rule, now these 2 rules
are also going to be used in later scheduling problems, such as job shop scheduling.
While the S P T provides an optimum solution, to the problem of minimizing sum of completion times,
earliest due date is an extremely popular rule to address problem that have due date
related objectives or due date related measures, E D D does not guarantee the optimum solution.
Now, there are a couple of other things, which we would look at for example, instead of minimizing
total tardiness, if we try to minimize total
lateness, where lateness is simply L j is
C j minis D j, if we want to do that, then
we could realize that, if we want to minimize
sigma C j minus D j. D j’s are known, so it is enough to minimize
sigma C j and we already know that S P T minimizes sigma C j, so S P T also minimizes mean lateness
or total lateness. And there are other rules,
which would say that, if S P T or E D D gives
only one tardy job then it minimizes the number of tardy jobs and so on. So, there are some
interesting rules and algorithms, which are
available, but then we have seen a small sub
set of it and at the end of it I would say
here that, at the our discussion on single
machine problems. We realize the two important rules have come out, one is called the S P
T rule, the other is called the E D D rule
or the earliest due date rule,
now we spend a few minutes on minimizing in parallel processors.
On parallel processors, now if we assume that, there are two machines instead of 1 and they are identical.
Now if we want to minimize
the Makespan, we have 4 jobs with 6 10 8 and 6,
so with 6 10 8 and 6. Here again, one can
show that, the problem of minimizing Makespan
becomes hard and difficult a simple rule that is often used is L P T, which means arrange
them in the non decreasing order of non increasing order or decreasing order of processing times,
to get 10 8 6 and 6 and starting locating
them to the 2 machines this way.
So, 10 goes here 8 comes here, the next 6
will come here and this 6 will come here,
if there is a 5th job, it will go here, the
6th 7th 8th and so on, now here the Makespan
is equal to 16, here the Makespan is 14. So,
the actual time, at which all the jobs are
over is the maximum of these 2, which happens to be 16, so one would say that, if we apply
this rule then the Makespan that, we get is
16. So, there are other rules and algorithms,
to try and minimize other objective, such
as mean completion time tardiness and so on.
But, has I have mentioned the end of discussion on single machine problems, we learnt two
important rules, one is called the shortage
processing time rule and the other is called
the earliest due date rule.
So, now we get into the next set of problems and sequencing and scheduling, which is called
flow shop sequencing called flow shop sequencing, we explain it through a small example.
As we have already mentioned, a flow shop is a shop, where say we have n jobs J 1 up to
J n and then we have M machines, which we would call as M 1 M 2 and M m an important
condition in a flow shop is that, all the
jobs will have to visit all the machines,
in the same sequence. So, J 1 requires processing on M 1 M 2 M 3
up to M m, it also requires processing in
the same order except that, the processing
times are different - the same with J 2, same with J 3 and so on. So, we it is customary
to say that the machines are already arranged, in the order, in which all the jobs visit
them, so we could say, we would call the machine, which all the jobs visit first as M 1, a machine
which all the jobs visit second as M 2 and
so on.
So, the machines are already arranged in the order of visit of the jobs, so this is a typical
flow shop problem and then we try to look
at, what we do for objectives such as Makespan
mean completion time and so on. In this lecture series will concentrate only on the Makespan
for flow shop sequencing, now we explain it through a small example, we first address,
what is called the 2 machine flow shop problem, where there are n jobs and there are only
2 machines.
So, we take a small example, to explain the
n job 2 machine problem, now this is an example,
where there are 5 jobs, which we call J 1
J 2 J 3 J 4 and J 5 and there are 2 machines
M 1 and M 2. Now, we want to find out, the
order or sequence, which minimizes the Makespan,
so there are 5 jobs, so there are maximum
of 5 factorial sequences, in which we can
arrange these 5 jobs. Now, we will check about this 5 factorial after a while, but then we
right now assume that, there are 5 jobs and
they can be arranged in 5 factorial ways.
And we right, now assume that one of the 5
factorial ways, by which we can send the jobs
is going to be optimal, for the Makespan.
So, first let us try and compute the Makespan
for a given sequence and then we try and optimize the sequence, so to make it very easy, we
try and compute the Makespan, for the sequence J 1, J 2, J 3, J 4 and J 5.
Now, first for a given sequence, let us find
out the completion times, now these are completion
times on machines M 1 and M 2, so we first send job J 1, so job J 1 goes first. So, machine
both the machines are free and their available at time equal to 0, so J 1 requires 10 units
of time on M 1, so it will start at 0 and
comes out time equal to 10 remember that,
these are completion times. So, it starts
at 0 and comes out at time equal to 10, so
at time equal to 10, it goes to M 2, M 2 is
free, at M 2 it requires another 6 units,
so it comes out at time equal to 16.
Now at 10 M 1 is free and therefore, the next
job goes to M 1, so right now we are evaluating it for a sequence J 2, so J 2 comes in next,
machine M 1 is available at time equal to
10, therefore J 2 starts at time equal to
10 and finishes at time equal to 16. Now,
in this case exactly at time equal to 16,
J 2 finishes at M 1, now M 2 is also free
at time equal to 16, so J 2 straight way takes
M 2 at time equal to 16, requires another
12 units of time, so at time equal to 28,
J 2 finishes at M 2 and J 2 goes out of the
system.
Now, once again at time equal to 16 M 1 is
free. So, M 1 can takes its next job, which
happens to be J 3, because we are evaluating a sequence 1 2 3 4 5, so it starts at 16 and
finishes at time equal to 24, 16 it requires
a further 8, so 16 plus 8 24 8 comes out.
Now, at time t equal to 24, J 3 comes out
of M 1 and J 3 wants use M 2, but M 2 is free
only at time equal to 28, so J 3 has to wait
for 4 units of time before, it could go to
M 2. So, right now we assume that, there is enough
space and buffer between M 1 and M 2, so that the person, who is working can take J 3 out
of M 1 keep it in the buffer space and then
take up the next job. So, the next job can
enter at time equal to 24 into M 1, but to
finish this computation, now we say that the
J 3 can take M 2 only at time equal to 28,
so at 28, it takes on it goes to M 2, so 28
plus 9 is 37. Now J 4 can enter at 24, because at 24 it is over, the person has taken the
job and kept it outside M 1 is free.
So, J 4 starts at time equal to 24 takes a
further 8, here finishes at time equal to
32, now J 4 again has to wait for 5 more units,
so that M 2 becomes free at 37 and then it
goes to M 2 at 37 takes 10 more units finishes
at 47 and then it comes out. Now, at 32 J
5 can enter M 1, so J 5 enters M 1 at time
equal to 32, once again, we assume that, there is enough space in between for the person
to take it and keep it, so that M 1 can be
released for it is next job, so it starts
at 32 and finishes at 44 - 32 plus 12 is 44.
And then it has to wait for 3 more time unit
to get into M 2, starts at 47 finishes at
47 plus 5, 52 and time equal to 52 all the
jobs are over and therefore, for this given
sequence the Makespan is 52.
Now, the question is what is the sequence or what is the order, in which
we can send this, such that the Makespan
is minimized? We also realize something interesting from this particular table. If we see very
carefully 44, which is the time, at which
M 1 completes all the jobs is actually, the
sum of the processing times here 10 16 24
32 plus 12, 44.
So, M 1 is not idle, till it finishes everything
then M 1 becomes idle in this case for 8 more
time units or 8 more minutes, other thing
that, we observe is M 2 cannot start at time
equal to 0, because of the flow shop condition. So, M 2 can now start here at 10 and then
we also realize that, there were some delays that happened here, so there was a delay of
4 units here, there is a delay of 5 units
here, there is a delays of three units here,
so 4 plus 5 9 plus 3 12.
Now, we look at this 18 27 37 plus 5, 42 plus
the other 10 was 52. 18 plus 9 27 37 42 10
52, but then it is started at 10, 10 plus
42 is 52, M 2 also was not idle. Now sequencing
and scheduling problems become important.
Because, this number, at which it completes
the Makespan largely depends on a couple of
things. It depends on 2 very important things here, which is called job waiting for the
machine and machine waiting for the job.
Now, in this case, we had situations, where
the job was waiting for the machine, this
job was over at time equal to 24, but then
the job had to wait till 28, here the job
was over at 32, it had to wait till 37.
Now, you look at this case machine is waiting for a job the machine was available at 0, but
then the machine has to wait for this job
it has to wait for 10 minutes. So, it is usually
the delays that happen, when the job has to
wait for the machine and the machine has to
wait, for the job that creates, this kind
of an issue, where the objective changes.
So, the problem that we are looking at is
given a sequence, we can find out the Makespan,
what is the sequence that minimizes the Makespan? Now, a very popular algorithm is called the
Johnson’s algorithm was developed by Johnson in the year 1954, which tries to minimize
the Makespan on 2 machines. Now, Johnson not only gave a very simple algorithm, but he
actually gave a very elegant proof for the
algorithm.
So, we will see the algorithm alone in this
lecture series, we will not see the proof,
but then we see how intuitive Johnson’s
algorithm is when, it comes to solving a 2
machine flow shop sequencing problem. So, Johnson’s algorithm works like this,
a very simple way to do it is there are 5 jobs.
So, create a small table with 5 positions,
now find out the smallest number in this table,
the smallest number is 5 and 5 happens to
be for job 5 on machine M 2, so we look at
this and the rule says, if the smallest number is on M 1, then come from the left and if
the smallest number is on M 2 come from the right. So, smallest number, which is 5 is
on M 2, so we come from the right in this
table and since that happens, for J 5, now
we come from the right and write J 5 here
as the last job.
So, once again look at this smallest number, the smallest number is 5 and check whether,
it is on M 1 or whether, it is on M 2 and
for, which job it is, so in the case the smallest
number is on M 2 and it is for J 5. Since
it is on M 2 come from the right and write
that job in the first available position from
the right, so first available position from
the right is here, so J 5 goes here, now remove this job from the list.
Now, look at the rest of the table and try
to find out the smallest number, the next
smallest number is 6 and it happens to be
on M 2 for J 1 and also happens to be on M
1 for J 2. So, when we have a situation like
this, where the smallest number is coming
on M 2 as well as M 1, we may be tempted to believe right now that there is a tie, because
6 is appearing in 2 places and 1 case, it
is on M 2, other case it is on M 1.
The fact that, it is on M 2 in one case and
on M 1 in the other indicates that, it is
actually not a tie and we can actually break
it, arbitrarily there are no problem at all,
if it happens to be on M 2 as well as on M
1 for 2 different jobs. So, we could take
either this or we could take this, now let
us take J 2 on M 1 as the smallest number,
so J 2 on M 1 means, since it is on M 1 the
rule says come from the left and take the
first available position from the left. So,
it is on J 2 on M 1, because it is M 1 come
from the left, take the first available position, so from the left take the first available
position and put J 2. Now, remove J 2 from the table temporarily,
now once again find the smallest number has to be is 6, it is on M 2 and it is for J 1,
since it is on M 2 come from the right and
take the first available position, so from
the right you take the first available position and put J 1 here. Now, at this point we realize,
what we said earlier, now we had the minimum coming here, for J 1 on M 2 and the minimum
coming here, which is J 2 on M 1, we first
took this and we put J 2 here and then we
ended up writing J 1 here.
If we are taken it the other way, we would
have taken this first then we would have written J 1 here and then J 2 would have come here,
the sequence has not changed, because the smallest number happened to be on M 1 for
a particular job and on M 2 for some other
job. So, now, remove J 1 also, so 3 jobs have
been put in the table, 3 jobs have gone out,
so what we have remaining are J 3 and J 4,
now find the smallest number, which happens to be 8, which is on M 1, which I for J 3
as well as, it is for J 4. Now here there is a tie whether, we take this
J 3 on M 1 or whether we take J 4 on M 1,
so as long as the smallest number happens
to be on 2 different machines, there are only 2 machines, happen to be on 2 different machines,
there is no tie, the sequence does not change or the sequence is not affected. But, if there
is a tie like this, when the smallest number
happens to be on M 1, now we would either
take J 3 or take J 4, so first let us take
J 3.
So, since it is on M 1 come from the left
and take the first available position,
so we take J 3 here, now remove this J 3 there is only 1 job remaining and there is only
1 position vacant, so put J 4 here. Now, we
had a tie, we had a tie between J 3 and J
4, both of them have a smallest number and it was on M 1, we resolve the tie by taking
J 3 first, now what would have happen, if
we had taken J 4 first, if we had taken J
4 first we would have got the sequence J 2
J 4 J 3 J 1 and J 5.
So, Johnson’s algorithm in this for this
particular example gives 2 sequences,
now 2 sequences happened, because of a tie occurring here and we had one sequence, which we resolved
by putting, J 3 first and then J 4 and the
other sequence by putting J 4 first and then J 3.
Now, let us try and evaluate the Makespan, for both the sequences will first do that
for this sequence, so the sequence that we
are taking is J 2 J 3 J 4 J 1 and J 5 and
completion times on M 1 and M 2.
So, J 2 goes first J 2 finishes starts at
0 finishes at 6, now J 2 goes to M 2 at 6
takes another 12 units, so finishes at 18.
J 3 goes next J 3 can start on M 1 at 6 needs another 8 units of time finishes at 14,
but M 2 is available only at 18. So it takes another 9, 18 plus 9, 27. Now, J 4 can start at 14
on M 1 takes another 8 units, so 14 plus 8,
22. Now it has to wait till 27, for M 2 to
be free takes at M 2 at 27, takes another
10 units of time finishes at 37.
Now J 1 can starts at 22 takes another 10
units of time finishes at 32, 22 plus 10.
It has to wait for another 5 units for 37,
so takes M 2 at 37 plus 6, 43 it comes out.
J 5 is the last job, it can start at time
equal to 32 takes another 12 units of time,
so finishes at time equal to 44. And now,
J 5 comes out at time equal to 44 M 2 is already
free at 43, but J 5 can take M 2 only at time
equal to 44, takes another 5 units finishes
at 49 and now all jobs are over at time equal to 49, which is the optimum Makespan according
to Johnson’s algorithm. Now here we can see both, we can a situation,
where this job is coming out at 14, but the
machine is available at 18 so here is the
case, where the job is waiting for the machine to be free. Here the job comes out at 44,
but the machine is already free at 43, but
the machine has to wait for the job to come,
so there is a machine waiting time of, one
which again increases the Makespan.
So, the Makespan increases due to two things one is job waiting for the machine the other
is machine is waiting for the job. It is only
these 2 delays that makes sequencing and scheduling
problems important and scheduling and sequencing problems difficult. So, based on the Johnson’s
algorithm, where we got the sequence, we got the Makespan to be 49, but we also got another
sequence here, so we will quickly try and
compute the Makespan for the other sequence.
So, here we are again computing cycle times, we have M 1 completion times, we have M 1
M 2, the sequence is J 2 J 4 J 3 J 1 and J
5, now for J 2, it is the same, so it is 6 and 18.
Next we have J 4, so J 4 comes here. So, J 4 takes M 1 at 6 finishes at 14 has
to wait for 4 units of time, so goes to M
2 at 18 and finishes at 28. Now, J 3 can start
at 14 takes another eight units of time, so
14 plus 8, 22, waits for 6 more units goes
to M 2 at 28 and then plus another 9 finishes at 37.
Now J 1 can go to M 1 at 22 takes 10 units
of time, finishes at 32 waits for 5 more units
of time to go to M 2 at 37 takes another 6
units and comes out at 43. Now J 5 goes to
M 1 at time equal to 32 takes 12 more units finishes at 44. Now, M 2 is waiting, so it
goes to M 2 at 44 takes another 5 units of
time and finishes at 49. So this sequence
also gives us the Makespan of 49. So Johnson’s sequences, because of tie is if we have more
than one sequence all the sequences are optimal and all of them give the same amount of Makespan.
Now, Johnson’s algorithm gives the optimum Makespan, for a general m jobs, but 2 machine
flow shop and Johnson’s algorithm is based on a very simple rule, that if you have n
jobs create a table with n slots, find out
the smallest number in the table. Now, if
this smallest number is on M 1, then come
from the left and put the corresponding job
in the first available position, if the smallest
number happens to be on M 2, then come from
the right and put the corresponding job in
the first available position.
Once a job is entered into the table temporarily eliminate or remove the job and once again
find the smallest number, repeat this till
all jobs go into this, if there is a tie,
can be broken arbitrarily. But, ties can result in more than one sequence a tie will happen,
if the smallest number is on a particular
machine say M 1 and more than 1 job has that.
Similarly, tie will happen when, it is on
M 2 and more than one job has it, but if the
same smallest time happens to be on M 1 and M 2, for 2 different jobs then it is not a time.
So, get as many solutions as there are ties
all of them are optimum with the same value of Makespan, now Johnson’s algorithm is
seen is the starting point of research in
sequencing and scheduling. Because, what appears
to be a complicated problem with the worst
case possibility of n factorial Johnson came
off with the very, very elegant algorithm,
which could solve the 2 machine case.
Now Johnson’s algorithm triggered a whole lot of research in sequencing and scheduling
and for well over 55 plus years 100 and 1000s of people have been working on sequencing
and scheduling problems. How, Johnson’s
algorithm can actually be extended, to a very
special case of the 3 machine problems, now
Johnson’s extension to the three machine,
we will see in the next lecture.
Johnson's algorithm is an algorithm that
improves on iterated bellman-ford and
iterated Dijkstra's algorithm for
finding all pairs shortest paths
ironically it does that by using both of
those algorithms we're going to study
Johnson's algorithm in Pearl and Hermes
atoll again which is a suitable location
because you need to search to find the
shortest paths and this a toll here
we're towing a buoy that measures water
conditions and weather conditions and
sends it to a satellite year-round we
have to replace an old one with this new
one and as you enter the atoll it looks
like you can just go straight across the
water at one point we could see the buoy
we wanted to replace and our able-bodied
seamen tried the head straightforward
but you can't do that in this at all
because it's actually a reticulated maze
of reefs and you keep running into reefs
so to find the shortest path from where
you are to the buoy you actually have to
follow a GPS track that is essentially
tracing your way through a graph so
let's look at how Johnson solved the
problem that we just saw in the previous
screencast so we've been using a weight
function W that map's e or V cross V to
real numbers how to make that symbol
that's the real numbers I guess for each
edge in the graph and it will give the
value infinite otherwise if the edge is
not in the graph and the problem is that
we want to make a modified weight
function we're going to call it W hat
that enables us to run Dijkstra's
algorithm by getting rid of the negative
weight edges so the problem is we we
ignored one requirement in that example
that I gave in the previous screencast
the requirement that we met with the
example in the previous screencast was
that we want to have the case that for
all edges u V in E the the W hat will be
non-negative so we did that but it
didn't work and it didn't work because
we neglected another requirement that
were going to meet here and that is the
requirement that essentially if we find
a shortest path in the graph with the
modified weight it'll also be a shortest
path in the original
so if a path is a shortest path from u
to V using W we want that to be true if
and only if it's a shortest path from u
to V using W hat
in other words finding a shortest path
and transformed graph finds it's a
shortest path in the original graph
tried a naive approach to coming up with
W hat and it didn't work
Johnson's insight was a better way that
meets both of these conditions to
understand this we need to start with
something called the rewedding lemma the
rewriting lemma will start with a graph
G V that's weighted and say let H be any
function that map's vertices also to the
real numbers limit doesn't care what
function H is now the lemma defines W
hat as follows it's W plus h of the
start vertex minus H of the end vertex
and keep in mind this H can be any
function that max maps vertices to
numbers now let's look at any path in
the graph we call it P it's a path from
V naught to BK the waiting lemma then
says that this meets conditions one
which will write out as follows just
mirroring the definition above it's also
going to have another conclusion which
will be that the graph G has negative
weight cycle under W if and only if it
has it under AB u prime so that's going
to let us test in the modified graph or
a negative weight cycle and it will tell
us whether there's one in the original
graph so how do we prove this first
we're going to show that this definition
this property here which was defined on
edges also applies to paths I'm going to
do so as follows
let's define W hat of the path to be the
obvious sum of W hat of all the edges on
the path
so this just says sum up for Michael 1
the k:w have of each edge you know I - 1
being 0 so that's starting at V sub 0
and so on at the V sub case that's just
applying a definition of the sum of the
weights of the paths but now let's apply
our definition here expanding w hat of
UV for each of these terms and in the
sum expanding using this here so we're
replacing w hat with W which is what
this definition does up here and we're
adding in the h's adding and subtracting
the HS that is now the next thing to
notice is that this some telescopes for
example when you've added in V of I
minus 1 you're going to then subtract it
out in the next increment as I
increments up you're going to subtract
it out with this term so the only
remaining terms are the first in the
last H of V 0 and minus H of V K so this
is equal to this so that's this term
here but then what I'm going to write
following is no longer inside the scope
of the summation so we've taken these
things out of the scope of summation
because of the telescoping sum only the
first in this survived the first in the
last survived and here they are but this
is precisely the definition of WP right
here plus h of v-0 minus h of VK so
therefore any path from V to V 0 to V K
as the W hat wait on that path is going
to be the W wait on that path plus the
number H assigns to the first vertex of
the path minus the number H assigns to
the last vertex of the path but how does
this show that if he is a shortest path
from the knot the VK under W it's also
shortest path under B not B K under w
hat well the only difference between
these two is the the H of the knot and H
IV K but those don't depend on the path
those are just the start in the end
position so anything if you have two
different paths and it has one path has
a lower weight than the other one under
this definition here it's also going to
have a lower weight than the other one
this definition here because it doesn't
depend on the path so this shows that
this part here is true what about this
bit about the cycles here well let's let
a cycle cycle is just like a path except
that II not is the same as VK so that's
going to say the cycle is like that path
okay so let's now write AB u hat of the
cycle is going to be from what we just
showed here W of the proposed cycle plus
h of V naught minus H of the K but these
two things are equal they cross out so
it's just simply equal to the weight
under W so the weight under any under W
hat of any cycle is going to be exactly
the same as the weight of the cycle
under W so therefore if we find a
negative weight cycle under W it's got
to be the same as a negative weight
cycle under W hat that proves a lemma
but what does it do for us well bringing
back in these two properties we need to
meet we've just shown that it meets
property one down here and it's
remarkable that that this lemma holds
under any assignment of H H can assign
any way to the vertices and the shortest
paths and negative weight cycles will be
preserved and since we have that
flexibility we're going to use that
flexibility to get property two under
Johnson's
other insight which brings us to his
choice of what H is we've shown property
1 now we just have to pick an HIV that
meets property 2 which is that it will
always be greater than or equal to 0 w
hat we defined is w UV plus h of u minus
h of v and that's got to be greater than
equal to 0 so how does Johnson do it now
the motivation for how Johnson got this
insight is from material we haven't
covered so we're just going to have to
take this as an insight out of the blue
and then we will show that it works
we're going to define a new graph and
we'll have a vertex set V prime which is
V with a new vertex S added and it'll
have an edge set E prime which is the
old edge set plus we're going to add an
edge from
ask to every vertex in the set of
vertices and finally here's how we're
going to define W from s to V so note
that no edges enter s so therefore G
Prime has to have the same cycles as G
including any negative weight cycles if
they exists so before we go on let's
make sure we understand what this means
if we start with this as our original
graph then we're going to add a start
vertex s it's going to have an edge to
all the vertices and that weight is
going to be 0 and that will look like
this so here's the original graph but
now we've got a new vertex and we've
added 0 weight edges to all the other
vertices and here's the crucial step how
he defines H of V H IV is going to be
the distance of the shortest path from s
to V so in terms of that example we just
looked at again here we have the
original graph the vertices are labeled
with their labels on the inside but then
when we augmented it here we've moved
the labels outside so it's 1 2 3 4 5 or
the labels outside and the shortest path
from the start vertex to each vertex
isn't now indicated inside the vertices
of course usually that would be 0 but in
some cases where there's negative weight
edges in the graph it's not 0 so here
the shortest path is 0 but here we can
get to this vertex by negative 4 because
that's cheaper than going the direct
route by 0 so notice that they're either
going to be 0 or they're going to be
negative they can never be positive
because I always get to a vertex by cost
to 0 from the start vertex so what we
need to show is that this definition of
H V gives us this property property to
that it's always going to be greater
than 0 so our claim is that if we define
W hat to be W plus h of u minus H of V
under this definition then it will
indeed be greater than equal to 0
proof let's take the triangle inequality
then triangle inequality the distance
from s to B shortest distance from s to
V has to be less than or equal to the
shortest distance from s
to you plus the weight from u to V now
if we defined HIV to be Delta SV we can
substitute that in here for both V and u
and we get this expression and now if we
subtract HIV from both sides we're going
to get zero w u the plus h of u minus H
of V which is exactly what we wanted to
prove w UV + hu - HV is greater than
equal to zero therefore we've met
property two so what this means is that
we can use HIV to define W hat and that
adjusts the weights in the graph weights
between the edges are adjusted in this
manner and that's guaranteed that if we
find a shortest path in the new graph it
will be the shortest path in the old
graph and furthermore if we find cycles
in one negative weight cycles and one
there will be negative weight cycle
corresponding negative weight cycle in
the other and we should add that this is
equal to W hat of U V so how does this
work well let's get back to this graph
here here's the Augmented graph and
let's look at this negative weight edge
you want to get rid of we're going to
give it a new weight so the w hat weight
will be the old way plus the H of its
start vertex and minus H of its end
vertex remember the H is equal to the
shortest path from this s vertex so the
shortest path here was zero in the
shortest path here was negative four so
the value is in the circles are the H so
we're going to take W UV which is
negative four plus h of u which is zero
minus H of V which is negative 4 so
negative 4 minus negative 4 negative 4
plus 4 this is going to become 0
similarly we got this edge here weight
of 3 we are going to add H of U which is
0 and we're going to subtract H of V
which is negative 1 so this 3 will
become a 4 and so on and so if you do
that we will get this updated weight
graph which will look like this so here
we have the negative weight edge has
been converted to a zero this has been
converted to a 4 we
rid of the other negative weight edge
and so on well that was pretty
complicated what we finally found our
way to where we want to be we've got the
old we here in the new one and the
diapers go down and start replacing it
as usual under the watchful eye of the
local bosses some pretty mean looking
dudes who aren't very happy so we've
developed the theoretical background
behind Johnson's algorithm but let's
take a look at the algorithm itself here
we can see in the beginning we compute G
Prime that's the Augmented graph where
we add the new start vertex s and then
we add from s to every other vertex in
the graph we add an edge so we're
augmenting e G prime of e is this new
set of edges including the ones from s
to other vertices and we initially set
the weight from s to other vertices to 0
and here's an interesting thing we're
going to run bellman-ford on this
augmented graph and if that returns
false remember that means it has a
negative weight cycle we aren't allowed
to have a negative weight cycle because
that means that there's a potentially
infinitely negative path length from one
vertex to another in the graph we can
have cycles with negative weights on
them they just can't be negative weight
cycle so that will lead us to exit
because the shortest paths are undefined
otherwise we're going to now define the
H value and the H values are defined by
the shortest path that we found up here
in bellman-ford when we search from the
start vertex s to all other vertices
because remember the H value is defined
to be that shortest path from s to all
the other vertices so we're going to set
H to be to every vertex to be those
values and remember that H is now used
to define the new W hat by taking the
original W and adding H of the start
vertex and subtracting the H of the
target vertex so this will require
iterating over
we're making it essentially we're making
a table you know this is a function but
we're making a table that says for each
edge this is the value of this function
for this edge we need a new matrix and
an N by n or V by V matrix to record the
results and then because this is all
pairs shortest paths we're going to run
for each vertex u being the start vertex
we will run Dijkstra's algorithm
starting from that start vertex using W
hat the you know the new weighting
function that gets rid of all those
negative weights so we can run
Dijkstra's algorithm and again it gets
rid of the negative weights in a what
manner that does not cause the problem
that we saw in the first screencast
where at some password were penalized
more than others that computes shortest
paths under this we'll call it Delta hat
which is based on the W hat so it gives
the shortest path distances under this w
hat function but once we found those we
need to convert them back into what the
path links would be under the original W
so whereas here we took the original W
and we added H of the start vertex and
subtracted the H of the N vertex we're
going to reverse that to get the actual
distance by subtracting the H of the
start vertex to undo our adding it here
and then adding the H of the end vertex
to undo our subtracting it here and
finally we returned this matrix of
distances between all pairs so before I
go on I just want to say what is the
point of this is it it's not just for
you to learn Johnson's algorithm it's to
learn some things about how when when a
problem doesn't seem to work with the
algorithms you have you can change the
representation of the problem work with
the algorithms you have so change of
representation is an important concept
here and also the interesting idea that
you can construct more complex or
sophisticated algorithms by combining
existing algorithms like we did here
with Belvin Ford and bike stris
bellman-ford had the property that it
can handle negative weight cycles but
it's not as efficient so we run it once
to discover the negative weights
cycles and bail out of there there at
the same time we run it to get the
shortest paths from s to all the other
vertices that defines this age function
and that enables us to run the more
efficient Dijkstra's algorithm from each
start vertex using our newly defined
wave function w hat but what about the
efficiency of this the runtime to
compute G prime here is going to be
order of e to go over all those vertices
and put in edges from s to compute
bellman-ford is going to be order of V
times e now here the runtime to compute
the H values is going to be for each
vertex so that's also going to be order
of e here we have for each edge we're
going to define the new weight function
for each edge in the graph so that's
going to be order of e if we initialize
this matrix the values in this matrix
that's going to be order of V squared
but all of this is dominated by the next
thing remember Dijkstra's algorithm here
is order of e log V but we're doing it
for each vertex so this whole line here
is going to be order of D for each
vertex times e log D and not
surprisingly this is the same as
iterated Dykstra's Dykstra is the log V
we're iterating at the x so it's not any
more efficient than iterated Dijkstra's
so what have we accomplished well we've
accomplished is we can now handle
negative weights so this is the most
efficient approach especially on sparse
graphs it is possible to improve
performance using Fibonacci heaps as
usual - or V squared log b + v e see the
CLRS text for that but we're going to
use this result here as the dominating
term for Johnson's algorithm
so let's recap with an example here is
our original graph augmented with s and
edges of length of 0 to all the vertices
and the vertices are now labeled with a
shortest path from s so for example how
do we get negative 4 here where you go 0
negative 4 how do we get this negative 1
here where you go 0 2 here negative 5
here 4 here which gives you a negative 1
and so on so that gives the H values in
in the vertices or all the H values that
are going to be used to adjust the
weights and now let's go ahead and
adjust the weights so now you can see
that we have adjusted all the weights
we've got no negative weights anymore
we've either either got 0 or positively
and so now we do the search we throw
away this Augmented part at this point
we just had to use the S and all the
extra links in order to find the H
values and now we throw them away and we
do runs from every vertex so here for
example is a run from vertex 2 we found
all the shortest paths using Dijkstra's
algorithm in this graph the numbers on
the left are the distances in this graph
the number on the right are the adjusted
differences distances in the original
graph so let's see how this works
of course it's 0 distance to itself now
in this graph it found the link of
length 0 to give us a path length of 0
to node 4 but to get the cost in the
original graph remember we have to
reverse the adjustment that was done in
computing w hat so we now have to
subtract the source vertex weight source
vertex weight is negative 1 from the
previous figure and we have to add the
target weight which was 0 so that gives
us this value here of 1 for a more
complex example let's look at node 5 in
this graph we get a path length of
length 2 so we've got 0 to 0 but now to
get the cost in the original graph we
are again going to subtract the source
vertex weight and add the target vertex
weight we don't have to subtract and add
all the weights along the path
because of that telescoping some deal
all we have to do is subtract the source
vertex weight which is negative one so
essentially we're adding one and add the
target vertex weight which was negative
four and that gives us negative one in
the end because that's two minus
negative one which is three plus
negative four makes it negative one so
you can figure this out for all the
other numbers in the graph you should
run an example it's kind of hard to
figure out without actually running
examples and of course we just did this
example we're going to Johnson's
algorithm we'll run it from all the
other vertices as to start vertices and
construct the matrix of all these
shortest paths solutions well that's the
end of a fun day at Berlin Harry's at
all working on Johnston's algorithm this
is David Lynch fogger and Susan
Middleton who are photographers working
for National Geographic and they were
taking specimens that the the fish
people brought back and had a complex
photography setup in the wet lab and
here is the octopus that they
photographed that became the cover of
their book you can google it on
amazon.com so we're all done with all
pairs shortest paths Johnson's algorithm
we just need to take a look at a dynamic
programming alternative
in this video we'll cover Johnston's
very cool algorithm it shows how to use
the rewriting technique we introduced in
the last video to reduce the all pair
shortest path problem in graphs that can
have negative edge links to a single
invocation of the bellman-ford shortest
path algorithm followed by n indications
of Dijkstra's shortest path algorithm
we concluded last video daydreaming
about this best case scenario where we
have a graph with negative edge lengths
but somehow we come up with these
magical vertex weights that transforms
all of the edge links to be non-negative
and it turns out that the magical vertex
weights which will realize this
best-case scenario our best computed by
a shortest path algorithm so before
describing Johnson's algorithm in
general let me just walk you through
some of the steps in an example
so I've drawn a directed graph with six
vertices and seven edges I've annotated
each edge with its cost in blue notice
that some of the edges do indeed have
negative costs so it is not currently an
option to run Dijkstra's algorithm on
this graph
there is no negative cycle however in
this graph it only has one cycle the
directed triangle at the top and its
overall cost is one so we could run the
bellman-ford algorithm on this graph if
we want
so our strategy is to compute a magical
set of vertex weights so that when we
transform the edge links using the
rewetting technique described in the
previous video we wind up with a graph
that now has only non-negative edge
lengths
so where do these vertex weights come
from well the great idea in Johnson's
algorithm is to compute them using a
subroutine for the single source
shortest path problem
to implement this we need a well-defined
instance of the single source shortest
path problem in particular we need a
source vertex
so that's a pretty good idea there's
only one small problem with it which is
that when you pick your arbitrary source
vertex it might not be able to reach all
of the other vertices and to get our
magical vertex weights we're really
going to want finite shortest path
distances from our arbitrary source to
everybody else for example in the graph
that I've drawn here on the slide it
doesn't matter which of the six vertices
you choose as your source vertex you
will not be able to reach all of the
other five
so how do we get around this issue well
with a simple hack we're just going to
add a new vertex so it's 7th vertex in
this example and we're going to connect
this new vertex which I'm going to just
call s2 all of the original vertices
with a direct arc of length 0
we are then going to compute shortest
paths from this new artificial source
vertex s to all of the vertices from the
original graph notice that by
construction we're going to get a finite
shortest path distance from s to all
other vertices we've installed a direct
path from s to everybody else
notice that because this new vertex s
has no edges going into it it's
effectively invisible from the
perspective of all of the original
vertices in the graph G so in particular
the shortest path distance between any
pair of vertices U and V in the original
graph is unchanged by this addition of
the vertex s similarly whether or not G
has a negative cycle is unaffected by
the addition of the vertex s
the next step of Johnson's algorithm is
to invoke a single source shortest path
algorithm using this newly added vertex
s as our source vertex
now in this example and in general we're
thinking about the case of negative edge
lengths so we're not going to be able to
use Dijkstra's algorithm to solve this
single source version of the problem
we're going to have to use the
bellman-ford algorithm to do this
computation
so let's now go ahead and figure out
what are the shortest path distances
from s to the other six vertices in this
graph on the slot
so let's start with the vertex a what's
the shortest path distance from s to a
well it's certainly no worse than zero
there's a path directly from s to a of
length zero and indeed in general all
six of the shortest path distances that
we compute will be zero or less
so the question is there a path from s
to a that is negative that's better than
zero well what are the other options we
could go from s to C at link zero but
then we'd pay for to get back to a so
that is length four so that's no good a
little bit better but still not good
enough would be the zip straight from s
to B that has length zero from B to C
now we're up to now we're down to minus
one but then we have to go from C to a
and so we add four so we get three so we
conclude that the shortest path from s
to a is indeed the direct one hot path
of length zero
most of the other vertices are more
interesting however think for example
about vertex B we could of course zip
straight from s to B along a path of
length zero but we can get shorter than
that
if we go first from s to a at length
zero and then from A to B then that path
has combined length minus two and that
is in fact the shortest path distance
from s to B
the shortest path distance to see is
just to take that same shortest path to
be and then concatenate the edge of cost
minus one from B to C that is the
shortest path from s to T goes s to A to
B to C for a combined length of 0 plus
minus 2 plus minus 1 minus 3 in all
so now let's move to the bottom of the
graph the vertex Z is pretty easy to
think about there's only one path in the
entire graph that's the direct 1 from s
to Z so Z is just going to have the
trivial shortest path distance of 0
so now for the vertex why you got a
bunch of different options you could of
course go straight from s to Y at length
zero but there's a lot of things better
than that you can also go from s to Z
and then along the minus four arc to Y
that would give you a path of length
minus four but you can do even better
than that by going first to a then to B
then to C and then to Y so that gives
you a path whose edge costs are 0 minus
2 minus 1 and minus 3 that gives you a
combined total of minus 6 the shortest
path distance to why
the shortest path to get to X the best
thing to do is to accumulate all of the
negative way to the top of the graph go
via vertex C and it's true you have to
pay the cost of two to get from C to X
but you still end up with a net length
of minus one which outperforms the
direct zero link path from s to X
now the brilliant insight in Johnson's
algorithm is that this shortest path
computation is extremely useful in fact
these computed shortest path distances
are exactly the magical vertex weights
weights that were seeking they're going
to transform all of the edge lengths
from general to non-negative
so let's define the weights piece of V
of a vertex V from the original graph
that is one of the six vertices in this
example that we started with as the
shortest path distance we just computed
from the extra vertex s to that vertex V
what I want to do next is see the effect
of reweighed in using these vertex
weights so to do that let me redraw the
example
so let's recall the formula for the
re-weighting technique given vertex
weights like these you define the new
length C prime e of an edge e say from u
to V as its original length C e plus the
weight of its tail piece of u minus the
weight of its head piece of B
so let's just do this computation for
each of the seven edges let's start at
the top edge a comma B so we start with
original length minus 2 we add the
weight of the tail so we're adding 0 and
then we subtract the weight at the head
so we're subtracting minus 2 that is
we're adding 2 so we get minus 2 plus 2
or 0 for the new length C prime e for
the edge a comma B
similarly for the edge B comma C we take
the original length minus one we add to
it minus 2 and then we subtract minus 3
but as we add 3 and again it all cancels
out we get 0 for the new cost of arc B
comma C
for the ark see comma a we take the
original length for we add minus three
and we subtract zero so the arc C comma
a has a strictly positive shifted length
it's now one
if we look at the arc C comma X so we
take the original length - we add minus
three and we subtract minus one so that
again all cancels out and C comma X has
a new cost of zero
same thing happens with the RFC comma
why we start with minus three we add
another minus three we subtract minus
six
that gives us zero
for the ark is Z comma Y we start with
one we add 0 we subtract minus 1 so we
get a new cost of 2 on the org Z comma X
finally for the ark Z comma Y we start
with minus 4 we add 0 we subtract minus
6 that is we add 6 so that gives us a
new length of 2
so I don't expect you to have intuition
or semantics for the computations that
we just did but at least in this example
the proof is in the pudding
we just used these shortest path
distances as weights and rhe waiting
magically made all seven edges have
non-negative edge links they all have
lengths either 0 1 or 2
so we've now done at least in this
example is realized the best-case
scenario we were dreaming about remember
what the key point of the rereading
technique video was we pointed out that
re-weighting preserves shortest paths if
you have some vertex weights some piece
of v's you change all the edge lengths
by weighting for every origin s and
every destination T you shift the length
of every s T path by exactly the same
amount by P sub s - piece of T the
difference between the vertex weights at
the origin and the destination so by
changing all paths by exactly the same
amount you preserve which path is the
shortest
so that's a cute party trick it's not
really clear would be useful but we're
hoping that maybe rhe waiting in a
shortest path preserving way could allow
us to transform the general edge links
version of a shortest path problem to
the non-negative edge links version of
the problem so that we are not stuck
with those slower bellman-ford algorithm
and instead we get to use the faster
Dijkstra's algorithm
and that's now exactly what we can get
away with here at least in this example
we did the transformation the new graph
has only non-negative edge links now we
can just run Dijkstra's algorithm once
for each choice of the source vertex to
compute all of the shortest path
distances
you
The following content is
provided under a Creative
Commons license.
Your support will help
MIT OpenCourseWare
continue to offer high quality
educational resources for free.
To make a donation or
view additional materials
from hundreds of MIT courses,
visit MIT OpenCourseWare
at ocw.mit.edu.
PROFESSOR: Let's get started.
Welcome back to 6046.
Today, we start
an exciting series
of algorithms for graphs.
We've done a lot
of data structures.
We're starting to get
back into algorithms
with dynamic
programming last week.
And today and the
next few lectures,
we're going to see lots of
cool algorithms about graphs.
First a bit of
recall-- we're starting
with shortest
paths, which you've
seen in 6006 in the context of
single-source shortest paths.
So typically, like you do a
Google Maps query, you think
of, I want to go from A to B.
But what you solved in 6006
was a harder problem, which
is I give you a point A-- here
it's called S for the source.
I give you a source vertex.
And capital V is a
set of all vertices,
capital E is a set of all
edges, remember graph notation.
Let's say it's a directed graph.
You've got edge
weights, like the time
it takes to traverse each road.
And you want to
know how long's it
take me to get from
S to V for all V.
So this is from one given
point to everywhere.
Today, we're going
to solve a harder
problem, which is all-pairs.
I want to go from all A to all
B. But what you saw in 6006
was single-source, where I just
give you one of the vertices,
and I want to know how
to get to everywhere.
The reason you saw this version
and not the A to B version
is because the best way
we know to solve A to B
is to solve this problem.
So at least from a
theory standpoint,
we don't know how to
beat Dijkstra's algorithm
and Bellman-Ford's algorithm
for the A to B problem.
So you get a little bit
more than what you asked
for sort of for the same price.
So let me remind you in a
few different scenarios what
algorithms we have,
and how long they take.
So the scenarios of interest
are the unweighted case,
a non-negative weighted
case, the general case,
arbitrary weights,
positive and negative,
and DAGs acyclic graphs.
These are some
interesting special cases.
And you should have seen in 006
algorithms for each of them.
Let's see if you remember.
So what's a good algorithm
for single-source shortest
paths in an unweighted graph?
BFS, good, Breadth-first
Search, that takes how long?
V plus E, good.
That's-- for graphs, V plus
E is considered linear time.
That's how long it takes
to represent the input.
So you got to look at the
input, most algorithms,
and the BFS is
optimal against that.
But we're going to start
getting worse as we--
well, for these two situations.
So for non-negative edge
weights, what do you use?
Dijkstra.
Ah, everyone's
awake this morning.
That's impressive.
And that takes how long?
This is a tricky question.
V log V plus E, wow, nice.
So this answer kind of depends
on which heap structure
you use, but this
is the best we know.
If you use a Fibonacci heap,
which we don't actually
cover but it's in
the textbook, you
achieve log V for extract
key and constant amortized
for each decreased
key operation.
So sorry, this is
for extracted min,
and this is for decrease key.
And so this is the
best we know how to do
with a Dijkstra-type approach.
If you use other heaps,
you get slightly worse,
maybe you get a log factor here.
But this is good.
This is almost as good as V plus
E. For moderately dense graphs,
if E is bigger than V log
V, then these are the same.
But if your graph is
sparse, like E is order V,
then you lose a log factor.
But hey, it's just a
log factor, not too bad.
We're going to get worse.
So for general weights,
what do you use?
Bellman-ford.
OK.
Which takes how long?
VE, that's the usual statement.
Technically, you should
assume VE is at least V
for this bound to hold.
But that's the way
to think of it.
So this is not nearly as good.
This is a lot slower.
If you think of-- we can
think of two situations.
One is when E is theta V, so a
very sparse graph like a tree
or planar graph or something.
And we could think of
when E is quadratic.
That's the dense case.
So here we get, whatever,
V and V squared for BFS.
For non-negative edge weights we
get V log V in the sparse case.
And we get V squared
in the dense case.
And for Bellman-Ford, we get
V squared in the sparse case,
and V cubed in the dense case.
So this is like a V
factor, a linear factor
larger than non-negative
edge weights--
makes a huge difference.
And finally for acyclic
graphs, what do you do?
STUDENT: Dynamic programming.
PROFESSOR: Dynamic programming
is one answer, yeah.
That works.
In some sense all
of these algorithms
are-- especially Bellman-Ford
is a dynamic program.
We'll see that little bit.
Another interpretation?
Topological sort, and
then Bellman-Ford,
yeah-- say, one round
of Bellman-Ford.
So Bellman-Ford actually
works really well
if you know the order you
should relax the edges.
And if in an acyclic graph,
you can do a topological sort,
meaning you visit
all the vertices,
so that whenever you visit
the right endpoint of an edge,
you've already visited
the left endpoint.
If you do Bellman-Ford
in that order, then
you only have to do one
pass and you're done.
Whereas normally, here,
you had to do it V times.
So the total cost of
this is just linear.
Good thing to remember,
especially on quizzes
and so on.
If your graph is acyclic,
you can achieve linear time.
But in the general
case, Bellman-Ford
is your answer
for single source.
Now, these are the
best algorithms
we know for each of these cases.
So I'm not going to
improve them today.
You saw the state
of the art 006.
But for all-pair shortest
paths, we can in some sense
do better, sort of.
So let me just quickly
define the problem,
and then tell you all
of the results we know.
And also, the results
we're going to cover today.
I didn't remind you of
the delta definition.
I want to go over this briefly.
So delta of s comma
v is the weight
of the shortest path from S to
V. The weight is well-defined.
Even though there may
be many shortest paths,
there's one best weight.
But there's some special cases.
It could be infinity,
if there's no path.
That's sort of by definition.
Say, well, it's infinite costs
to get-- if there's no path,
then we said there's
infinite weight one.
And it could be minus
infinity in the presence
of negative weight cycles.
So let's say, if there's
a negative weight
cycle on the way, if you
could reach a negative weight
cycle from s, and then still
get to V from there, then
the best way to get there
is to go to that cycle loop
around infinitely many
times, and then go to V.
OK, so the algorithms
you saw probably
didn't actually compute
correctly in this case.
They just said,
negative weight cycle--
I don't know what to do.
But it's actually not that hard.
With a little bit
more effort, you
can figure out where the
negative infinities are.
We're not going to rely
on that, but I'm just
throwing it out there to make
this a well-defined definition.
Once you have the
shortest path weights,
you can also store
parent pointers,
get the shortest path
tree, then you can actually
find shortest paths.
But again, we're not
going to talk about here.
We'll focus on computing delta,
but with the usual techniques
you saw in 006, you could
also reconstruct paths.
So for all-pairs shortest
paths, we have a similar set-up.
We have a directed graph, V,E.
And we have an edge weight
function w-- in general,
could have negative weights.
And our goal is to find delta of
u comma v for all u and v. OK.
Single-source shortest
paths is the sort of thing
that you might want to do
a few-- just given a graph,
and you want to find a shortest
path from A to B. I said,
this is the best way we know
how to do A to B, essentially.
But all-pairs
shortest paths is what
you might want to do if
you're pre-processing.
If you're Google
Maps, and you want
to be able to very quickly
support shortest path
queries between major
cities, then you
may want to first compute
all-pair shortest paths
for all major cities, because
road networks don't change
very much, the large scale.
This is ignoring
traffic and so on.
Pre-compute this, and then
given a query of two vertices,
come in-- probably
get a million queries
a second-- you could
very quickly know
what the answer is.
And this is the basis for
real world shortest paths.
Typically, you don't compute
shortest paths from A
to B every single time.
You use waypoints along the way.
And you have pre-computed
all-pair shortest paths
between waypoints.
So that's the motivation.
Yeah, I guess in some
sense, internet routing
is another situation
where at any moment
you may need to
know the shortest
path to get to-- the
fewest hop path say
to get to an internet site.
You know the IP address.
You need to know where to go.
You don't need to
know the whole path.
You need to know the next step.
But in some sense,
you're computing
all-pair shortest paths.
That's a more dynamic situation.
OK.
So here are the results we know
for all-pair shortest paths.
I think I'm going to cheat,
and reuse this board.
So same situations,
except I won't think
about acyclic graphs here.
They're a little
less interesting.
Actually now, I'm
curious, but I didn't
intend to talk about acyclic.
And so the obvious thing to
do to solve all-pairs shortest
paths is just run the single
source algorithm V times,
once from each source.
So I could do V times
Breadth-first Search, V times
Dijkstra, V times Bellman-Ford.
And now, I just need
to update my bounds.
OK so VE becomes
V squared plus VE.
If you're a little bit clever,
or you assume E is at least V,
that becomes VE.
If I run Dijkstra
V times, I'm going
to get V squared log
V plus V times E.
And if I run Bellman-Ford
V times, I get V squared E.
OK.
And over here,
everything's just going
to increase by a V factor.
So a little more intuitive is
to think about the sparse case.
I get V squared, V squared
log V, and V cubed.
Check that those match
over there-- 1 equals V.
And over here, I get V cubed,
V cubed, and V to the fourth.
OK.
So pretty boring so far.
The interesting
thing here is that we
can beat the last result.
The last result, which
is the slowest one,
could take as long as V
to the fourth time.
We can shave off
a whole V factor.
So a better general
case algorithm
is called Johnson's algorithm.
That will be the last
algorithm we cover today.
And it achieves
this bound, which
is the same as running
Dijkstra V times.
So it's between V squared
log V and V cubed.
And that's cool, because
this is the best algorithm
we know for all-pairs,
non-negative edge weight,
shortest paths, just
running Dijkstra V times.
Not very smart-- but it's the
best thing we know how to do.
And what this says is, even when
we have negative edge weights,
actually we can achieve the
same bound as running Dijkstra.
This is a bit counter intuitive
because in 006 you're always
told, if you have negative edge
weights, can't use Dijkstra.
Turns out, in the all-pairs
shortest paths case,
you kind of can.
How can that be?
Because this is
a harder problem.
If you could solve all-pairs
shortest paths, of course
you could solve single-source.
And that's actually the luxury.
Because it's a
harder problem, we
have this VE term
in the running time,
which lets us do things
like run Bellman-Ford once.
And running
Bellman-Ford once will
let us run Dijkstra V times.
That's the reason we
can achieve this bound.
But we won't be seeing
that for a while.
For starters, I want to
show you some connections
between all-pairs shortest
paths, and dynamic programming,
and matrix multiplication,
which turn out
to give-- for
dense graphs, we're
just achieving V cubed
in all situations.
So our first goal is going
to be to achieve V cubed time
for general edge weights.
So we're going to first
achieve this bound.
That will be a lot easier.
And then eventually, we
will achieve this bound.
So the Floyd Warshall
algorithm and some of these
will get very close to V cubed.
All right.
So we're going to start
with our first approach
to solving all-pairs
shortest paths-- that is not
using an existing single
source algorithm-- is
dynamic programming.
Someone mentioned that already.
It's a natural approach.
Shortest paths is kind
of dynamic programming.
In fact, most
dynamic programs, you
can convert to single-source
shortest paths, typically
in a DAG-- not all,
but a lot of them.
So we could try
dynamic programming.
Now, I'm going to preach
to you a little bit
about my way of thinking
about dynamic programs.
If you watched the
006 OCW version,
you've seen the five easy
steps to dynamic programming.
And if you haven't, this will
be new, otherwise, a reminder.
First thing I like to think
about in a dynamic program
is, what are the subproblems?
The second thing I like to think
about is, what am I guessing?
I'm going to guess some
feature of the solution.
Third thing is, I want to
write a recurrence relating
subproblems solutions.
Then, I'm basically
done, but there's
a couple wrap up
things, which I'm
going to have to use
another board for.
So number four is, I need
to check that I can actually
resolve these subproblems
in some order that's valid.
Basically, this is saying
that the constraint
graph on some problems
should be acyclic.
Because if there's a cycle
in the constraint graph,
you take infinite time.
Even if you memoize, if you do
infinite recursion-- bad news.
You'll never actually
finish anything,
so you never actually write
anything in the memo table.
So I want to make sure
that it's acyclic.
I mean, this is
really the same thing
we're talking about in
this erased row, which
is topological ordering.
Personally, I like to-- you
could argue that it's acyclic.
I like to just write down,
here's a topological order.
That's a nice proof
that it's acyclic.
If you write that
down as for loops,
then you actually
have a bottom up dp.
If you just take the recurrence,
stick it inside the for loop,
you're done, which
we'll do in a moment.
I guess I need a row for that.
And finally, you need to
solve the original problem.
And then, there's
analysis and so on.
But for specifying
the algorithm,
these are the key
things you need to know.
The hard part is figuring out
what the subproblems should be,
so that your dp becomes fast.
Running time is going to
be number of subproblems
times time per subproblem.
For each subproblem,
usually we're
going to want to guess some
feature of the solution
to that problem.
Once we do that, the recurrence
becomes pretty trivial.
Just for each guess, you
say what it should be.
So these are the
really hard two steps.
And then, OK, we checked
that it's acyclic.
And we make sure
that we can actually
solve our original problem
using one of the subproblems.
Sometimes, our original problems
are some of the subproblems.
I think that will happen here.
But sometimes you need to do a
little bit of post-computation
to get your answer.
All right.
So what am I going to
do for subproblems?
Well obviously, I have a bunch
of different problems involving
pairs of vertices.
I want to find delta of u,v for
all u and v. That, I erased.
But that's the problem.
So I want to know what is
the weight of the shortest
path from u to v?
If I stop there and said
that was my subproblems,
bad things are going to
happen, because I will end up--
there's no natural way to
make this thing acyclic.
If I want to solve u to
v using, I don't know,
u to x, and then
x to v, something
like that-- there's
no way to get out
of the infinite recursion loop.
OK?
So I need to add
more subproblems
to add more features to my
solution, something that
makes it so that when I
try to solve my subproblem,
I reduce it to
other subproblems.
Things get smaller, and so I
could actually make progress.
So there are actually two
natural ways to do this.
I'll call them method
one a method two.
Method two is actually
Floyd Warshall.
But any suggestions on
how we might do this?
This is a harder problem.
This is in some sense,
a kind of guessing,
but it's like I'm
going to guess ahead
of time that somehow
there's an important feature
of the shortest path.
I'm going to
parameterize by that,
and somehow it's
going to get smaller.
Yeah?
STUDENT: [INAUDIBLE]
PROFESSOR: Right,
shortest path--
it uses at most a
given number of edges.
Let's parameterize
by how many edges.
I think I'll use m.
So using at most m edges.
Very good.
So good, I think it
deserves a purple Frisbee.
All right, I'm getting
better, slowly.
By the end the semester,
I'll be pro at frisbee.
I should enter a competition.
So this is, of course, an
additional restriction.
But at the end of the day,
the problem I want to solve
is going to be essentially
duv, let's say, n minus 1.
If I want a shortest
path that does not
repeat any vertices,
then certainly
it has at most n minus 1 edges.
So in fact, the claim would
be that duv equals duv n.
I mean, and so on.
If you go larger than n minus
1, it shouldn't help you.
If you know that your
shortest paths are
simple-- if you know that
shortest paths don't repeat
vertices.
So this would be if there are
no negative weight cycles.
If there are no
negative weight cycles,
then we know it never
helps to repeat vertices.
So in that situation,
we would be
done, if we could
solve this for all m.
Now, slight catch--
well, how do we
know there's no
negative weight cycles?
You know, we could run
Bellman-Ford, I guess.
That's a little tricky,
because that only
finds reachable
negative weight cycles.
In fact from this
picture, we will end up
knowing whether there are
negative weight cycles.
So there will be no negative
weight cycles, if and only
if there's no negative diagonal
entry, say dvv n minus 1.
So it turns out,
this algorithm will
detect there's a
negative weight cycle
by finding that the distance
from v to v is negative.
Initially, it's going to be 0.
If it turns out
to be negative, we
know there's negative
weight cycle.
With more work,
you could actually
find all the reachable
pairs, and so on.
But I'm not going to worry.
I'm just going to say, hey,
a negative weight cycle.
I'm going to throw my
hands up in the air
and give up for today.
OK.
Cool.
So I can solve my
original problem,
if I can solve
these subproblems.
And now, things are easier,
because we can essentially
assume in solving this
problem that we've
solved smaller subproblems
for however we define smaller.
That's what's given by
this topological order.
Obvious notion of
smaller is smaller m.
Presumably, we
want to write this
with an m in terms of this
with an m minus 1 or smaller.
So this gets to
our guessing part.
What feature of a
shortest path could we
guess that would make
it one edge shorter?
There's probably
two good answers.
Yeah?
The next edge, which I guess
you mean the first edge?
Sure.
Could guess the
first edge, or you
could guess the second edge.
Or no, that would be harder.
Or I mean, the last edge,
that would also work.
Okay, this is a harder one.
Uh, nope.
Good thing there's students
everywhere to catch it.
Cool.
So I'm going to
guess the last edge.
That's just how I've
written the notes.
But first edge would
also work fine.
So I'll call the last
edge x comma v. We
know we end by going into v. So
let's guess the vertex previous
to it in the shortest path.
Now of course, we don't
know what that edge is.
The guessing just means try
them all as you saw last time.
So now, it's really
easy to write
the recurrence-- see if I can do
it without looking at my notes.
So we've got duv of m.
We want to write-- we want
to find the shortest path,
so it's probably going to
be a min on the outside.
And we're going to consider
the paths of the form-- d go
from u to x using fewer edges.
Right, if this is the last edge,
then we use m minus 1 edges
to get to x.
And then, we follow
the edge x comma v.
So I'll just add on the
weight of the edge, x comma
v. If x was the right answer,
this would be the cost
to get from u to v
via x, where xv is
a single edge at the very end.
We don't know what x
should be, so we're just
going to do for loop
over x for x in v.
So this is using
Python notation.
And that will find
the best answer.
Done, easy.
Once you know what
the subproblems are,
once you know what
the guessing is,
basically, I'm just adding
in a min and a for loop
to do the guessing.
So that's my recurrence, except
I should also have a base case.
Here it's especially important.
So base case is going to
be when m is the smallest.
So that, let's say, is 0.
What is the weight of getting
somewhere using 0 edges?
Well, typically it's
going to be infinity.
But there is an interesting
situation, where at 0 namely
when u equals v.
Hey, there is a way
to get from u to
itself with 0 edges.
And that costs 0.
But anywhere else is
going to cost infinity.
There's no path.
So the sort of a
definition, I should say.
If it exists, otherwise
it's infinity.
So then I get those infinities.
But this is kind of important,
because maybe I actually use
fewer than m edges.
I wrote less than
equal to m here.
This is also less than equal
to m minus 1 edges here.
But in some sense, I'm
including the case here,
where x equals v. So I just
stay at v at the very end.
So it's because I have
a 0 here, I'm implicitly
including a situation
where actually, I
just use duv m minus 1.
That's in that min here.
So it's important to
get the base case right.
Cool.
Almost done.
I need an acyclic ordering.
So as I said, things get
smaller when m is smaller.
So all that means is do the
for loop for m on the outside.
Then do the for loop for
u in v. For those, it
doesn't matter what
order you do it,
as long as you've done
all of m equals 0,
before you do all of m equals
1, before you do all of m
equals 2.
So that's the nested for loops
that gives you the right order.
And so, I guess I take this
line and put it on the top.
And I take this line, the
induction of the currents,
put it inside the for loops,
that is my bottom-up dp.
OK, I'm actually going to
write it down explicitly
here for kicks.
Should I bother?
Uh, what the hell.
So first we do a for loop on m.
Then, we're going to do
the for loops on u in V.
Now, inside the for loop,
I want to compute this min.
I could use this
single line, but I'm
going to rewrite it slightly to
connect this back to shortest
paths.
Because this type of
statement should look familiar
from Dijkstra and Bellman-Ford.
This is called the
relaxation step.
OK.
It probably would
look more familiar
if I wrote here w of x v.
You could also write wxv.
That's an alternative
for both of these--
probably should
write the same way.
But in either case, we call
this a relaxation step,
because-- it's kind
of a technical reason
but-- what we'd
like to satisfy--
we know that shortest paths
should satisfy the triangle
inequality.
If you look, there's three
vertices involved here,
u, v, and x.
We're looking at the
shortest path from u
to v compared to the
shortest path for u to x,
and the shortest
path from x to v.
Certainly, the shortest
way to get from u to v
should be less than or equal
to the shortest path of u
to x plus x to v, because
one way to get from u to v
is to go via x.
So this if condition
would be a violation
of the triangle inequality.
It means we definitely do
not have the right distance
estimates if duv is
greater than ux plus xv.
OK.
So if it's greater, we're
going to set it equal.
Because here, we know a way
to get from u to v via x.
We know that it's possible to do
this, assuming our d values are
always upper bounds on reality.
Then, this will be an
upper bound on the best way
to get from u to v.
So this is clearly
a valid thing to do.
Relaxations are never bad.
If you start high,
these will always
improve your shortest paths,
so you get better estimates.
And that's exactly what
Dijkstra and Bellman-Ford did,
maybe with w instead
of d, but they're all
about fixing the
triangle inequality.
And in general and
optimization, there's
this notion of, if
you have a constraint,
an inequality constraint
like the triangle inequality,
and it's violated,
then you try to fix it
by the successive relaxations.
So that's where the
term comes from--
doesn't really matter here,
but all of our shortest path
algorithms are going
to do relaxations.
All the shortest path algorithms
I know do relaxations.
So this is familiar, but it's
also doing the same thing here.
I just expanded out
the min as a for loop
over x, each time checking
whether each successive entry
is better than
what I had already.
And if it is, I update.
So in the end, this will
compute a min, more or less.
I cheated also because I
omitted the superscripts here.
If I put m here, and m
minus 1, here and 1 here,
it would be exactly
the same algorithm.
I'm omitting all
the superscripts,
because it can only help me.
Relaxing more can
only get better.
And if I was guaranteed
correct over there,
I'll still be guaranteed
correct over here.
You have to improve
the invariant,
but you never--
relaxation is always safe.
If you start with upper bounds,
you always remain upper bounds.
You're doing at least the
relaxations over there.
And so you will in the end
compute the correct shortest
path weights.
The advantage of that,
mainly, is I save space.
And also, it's simpler.
So now I only need
quadratic space.
If I had a superscript,
I'd need cubic space.
Right?
So I did a little simplification
going from the five step
process to here-- both of the
polynomial time and space,
but this is a little
bit, a little bit better.
But how slow is this algorithm?
How long does it take?
Yeah?
V cubed, that would be great.
V to the fourth, yeah, good.
Sadly, we're not doing so great
yet-- still V to the fourth.
V to the fourth, I guess
I already knew how to do.
That was if I just run
Bellman-Ford V times,
I already knew how to
do V to the fourth.
So I haven't actually
improved anything.
But at least you see, it's all
dynamic programming in there.
So n here is the size of V.
That's probably the first time.
Cool.
I omitted 0, because
there was the base case.
That's done separately in
the line that I didn't write.
So that was dp one.
Time for dp two, unless
there are questions?
Everyone clear so far?
Yeah?
STUDENT: When you
iterate over x,
why do you do you
every [INAUDIBLE]
PROFESSOR: As opposed to--?
STUDENT: Like, just adjacent
vertices [INAUDIBLE]
PROFESSOR: Oh, yeah.
OK.
Good, fair question-- why do I
iterate over all vertices, not
just the incoming?
If I'm writing w of xv, I
could afford to just say, just
consider the incoming vertices.
And that would let
me improve probably
from V to the fourth to V cubed
times-- V squared times E,
I think, if you do
the arithmetic right.
You could do that.
It would be better.
For dense graphs, it's
not going to matter.
For sparse graphs
it will improve
V squared to E, basically.
But we're going
to do even better,
so I'm not going to
try to optimize now.
But, good question.
When I say this at
the moment, if there
is no edge from x to v,
I'm imagining that w of xv
equals infinity.
So that will never
be the minimum choice
to use a non-edge.
I should say that.
If there's no edge here,
I define the weight
to be infinity.
That will just make
algorithms cleaner to write.
But you could optimize
it the way you said.
So, where were you?
Yeah.
Ah, perfect.
OK.
Other questions?
More Frisbee practice?
No?
OK.
So that was dp one.
Let me do dp two.
Not yet, sorry-- diversion.
Diversion is matrix
multiplication.
Before I get to dp
two, I want to talk
about matrix multiplication.
This is a cool connection.
It won't help us directly
for shortest paths,
but still pretty good.
And it will help-- it will
solve another problem especially
fast.
So shortest paths is
also closely linked
to matrix multiplication,
a problem we've
seen a couple of times,
first in the FFT lecture,
and then in the
randomization lecture
for checking matrix multiplies.
So you remember, you're
given two matrices, A and B.
And you want to
compute C equals A
times B. You've seen Strassen's
algorithm to do this.
There's also these--
here A and B are squared.
OK.
So the n by n, product
will be n by n.
So standard approach
for this is n cubed.
With Strassen, if
I can remember,
you can get n to the 2.807.
And if you use CopperSmith
Winograd, you get 2.376.
And then, if you use the
new Vassilevska-William's
algorithm, you get n to
the 2.3728 and so on.
And that's the best
algorithm we know now.
There's some evidence maybe
you can get 2 plus epsilon
for any epsilon.
Turns out, those are going
to help us too much here.
But what I want to show is
that matrix multiplication
is essentially doing
this, if you redefine
what plus and dot mean.
We redefine addition and
multiplication-- talk about
whether that's valid
in the moment--
so remember what is
matrix multiplication?
Cij is a dot product
of a row and a column.
So that's aik with bkIj.
K equals 1 to n.
OK.
Now that sum looks
a lot like that min.
Actually, more like the
way I wrote it over here,
with the d's instead of the w's.
This-- right-- x is the
thing that's varying here.
So this is like, aik plus
bkj, except, I have plus here,
whereas I have times over here.
And I have a min out here,
but I have a sum over here.
So it sounds crazy, but let's
define-- be very confusing
if I said define dot
equals plus, so I'm
going to define a new
world called circle world.
So if I put a circle around
a dot, what I mean is plus.
And if I put a circle around
a plus, what I mean is min.
OK?
So now, if I put a
circle around this dot,
I mean circle everything.
So I've got to circle the
summation, circle this thing.
So then I get shortest paths.
Crazy.
So all right, I'm going to
define d to the m-th power.
I should probably
circle-- whatever.
Slightly different.
OK, I want to define,
like, three things at once.
So let me write them down,
and then talk about them.
So many infinities.
All right.
OK.
I guess, I should
write this too.
OK.
If I define the vert-- suppose
I number the vertices 1
through n, OK?
I just assume all vertices are
an integer between 1 and n.
So then, I can actually express
things in a matrix, namely
the weight matrix.
This kind of defines
the graph, especially
if I say wij is infinity
if there is no edge.
Then, this is the matrix of
all pairwise edge weights.
For every i and j, I
have a weight of ij.
That gives me a matrix, once
I set V to be 1 through n.
Now, I'm also defining this
distance estimate matrix.
So remember, we
defined duvm-- I'm
going to now call it dijm,
because the vertices are
integers.
That is, the weight
of the shortest path
using, at most, m edges.
If I define it that
way, then I can put it
into a matrix, which is for
all pairs of vertices ij.
What is the distance,
shortest pathways
that uses it at most m edges?
That gives me a matrix,
d parenthesis m.
Then, I claim that if I take
circle product between d
of m minus 1 and w, that is
exactly what's happening here,
if you stare at it long enough.
This is the inner
product between row u
of d to the m minus
1, and column v of w.
And that's exactly what this
circle product will compute.
So this is dp.
But when you look at
that statement, that's
saying that d to
the parentheses m
is really w to the
circle power m, right?
This is a definition
in some sense of power,
of exponentiation,
using circle product.
So when I circle
the exponent, that
means I'm doing circle
exponentiation in circle land,
OK?
OK so far?
So this is circle land.
So you might say, well, then I
should compute these products
using matrix multiplication.
Now, just to see how
good we're doing,
if I execute this operation
n times, because I
have to get to d
to the n minus 1--
so it's basically d to the n.
If I do this product n
times, and for each one of I
spend n cubed time, then I get
an n to the four algorithm.
Same algorithm in fact,
exactly the same algorithm--
I've just expressed it
in this new language.
OK, there are two ideas
on the table though.
One is, maybe I could use a
better matrix multiplication
algorithm.
Let's shelve that for a moment.
The other possibility
is, well, maybe I
can exponentiate faster than
multiplying by myself n times,
or multiplying by w n times.
How should I do it?
Repeated squaring, good.
You've seen that
probably in 006.
Repeated squaring idea is, we
take-- to compute w-- well,
I take w to the 0.
I multiply it by w to the 0.
Sorry, circle 0--
that's this thing.
Oh, that seems weird.
Let's start with 1.
1 seems better.
I'm not going to get much if
I multiply that by itself.
I should get exactly
the same matrix.
So I take the circle product
between w to the 1, w to the 1.
That gives me w to the 2.
And then, I take w to
the 2 times w to the 2.
Everything's circled.
I get w to the 4.
Oh cool, I doubled my exponent
with one multiplication.
If I take w to the 4 by w to the
4, I get w to the 8, and so on.
My goal is to get to n, so I
have to do this log n times.
Log n squaring operations,
each squaring operation
is an n cubed thing.
So this is repeated squaring.
And I get V cubed log V--
finally, an improvement.
So we went from V to the 4,
which was in the dense case
the same performance
as Bellman-Ford,
running it V times.
But now in the dense case,
I'm getting V cubed log V,
which is actually pretty good.
It's not quite V
cubed, but close.
All right.
I'm pointing at V cubed.
This is actually the one
result that is not optimal.
This is the one we
want to improve.
But we're kind of-- we're
in this space right now.
We're getting close to as
good is this algorithm,
the Johnson's algorithm.
But we still a log V Factor.
So this is great,
just by translating
into matrix multiplication.
Now technically,
you have to check
that repeated squaring actually
gives you the same result.
Basically, this works because
products are associative.
Circle products of
matrices are associative,
which works because circle
land is a semi-ring.
If you want the
abstract algebra,
a ring is something that
you wear on your a finger.
No.
A ring is an algebra where
you define plus and times,
and you have distributivity.
Semi-ring, there's no minus,
because min has no inverse.
There's no way from
the min to re-compute
the arguments, right?
No matter what you apply to
it, you can't-- you've lost
information.
So that's the semi-ring.
Normally, you have a minus.
But semi-ring is enough
for the repeated squaring
to give you the right answer.
However, semi-ring is not enough
for all these fancy algorithms.
So if you look at Strassen's
algorithm, the one you've seen,
it uses minus.
There's no way to get around
that, as far as we know.
So if you have no minus, n cubed
is the best we know how to do.
So sadly, we cannot improve
beyond this with this
technique.
It sucks, but that's life.
However, we can do something.
If we just change
the problem, there
is another problem which
this is the best way to do.
So let me briefly tell
you about that problem.
It's called transitive closure.
Transitive closure
is, I just want
to know is there a
path from i to j.
So it's going to be 1, if there
exists a path from i to j.
And it's going to
be 0 otherwise.
OK.
I guess it's kind of like
if you set all the weights
to 0 or infinity.
Then, either there's
going to be as 0 way path,
or there's no path, meaning
there's an infinite way path.
So it's not quite the same.
Here, I want 1 and 0.
I flipped.
It used to be, this was
infinity, and this was 0.
This is one saying there is a
path from i and j, 0 otherwise.
If I write it this
way, and then I
think about what
I need to do here,
it is still in some sense
plus and min, but not really.
Because I just want to
know, is there a path?
So if I have a way to get
there and a way to get there,
instead of adding
up those values,
really I'm taking
some other operator.
So I want to know OR.
Yeah, exactly-- who said OR?
Yeah, all right, tough one.
Close, close, close.
So here, we have basically
a circle product is OR,
and circle sum is AND.
OK?
I mean plus and min
would work, but it's
a little bit nicer over here.
Sorry, it's the other
way around, I think.
It's definitely Booleans.
We want to know there is a
way to get to x, and then
from x to where we're going.
That's an AND.
And then, to get
a path in general,
it has to work for some x.
So that's the OR.
And this is a ring.
And once you're ring,
you have negation.
You can apply
Vassilevska-Williams.
And you solve this problem
in n to the 2.3728.
And if I just make a little
change in the dot dot dot,
I can absorb the log.
So you could put a log n here.
And it's log n if you get
the exponent exactly right.
But if you just tweak the
exponent by 0.00000001,
that's bigger than log n.
So we usually omit
the log there.
Cool.
Transitive closure-- so it's
a problem you didn't know you
want to solve, but it is
actually a common problem.
And this is the
best way we know how
to solve it for dense graphs.
OK, it beats, you know, V cubed.
This is the algorithm we're
aiming for for dense graphs.
For sparse graphs,
we can do better.
But for dense graphs,
this is better.
Finally, we get to go to
dynamic programming number
two, also known as the
Floyd-Warshall algorithm.
So we had this dp
in V the fourth.
If we forget about
transitive closure,
we've now are down
to V cubed log V.
Our next goal is to achieve V
cubed, no log V. Let's do that.
So again, I'm going to
express it in my five steps.
First step is, what
are subproblems?
And this is the key
difference, and the key insight
for Floyd-Warshall is to
redefine the dij problems.
To avoid conflict,
I'm going to call
them cij, or in this case,
cuv, because here, the matrix
product view will
not work, I think.
Yeah, it won't work.
So it's a totally
different universe.
I'm still going to assume that
my vertices are numbered 1
through n.
And now, the idea
is, first I'm going
to think about the graph formed
by the vertices 1 though k,
roughly.
And I want to know for every
vertex u and every vertex v,
what is the shortest
path from u to v,
or the weight of the
shortest path from u to v
that only uses intermediate
vertices from 1 through k.
So actually, u and
v might not be--
they might be larger than k.
But I want all the vertices
in the path to be 1 through k.
This is a different way
to slice up my space,
and it's the right way.
This is going to do
a factor of n better.
It turns out, and
that's just an insight
you get from trying all the
dp's you could think of.
And eventually, Floyd and
Warshall found this one,
I think in the '70s.
So it was easier back
then to get a new result.
But I mean, this is very
clever-- so very cool idea.
So now the question is,
what should I guess?
Before I guessed what
the last edge was.
That's not going to
be so useful here.
Can anyone think of a
different thing to guess?
We're trying to solve
this problem where
I get to use
vertices 1 through k,
and presumably I want
to use subproblems
that involve smaller k,
that say involve vertices 1
through k minus 1.
So vertex k is relevant.
What should I guess
about vertex k?
Yeah?
STUDENT: Guess that vertex
k is the [INAUDIBLE]
PROFESSOR: You want
to guess vertex k is
the i-th intermediate vertex.
That would work, but I would
need to parameterize by i here,
and I lose another
factor of n if I do that.
So I'd like to avoid that.
That is a good idea.
Yeah?
STUDENT: [INAUDIBLE] visit
k, before you visit v.
PROFESSOR: You're going
to guess that I visit k,
and then I go to where
I'm trying to go.
OK.
That's not a-- OK.
That's a statement.
But to guess, I should
have multiple choices.
What's my other choice?
STUDENT: [INAUDIBLE]
PROFESSOR: Yes.
So either I use
vertex k, or I don't.
That's the guess-- is k in
the path at all from u to v?
So that's a weaker
thing than saying,
k is at position i in the path.
Here I'm just saying,
is k in the path at all?
And that's nice,
because as you say,
I already know how to get
there without using k.
Because that's cuvk minus 1.
And then, you just also have
to consider the situation where
I go to k, and then I leave.
So the recurrence is going to be
cuvk is the min of two things.
One is when k is not in the
path, that's cuvk minus 1.
And the other option is that
I go to x first-- or sorry,
I go to k first.
It used to be x.
Now, I've renamed it k.
I don't know why.
k minus 1-- and then I go
from k to the v-- k minus 1.
That's it.
Min of two things-- before, I
was taking the min of n things.
Before, there were n
choices for my guess.
Now, there are two
choices for my guess.
Number of subproblems is
the same, still V cubed.
But the guessing part
and the recurrence part
is now constant time
instead of linear time.
So I'm now V cubed
time-- progress.
OK?
This is pretty cool.
The old dp led us to this
world of matrix multiplication.
That's why I covered it.
This new dp is just
a different way
of thinking about
it-- turns out to be
faster, just by log factor,
but a little bit faster.
I need some base cases-- cuv
of 0 is going to be-- now
it's the weight of the edge uv.
It's a different base case.
Before, I was using 0 edges.
Now, it's not using any
intermediate vertices.
So that's how the weights
come into the picture,
because actually there are
no weights of edges up here.
So that's a little weird.
The only place the weights
come in is when k equals 0.
This is in some sense
still relaxation,
but it's a little bit weirder,
little bit different order.
I mean the key thing
here is, because the way
we set up these subproblems
with the intermediate vertices,
we know k is the only
vertex in question.
Before it's like, well, I don't
know where you go at the end.
But now we know that either
k is in there, or it's not.
And in each case, we can compute
it using smaller subproblems
and so we save
that linear factor.
STUDENT: Is this only
for [INAUDIBLE] graphs,
or is does it also [INAUDIBLE]
PROFESSOR: This is
for directed graphs.
u and v are ordered here.
And this is the weight from u
to v-- will work just as well.
It's probably a
little bit instructive
to write this down as
nested for loops again.
Why not?
Because then, you'll see
it's just relaxation again.
So I'll even write the base case
here, because it's very simple.
We're doing k in
order, let's say.
These are really the
same kinds of for loops.
But I'll write them slightly
differently, because here we
care about the order slightly.
Here, we do care
about the order.
Here, we don't care
about the order.
Vertices, and all we're
saying is-- almost exactly
the same code as before.
This is, again, just
a relaxation step.
We're just relaxing
different edges
in different orders, basically,
because k is evolved in here.
We do that for k equals 1.
Then for k equals 2, and so on.
But in the end, it's
just relaxations,
so you can use that to prove
that this actually computes
the right shortest paths.
I won't do that here.
But clearly, cubic time
instead of quartic.
Pretty cool.
That's Floyd-Warshall.
It's very simple.
And so a lot of
people-- if you need
to solve all-pairs shortest
paths in dense graphs,
this is the best
we know how to do.
So this is what you
should implement.
It's like, five lines of code.
And you achieve this bound.
But for our sparse
graphs, we can do better.
And the rest of
lecture is going to be
about Johnson's algorithm,
where for sparse graphs
we're going to get
closer to quadratic time.
We're going to match
running Dijkstra,
and it's V squared log
V plus E times V, sorry.
So when E is small, that's
going to be close to quadratic.
When E is big, it's
going to be cubic, again.
So we'll never be worse than
this Floyd-Warshall algorithm.
But for sparse
graphs it's better.
OK.
Johnson's algorithm.
I was going to make some joke
about Johnson and Johnson,
but I will pass.
So Johnson's algorithm--
I mean, dp is five steps,
but Johnson's algorithm's only
three steps-- clearly simpler.
It's actually much
more complicated,
but it's all about
what the steps are.
So here's the crazy idea
in Johnson's algorithm.
We're going to change
the weights on the edges.
And to do that, we're going
to assign weights to vertices.
We're going to
choose a function h.
Think of it as a height
function, I guess,
that maps vertices
to real numbers.
And then, we're going to
define w sub h of u,v.
This is a new way to think about
edge weights that depends on h
that's defined in a simple way.
It's the old edge weight
plus h of u minus h of v.
You could define
it the other way,
but it's better to
be consistent here.
So this is a way to
tweak edge weights.
For every edge-- this is
for directed graphs clearly.
For u, u is the beginning,
the head of the-- I
don't know if it's
the head or the tail--
the beginning of the edge.
v is the end of the edge.
I'm going to add
on the height of h,
and subtract out
the height of v. OK?
Why?
Because that's the definition.
I want this to be greater
than or equal to 0.
That's the such that.
I want to assign a function
h, so that these new weights
are all greater or equal to 0.
This is for all u and
v. Why would I do that?
STUDENT: To use
Dijkstra instead of--
PROFESSOR: To use Dijkstra
instead of Bellman-Ford,
exactly.
So that's step 2.
Run Dijkstra on, I
guess, the usual graph.
But now, this new
weight function,
w sub h, if all the
weights are non-negative,
I can run Dijkstra.
So this will give me what I
call the shortest path sub
h of u comma v for all u and v.
It doesn't give me the actual
shortest path weights I want.
It gives me the shortest
path weights using this wh.
But I claim that's
almost the same.
I claim that this
re-weighting preserves
which paths are shortest.
Because-- so in particular,
I claim that delta of u,v is
delta sub h of u,v--
should be the other way--
minus h of u plus h of v. OK.
If this was a single edge, you
can see I'm just cancelling off
these terms.
But in fact, I claim for a whole
path, every path from u to v
gets changed by exactly
the same amount.
So this is a claim about
the shortest path--
in effect, a claim
for every path from u
to v, shortest or not.
If I measure it in
regular weights w,
versus weights w sub
h, the only difference
is this fixed amount,
which depends only on u
and v-- does not
depend on the path.
And therefore, which paths
are shortest are preserved.
And so when we compute
these shortest path weights,
we can translate
them back to what
they should be in the
original weighting function.
And furthermore, if you
have parent pointers,
and you actually find the paths,
the paths will be the same.
Shortest paths will be the same.
OK.
So let's prove that claim.
It's actually really simple.
Let's look at a
path from u to v.
So I'm going to label the
vertices along the path.
V0 is going to be u.
That's the first one, then
V1, then V2, and so on.
Let's say path has length k.
And Vk is v. OK, that's just
a generic path from u to v.
And now, I want to compute
the w sub h of that path.
Excuse me.
So the weight of a
path is just the sum
of the weights the edges.
So I could write this as
a sum from i equals 1 to k
of w sub h of Vi
minus 1 comma Vi.
I think that works,
got to be careful not
to get the indices wrong.
OK, now, w sub h is defined
to be this thing-- w
plus h of u minus h of v. So
this is the sum i equals 1 to k
of w Vi minus 1 Vi plus h
of Vi minus 1 minus h of Vi.
What does the sum do?
Telescope.
So success-- this Vi is going
to-- this negative h of Vi
is going to cancel with
the plus h of Vi minus 1
in the next term, except for
the very first one and the very
last one.
So this is going to be
this sum, which is just
the weight of the path according
to regular weight function,
plus h of V0 minus h of Vk.
And that is just the weight to
the path plus h of u minus h
of v. Did I get it right?
Nope.
Yes?
STUDENT: [INAUDIBLE]
subtract [INAUDIBLE]
PROFESSOR: But it's not-- it's
opposite of what I claimed.
So right, because it's
the other side, good.
This has h on the
right hand side.
This has not h on
the left hand side.
But here, I have h on the
left hand side, and not h
on the right hand side.
So if I flip it around,
if I take these two terms,
put them on the
left hand side, then
I get this with the right sign.
Cool, whew.
Self-consistent.
OK.
So this was talking
about are arbitrary path.
And so this is proving
the stronger thing
I said, that every
path gets lengthened
by this function,
which is purely
a function of the endpoints.
So in particular, that means
the shortest path in w land
will still be the shortest
path in w sub h land--
slightly less cool name than
circle land, but oh well.
All right, so this means
shortest paths are preserved.
Shortest paths are
still shortest.
And therefore, if I look at
the delta function, which
is about the shortest path
weights, this claim holds.
So that's the
proof of the claim.
Cool.
There's one gaping problem
with this algorithm,
which is how in the
world do you find this h?
If we could find h, then we
know we could run Dijkstra,
and we can do this thing.
And Dijkstra is going to
cost the VE plus V squared
log V. I didn't say it, but
we run V times Dijkstra.
All right, we run it V times.
That's going to take V
squared log V plus VE to do.
This is just going to
take quadratic time,
V squared to update
all the weights,
update all the delta functions.
The missing step is how do
we find this weight function?
I claim this problem of finding
h that has this property,
is very closely related
to shortest paths.
It's weird, but we're going
to use shortest paths to solve
shortest paths.
So let's do it.
Step 1, finding h.
What I want to do, so I
want to have w of u,v--
let me just copy that down--
plus h of u plus h of v to be
greater than or equal to 0.
Whoops, minus.
I'm going to put the h's
on to the right hand side,
and then flip it all around.
So this is like saying h of
v minus h of u is less than
or equal to w of
u,v for all u and v.
This is a problem we
want to solve, right?
w's are given.
h's are unknowns.
This is called a system
of difference constraints.
If you've heard about linear
programming, for example,
this is a special case
of linear programming.
Don't worry if you haven't
heard, because this
is an easy special case.
We're going to solve it
much faster than we know
how to solve lineal programs.
It's a particular kind of thing.
This is actually useful problem.
You could think of,
these are maybe times
that various events happen.
And these are constraints
about pairs of them.
Says, well, the start
time of this event
minus the end time of that
event should be less than
or equal to 1 second.
You can use this to do
temporal programming,
if you could solve
these systems.
We're going to solve
these systems, when
they have a solution.
They don't always have a
solution, which is a bit weird,
because we're relying on them
always having a solution.
How can that be?
Negative weight cycles.
This is all going to work, when
we don't have negative weight
cycles.
And that's exactly going
to be the case when
this system of difference
constraints has no solution.
So let me show you that
in a couple of steps.
First theorem is that if the
graph V,E,w has a negative
weight cycle, then that
system has no solution--
no solution to the
difference constraints.
This is going to be,
again, an easy proof,
kind of similar to
last one actually.
So consider a
negative weight cycle.
Let's call it V0, to V1,
to V2, to Vk, back to V0.
So the claim is the sum of
these weights is negative.
And now, I'm just going to
write down these constraints,
which are supposed to have a
solution, or maybe they won't.
So if it has a
solution, then this
must be true, where u
and v are plugged into be
Vi and Vi minus 1, because
those are all edges.
So I'm going to write h of
V1 minus h of V0 is less than
or equal to w of V0 wV1.
And then h of V2 minus
h of V1 less than
or equal to w of V1 V2.
Repeat that k times,
I'm going to get
h of Vk minus h of Vk minus 1.
And then, the last
one, the wrap around h
of V0 minus h of Vk w of V--
did I get it right-- Vk V0.
What do I do with
these inequalities?
Sum them.
Time for a Good Will Hunting
moment-- do you remember?
I hope we've all seen
Good Will Hunting.
I don't have a
janitor here, so I
have to do all cancels
by hand-- and this.
So I end up with
0 at the bottom.
Everything cancels, and then,
over here I have less than
or equal to the weight
of the whole cycle.
I'm just adding up the
weight of the cycle.
I didn't give the cycle
a name-- call it C.
Now, the cycle has
negative weight.
So this is less than zero,
strictly less than zero.
So we're saying that 0
is strictly less than 0.
That's not true.
So that means
there's no way to get
all of these constraints
simultaneously true-- proof
by a contradiction.
So that establishes a
connection in the direction
we don't want it.
What we want is
they're converse,
which is if there's no
negative weight cycle,
then there is a solution.
Luckily, that is also true.
But this is a little
easier to see.
So now, we do the other half.
OK.
And this will-- I mean, it's
going to be constructive proof.
So we're going to
actually know how to solve
this problem with an algorithm.
So it's going to be-- there
is a negative weight cycle if
and only if there's no solution.
So in particular, the
case we care about
is if there's no
negative weight cycle,
then there is a solution.
We kind of care about both,
but this is the more practical
direction.
So let's prove it.
You can already
see-- you've seen
that there's a connection
in negative weight cycles.
Now, I'm going to show there's
a real connection to shortest
paths.
Negative weight
cycles are just kind
of a symptom of the shortest
paths being involved.
So now, we're going
to use shortest paths.
Suppose we have some graph.
I'm going to draw a simple
little graph with weights.
What I'd like to do is
compute shortest path
from a single source
in this graph.
The question is, which source?
Because none of the vertices--
I guess in this case,
this would be a pretty good
source, because it can reach.
From here, I can
get to every node.
But in general-- maybe
there's another vertex
here-- draw a more
complicated picture.
It could be, there's
no one vertex
that can reach all the others.
For example, it may be
the graph is disconnected.
That's a good example.
So there's no single source
that can reach everywhere.
I really want to
reach everywhere.
So what am I going to do?
Add a new source.
Call it s.
I'm going to add an edge
to every other vertex.
Now, I can get
everywhere from s.
OK?
What are the weights?
0.
0 sounds good.
I don't want to change the
weights, in some sense.
So I put 0, and add
0 to everything.
That's not going to change much.
Now, notice I add no
cycles to the graph.
So if there were no negative
weight cycles before,
still no negative weight
cycles, because the cycles are
the same as they were before.
But now, from s I
can reach everywhere.
If there's no negative
weight cycles,
that means there's a
well-defined, finite value
for delta of s comma v
for all V. And that is h.
What?
It's crazy man.
All right, so we add s to
V. We're going to add s
comma v to e for all
V. That's the old V.
And I'm going to set
the weight of s comma v
to be 0 for all V. OK,
that's what i just did.
And so now, delta of s
comma v is finite for all V.
It's not plus
infinity, because I
know there is-- it's got
to be less than 0, right?
I can get from s to everywhere.
So it's less than
positive infinity.
It's also not negative
infinity, because I've
assumed there's no negative
weight cycles anywhere.
So I'm going to let h
of v be delta of s,v.
I claim that just
works, magically.
That's insane.
Every time I see it, it's
like, got to be crazy
man-- crazy but correct.
That's Johnson.
It's like you just pray that
this happens, and it works.
Why would it happen?
Why would it be that--
what do we want to say--
w of u,v plus h of
u minus h of v--
we want this to be greater
than or equal to 0.
I guess I had already
rewritten this way.
Neither way is the right
way, so it doesn't matter.
So let's see.
We have a weight of u,v. We have
the shortest path from s to u.
And we have this the
shortest pathway from s
to v. We want that to be
greater than or equal to 0.
Why?
Put this over there, and I get
delta s,v is less than or equal
to delta of s,u plus
w of u,v, which is?
Triangle inequality,
which is true.
It turns out, this thing we've
been staring at for so long
is actually just
triangle inequality.
So of course we want to
compute shortest paths,
because shortest paths
satisfy triangle inequality.
The whole name of the
game in shortest paths
is to find a place
where you don't satisfy
triangle inequality and fix it.
So if it makes sense, if
that's possible to do,
Bellman-Ford will do it.
So how we're going to do step 1?
We're going to run
Bellman-Ford once.
We're going to add this
source vertex, so that there
is a clear source to
run Bellman-Ford from,
and then, run Bellman-Ford
from there only.
That will give us
a weight function
for the vertices,
namely how long
does it take to get from
s to those vertices.
Those weights will
actually all be negative.
But then, we're going
to modify all the edge
weights according to
this formula, which
negates some of them.
So some of them are going
to go up some, some of them
are going to go down.
It's kind of weird.
But when we're done,
all of the weights
will be non-negative because
we had triangle inequality.
And now, we can run
Dijkstra from every vertex.
So it's like we
bootstrap a little bit.
We run Bellman-Ford
once, because we know
it handles negative weights.
It will also tell us if there
are any negative weight cycles.
That's why we want this theorem.
Maybe Bellman-Ford says, I can't
satisfy triangle inequality,
because there's a
negative weight cycle.
I don't know what to do.
Then, we know, well actually,
then there was no solution.
OK, that's kind of interesting.
But then, we'll have to
deal with the shortest
paths-- sorry-- deal with
those negative weight cycles.
I won't cover how
to do that here.
But you can.
And otherwise, there's no
negative weight cycles, then
Bellman-Ford finds valid h.
Then, we plug that h into here.
Then, we have
non-negative weights.
So in VE time, we've
reduced to the non-negative
all-pair shortest paths.
And then, we run
Dijkstra V times.
Then, we get almost
our answers, but we
have to modify them to get
back the correct weights
on our shortest paths.
And so we computed shortest
paths in V squared log V
plus VE, because this is
how much Dijkstra costs,
and because Bellman-Ford
takes less time.
We're good.
That's the magic.
And that's all-pairs
shortest paths.
we have a wonderful program coming up
I'm David Eisenbach I'm very pleased to
inch to welcome you to this mathematical
horizons 12 wonderful speakers and I'm
looking forward to their talk first is
Sarov Chatterjee he was a student of
persi diaconis at Stanford then i'm
happy to say he came to Berkeley
unfortunately he left Berkeley again
after a while and went to New York and
then came back to Stanford but at least
he's our near neighbor and a frequent
collaborator and visitor here he won the
rollo davidson prize in 2010 and most
recently lavey prize in probability in
2013 and i'm looking forward to his talk
on nonlinear large deviations Seraph
thank you for the gracious introduction
and thank the organizers for the
invitation so so this is a something
that I have been working on for the last
few years so I was asked to prepare the
talk at the level of incoming graduate
students so that's what I did I don't
see that tiny Gavin graduate students
here up so I'll do all the senior people
you know please don't feel insulted if
it's too easy so I'll try to okay so so
let me start by saying a few things
about the area of large deviations you
know it's a technical part of
probability theory and there are two
main goals of large deviations theory
one is to study the probabilities of
rare events so events that are very rare
but probably is a very tiny but you want
to want to understand how I need is and
you want to understand the conditional
distributions given that some really
vendors have as occurred so so the thing
is the world doesn't look the same if
something unlikely has happened so
things change so you want to understand
how things change you've given the Sun
likely happen so you know a silly
example here so like what is the chance
it will win the lottery and the second
thing is how is a life kind of change if
you win the lottery so that's the
conditional distribution part so often
the second question is more interesting
than first because the first is just a
number it's some small number is a
number the second question is more
complicated you know you don't know how
things are going to be affected if
something very unlikely happens but it's
usually essential to answer the first
question before you want to you know you
you can understand the second so you
want to you have to know the probability
of the event before understanding and
then there are technical reasons for
that why that is nice actually so a
simple example in all the the lottery
example is some real life thing which is
too complicated to capture by
mathematical models but a simple example
starts a fair coin 10 times where
innocent large number and under normal
circumstances this is what you'd expect
it you'd expect to get approximately n
over two heads okay since it's a fair
coin and you also want to know the
number of times should get two
consecutive heads so head and head you
know so so roughly n over four pairs of
these you'll get consecutive heads okay
so that's some simple prob lista
computation that you can do so suppose
the following rare event occur so that
instead of n over two heads you get to
and over three okay so if you're tossing
the coin a thousand times instead of
getting 500 head so you get 666 or more
heads and there are some general purpose
tools I'm going to briefly describe how
these computations are dunce of this
event you can write down the probability
is approximately asymptotically it's
like e to the minus n times some number
okay and moreover you can make
conclusions like the following that if
this happens then this this thing the
number of conservative hads pairs of
consecutive helps that you get instead
of being n over
or it will likely be four and over nine
so it'll increase so you can say this
kind of conditional thing that given
this rare event has happened you you get
you see other things also changing other
things that were very likely before
become unlikely now and things that were
unlikely before become likely now so so
this is a very simple thing you can do
it in many ways if you know if you're
familiar with Sterling's formula and all
that you can it there are 100 different
ways you can do this but you know as in
math there is only one way we
generalizes ok so there are many ways to
do this but there is only one way which
becomes abstract and generalizes to a
more complicated settings so the next
slide will be a little bit of math and
then I'll you know move move back to
this mode where I'm not showing so so
just this one slide of math I have so so
this is how you do this so how is this
estimate obtained this this Pro
ballistic estimate that i wrote down so
this is how you do it's a little bit of
undergraduate exercises so suppose x1 to
xn are independent random variables each
is 0 or 1 it prob'ly a half then a
number of heads in entasis is same as
the sum of these random variables and
then you do this little competition here
so you want to find the chance that you
get at least two and over three heads so
you take some theta you write this event
as saying e to the theta s n is bigger
than it would to theta n over 3 so these
two are the same things now this is the
general purpose markov inequality that
you have in probability that probability
x bigger than t is less than expected
value of x over T so you just write this
down now the nice thing about this one
is that it's a product of independent
random variables so it breaks up as a
product of expectations and each of them
is easy to compute this these are just
soooo one variable so you can write this
down so use markov inequality use
independence and then you optimize this
bound over theta and you get some upper
bound on this probability ok so this is
less than or equal to now the
interesting thing is that this upper
bound is actually the correct asymptotic
and that that requires a different idea
now both this technique and this other
idea which i am not showing here they
generalize to an abstract setting
this is sort of the beginning idea in
the theory of large deviations this is
how you compute large deviation
probabilities in most cases so so you
see there are several things here
they're independent random objects their
summing up to give something and then
you apply this Markov inequality and the
main the key point is that this
expectation breaks up as a product and
then you optimize and you get a bound
and it's unclear why this bound should
give you the right answer but it does
and so this example has this built-in
linearity which allows you to compute
this expectation and this idea
generalizes this is there is a fast
generalization of this this simple idea
and the corresponding idea for the lower
bound and this is the main technique
behind the theory of a classical theory
of large deviation so so you go from the
zero and random variables to more
complicated objects you go to function
spaces and you generalize and you you
get you know amazing things so your
literature spanning 50 years so but the
thing is there are no general tools for
non linear functionals okay so there's
this linearity here okay but there are
no tools for nonlinear functions I'll
give you an example in the next slide
okay what i mean by nonlinear and so you
see you understand how linearity helps
in in the previous slide now you know in
this expectation breaking up as a
product because you get a sum of
independent things or you know any
linear combination would you can do the
same thing but it nonlinear you cannot
do the same trick okay so so here is a
real life example sort of so analysis of
real world network so this is becoming
more important these days and you know
lots and lots of papers now really the
thing is rarely vents on networks large
deviations or networks are often
nonlinear in nature okay so consider the
following simple model there are n
individuals and any to our friends with
a certain probability P and not friends
with probability 1 minus P and
friendships are all independent okay so
this is known as error shiny model and
this is not realistic mainly because
this assumption that friendships are
independent which is not true in reality
but there is a first step to
understanding real networks okay so so
in graph theoretic terminology and
individual is called a vertex and a
friendship is called sorry it's called
an edge so so you this is a model of a
graph and you have you have these edges
in the graph and you consider a simple
object which is the number of triangles
a number of you know Triplets of people
who are all friends with each other and
there's an easy computation you can
compute that the expected value of the
number of triangles in this graph you
can exactly write it down so this isn't
you know n choose 3 is a total number of
triangles and each of them is actually a
triangle in that graphic probability P
cubed and so on so that's so now the
large efficient question is what is the
probability that there are K triangles
where K is some number much bigger than
this expected value so what is the
chance that there are many more
triangles than what you would expect ok
so that's a that's a basic question that
you can ask about the graph and what
does a graph look like if such a rare
event happens so if if you observe many
more triangles you know you observe you
know three people being all friends with
each other many more times than you
would expect just by chance now what
does a graph look like so this is an
example of a nonlinear problem because
the number of triangles is a nonlinear
function of the adjacency matrix they're
using the matrix of the graph is you
know make the matrix with which which
has zeros are ones depending on whether
there is an edge between I&amp;J or not and
this is a Phi entries are all
independent and this is a nonlinear
function of Zen tree so this is a very
simple basic nonlinear question that one
of the simplest you can think of however
the strange thing is that you know this
was open so this was completely or
people didn't know how to do this this
large so so this this thing about a
number of heads in n coin tosses this
was a basic linear question and you know
it is very old peep
knew how to do this for a very long time
but this this question this you go one
step higher you just take a random graph
it all independent edges and you look at
the number of triangles and what is the
chance that you know there are many more
triangles and you expect or what is a
what does the graph look like if so by
many more I mean maybe two times or you
know 1.5 times more triangle so if it
was just a little bit more people
understood that you know very small
increasingly more people understood but
if it was a fraction more you know there
was no understanding of firm you know
how to approach how to even approach
this question so this was done so I you
know together with raghav are done at
NYU where we did this in 2011 and so the
so this theory brought together things
from various areas so it's a bunch of
large deviation techniques and they were
you know semi his regular dilemma and
then there was the graph limit theory
developed by loafers and quarter so
various things came together which was
which was pleasing so so here is an
example the kind of results so i'll give
you in this slide i'll give you a very
counterintuitive sort of result that you
get from this this investigation so we
call this model you have n individuals
any to our friends with probit be
independently and TB the number of
triangles let ET be the expected number
of triangles and so you want to know
what is the most likely structure of the
graph if this rare event happens that
the number of triangles is a fraction
more than the expected value so so there
are various possibilities one is that
all the extra triangles are contain a
small subset of vertices which i have
high connectivity among themselves so
the graph can look like that there is a
small bunch of vertices which are highly
connected they sort of click of people
so that they give all the triangles or
it can happen because there is a nexus
number of edges spread independence just
that there are many more friendships
than you would expect so just uniformly
distributed so what you know what is it
you know what is what is the most likely
scenario
so surprisingly the theory implies that
both scenarios can happen okay so that
is that was a surprise to us so more
precisely it's the following so I'll
cite in words in the next slide but just
let me cite in math in this slide there
exists a delta 1 and Delta two two
numbers so that if if this Delta here is
between Delta Delta 1 and 0 are bigger
than Delta 2 then conditional on his
event the graph just behaves like an
addition any graph so it just behaves as
if there were an external extra number
of friendships or edges spread out
uniformly all over okay on the other
hand if Delta is between delta 1 delta 2
then the conditional structure is
different so then there is a clustering
okay so that you know so these and I
should say that then you know that there
was this very nice work of Lubezki and
Ja afterwards where they've found this
formulas for Delta 1 and Delta too you
know so there's this symmetric and
broken symmetry regime and they found
the exact boundaries between so they had
a phase diagram in the language of
statistical mechanics okay so so in
words what this means is the following
that if the number of triangles exceeds
the expected value by a little bit or by
a lot then the most likely scenario is
that the excess number of edges spread
uniformly okay so you have this graph
you observe a nexus number of triangles
if the excess number is a small fraction
more or is a large fraction more then
you would expect the error shiny
behavior then you would expect that the
all the edges are spread uniformly
whereas if the if it belongs to middle
range then there is a clustering of
edges then you can say that with high
probability there is a clustering of
edges and the exact nature of this is
still not fully understood so in the
sense of this graph limit theory which I
you know don't have time to go into now
but it comes as a solution of
variational problem and we cannot solve
the problem but you can show that there
is a clustering okay and the thing is so
this is some of this demonstrates the
value of mathematics because these
results cannot be guests and I still
don't know what's the intuition that you
get this sort of uniform behavior
if you have a little more of a lot more
but somewhere in between you get a
clustering so you know I you know we
don't have any understanding of why this
happens it just comes out of of the
calculations that earring on you know
now we have tools by which to calculate
things and these calculations can yield
these results which you know you you
cannot you know justify by other means
any questions now yes it is a phase
transition it is a phase transition so
so you know I I wasn't you know wanting
to go into that so it's in a statistical
mechanics language there is an order
parameter which is what is called a
graph on in the graph limit theory and
this order parameter so there is a
variation problem whose solution can
either be a constant function which is a
symmetric regime or it can be a non
constant function so is a broken
symmetry region of broken symmetry so
and and there can be more than you know
so we identify these two phase
transitions that happen there can be
more happening in between and we still
don't know whether whether it happens
yes there can be you know we don't know
it doesn't it doesn't yeah yeah yes
I know they depend on they depend on P
so they don't depend on n because it's
not synthetic result so so this result p
is fixed but n is going to infinity okay
so so they depend on p delta 1 and delta
2 so this was a very you know clever
piece of work where they actually found
this in a wing we tried very hard arago
and i tried very hard to get this but we
we couldn't pictures what pictures of
graphs representing the cluster know so
the thing is there is a days of
difficulty doing that because you have
to okay so this is another value of
large deviations you have to simulate
conditional or rare event now the event
is rare right so so you know the chances
are extremely small so you may have to
wait for many years before you get
something by these simulations but the
thing is rare events do happen you know
hundreds of millions of people by the
lottery and somebody wins so okay so so
rare events do happen okay so then there
are some other applications I there are
these exponential random graph models
which are you know widely used in the
Earth's of real social networks and you
know there are we can say things about
those also now there is an
incompleteness of this theory so this is
large equation 3 4 random graphs it's I
know it was fairly well developed in in
the pin in our paper and subsequent
papers what has one very serious
shortcoming which is that it applies
only to dense graphs so what does it
mean so a graph is called dense if most
vertices are connected to a sizable
fraction of other vertices okay so for
instance in the editor shiny graph where
n is 10,000 and peas point 3 so there
are 10,000 people and any two are
connected to chance thirty percent then
each person has approximately three
thousand other friends okay which is an
unrealistic scenario so so if you have
10,000 people in a real social network
you know even if the editor any model is
true p would be like point zero one you
know 100 people 100 friends so the real
networks are usually sparse so this
dense case we did just as a first step
to understanding the thing now
the problem is the main issue so there
is a deep in a graph graph theoretic
issue here which is that the graph
theoretic tools that you used for this
analysis samara's lemma and graph limit
theory of lavas and co-authors they are
useful only for the dense case okay and
and this for instance you know
developing a satisfactory version of
Somalia's regularity lemma so this is
you know this is an open problem for 40
years so and i'll tell you why that is
so so i'll tell you why what's what's
the underlying reason why this is this
is challenging so so the main issue is
the following that the problem is to
anywhere the number of graph of the
given set of properties for instance you
have to understand how many grafts are
there with em edges and k vertices okay
this kind of this is the kind of thing
that you have to understand now for
dense graphs this approximate counting
is possible with semi radius lemma so
how is this so i won't tell you know i
don't have time to tell you what this
this thing this lemma is but what it
does is the following it classifies the
set of all dense graphs and n vertices
in a bounded number of types and the
number of times depends on the desired
accuracy of approximation so you want to
approximately count something and and
you want you know approximation up to
error epsilon so depending on epsilon
you can classify the graphs into into
into type so it's sort of a compactness
thing you can you can and the number of
types will just depend on epsilon and
within each type you can count using
classical large deviation techniques so
that's that's the combination of large
deviations and summer a dilemma that
comes in and the graph limit theory will
allow you to take a limit to infinity so
the embed the whole thing into into an
infinite object so the regularity lemma
is inapplicable in the in the sparse
setting we do not know how to classify
sparse graphs in two types so I'll tell
you a little bit more on how this means
so what this means so for instance an
excess number of triangles in a sparse
graph can occur because there are extra
edges that are distributed uniformly or
a small number of vertices that are
highly interconnected or a small number
of vertices that are high connectivity
to the rest and these small things
happen in very tiny regions in sparse
graphs so in a dense graphs very tiny
regions don't matter so you can just
forget about them but in sparse graphs
even very tiny regions can give rise to
a serious anomalies and this
classification you know what are the all
possible structures of sparse graphs
this is not yet captured by anything any
tool that we have in graph theory okay
so we really don't understand this now
so we tried hard to get around this
problem and then you know so with me
Denbo at Stanford I have a paper which
is the same title as a dialogue to start
nonlinear large deviations which goes
beyond this graph theoretic setting in
and bypasses a regularity lemma so I
look at it from this different viewpoint
going beyond the graph theoretic setting
and then using this I'll tell you one
outcome of this of this new theory is
that there is this more recent work of
lubinski angele where they show the
following you considered the editor
negraph GNP you look at the probability
that the number of triangles exceeds the
expected value by a fraction Delta now
Delta is fixed but here n is going to
infinity but and P instead of remaining
fixed is going to 0 but okay we cannot
we have there's this barrier so the
theory is still incomplete but going to
0 slower than this then you have this
explicit formula of the probability it's
like e to the minus N squared P squared
log 1 over P times a constant which is
the minimum of these two things and and
and then you know there is a follow-up
papers I'll tell you about that so so
this problem is open for a very long
time this upper tail problem so there is
there is a paper of Swantee handsome
called infamous upper tail this upper
tail problem for triangles so initially
they didn't even know what's right what
are the right exponents on N and P and
then that was fixed and then there was
you know this log factor was missing and
then you know I was one of the people
who fixed that there's a log factor but
this constant dependence on Delta was
was completely open and now now this is
known because of this work which uses
this and this you know this
comes because of some limitations of
this theory of the nonlinear large
deviation theory and this has been
generalized by these four others more
recently where the go beyond triangles
they go to other sub graph counts and
they can handle these these objects okay
okay so here is a here is a summary so
the main problem is that we do not yet
understand the nature of sparse graphs
the thing is we understand some sparse
graph structures but not all possible
structures in totality so the seminary
lemma gives you a classification of all
possible structures that a dense graph
can have so somehow it gives a complete
classification of that you know as a
compactness theorem so it tells you that
it's a compact essentially it's it's
embeddable in a compact space and you
can identify the row correct metric and
so on but we do not understand the
sparse graphs we don't know how to embed
sparse graphs in a compact space and and
you understand the structure by that
method the problem can be solved by by
you know if somebody can prove a
suitable version of summit of the
regularity lemma for sparse graphs but
you know this would have many
consequences including various famous
things of recent times such as the green
tout theorem that but you know this
there is no hope for a solution as far
as I know so so instead what we did is
you know circumvented the problem using
this these tools of nonlinear large
deviations which unfortunately I don't
have the time to tell you what the main
results are and what the things are but
well it they already have several
breakthroughs and the theory is still
not complete you search hard a 10 to the
minus 1 over 42 and you know that should
go to n to the minus a half so that
needs improvement and from the applied
perspective there is a gap between
networks that apply appear in the real
world versus networks with large
deviation properties can be
theoretically analyzed so so many many
networks appearance in the real world
and you would want to understand what
happens if something where
the main problem with rare events
relevant analysis is that there it's
hard to just do a simulation and okay
you don't need to do math you just do a
simulation ok you can wait forever
simulation will never converge so so you
cannot do a rare event analysis using
simulations you need to do the mat and
we don't know how to do that so that's a
problem that needs to be bridged okay
thank you thank you i think we have time
for some questions if people would like
to ask yes yes i get out of your way
yeah laughs no no it doesn't so facebook
you know the first of all the structure
of facebook it's hard to do mathematical
modeling is mainly because the data is
not available they will give us the data
so so in and I don't see any chance of
that so so we I don't know what what's
their interest you know they you know
they have their they have their own
people who are trying to increase
revenues or that that's mainly mainly
what yeah so yeah the question for the
mic okay yes yes oh sorry are there
examples of dense graphs in real life
that we might be familiar with it not so
much so you know for instance if you
consider very small networks like there
is this example of network of students
at Cal Tech which are like 700 some some
students and and in that it's not dense
but let's say if we consider all people
who are friends of friends okay so then
that it's an artificial example of a
dense network so people have some
artificial examples of that I've heard
some other examples which come up
in from biology and so on but not not so
much so real-world graphs are almost
always sparse question there this is a
microphone if one writes down
similarities theorem in a quantitative
way the quantitative balance are
notoriously a kind of weak does that
have any shadow any effect on your
theory so the thing is that was the main
reason why we needed to develop this new
theory because the quantitative bounds
will let you let P go to infinity only
as well you can use a weak weak
regularity lemma and even if you use a
week regularity lemma that the peak and
go to zero only like some negative power
of log N and that's too slow so we
wanted to break that barrier and get to
n to the minus something and you know
that's why we had to develop this new
theory and the graph theorists think
that you know this this theory may have
some bearing on on how to break the
barriers that cannot be cannot be broken
by semi design but you know we aren't
sure about that so you know so but but
if you just use the qualitative
regularity lemma it won't give you
anything like n to the minus some
negative power of n you won't get that
all right thank you very much that was
lovely
you
in our previous lesson we introduced you
to graphs we defined graph as a
mathematical or logical model and talked
about some of the properties and
applications of graph now in this lesson
we will discuss some more properties of
graph but first I want to do a quick
recap of what we have discussed in our
previous lesson a graph can be defined
as an ordered pair of a set of vertices
and a set of edges we use this formal
mathematical notation G equal V II to
define a graph here V is set of vertices
and E is set of edges ordered pair is
just a pair of mathematical objects in
which order of objects in the pair
matters it matters which element is
first and which element is second in the
pair now as we know to denote number of
elements in a set that we also call
cardinality of a set we use the same
notation that we use for modulus or
absolute value so this is how we can
denote number of vertices and number of
edges in a graph number of vertices
would be number of elements in set V and
number of edges would be number of
elements in set E moving forward this is
how I am going to denote number of
vertices and number of edges in all my
explanations now as we have discussed
earlier edges in a graph can either be
directed that is one-way connections or
undirected that is two-way connections a
graph with only directed edges is called
a directed graph or digraph and a graph
with only undirected edges is called an
undirected graph now sometimes all
connections in a graph cannot be treated
as equal so we label edges with some
weight or cost like what I'm showing
here and a graph in which some value is
associated to connections as cost or
weight is called a weighted graph a
graph is unweighted if there is no cost
distinction among edges okay now we can
also have some special kind of edges in
a graph these edges complicate
algorithms and make working with graphs
difficult but I'm going to talk about
them anyway an edge is called a self
loop or self edge if it involves only
one vertex
if both endpoints of energy are same
then it's called a self-loop we can have
a self-loop in both directed and
undirected graphs but the question is
why would we ever have a self-loop in a
graph well sometimes if edges are
depicting some relationship or
connection that's possible with the same
node as origin as well as destination
then we can have a self loop for example
as we have discussed in our previous
lesson interlinked web pages on the
internet or the world wide web can be it
presented as a directed graph a page
with a unique URL can be a node in the
graph and we can have a directed edge if
a page contains link to another page now
we can have a self loop in this graph
because it's very much possible for a
web page to have a link to itself have a
look at this web page my code school
comm / videos in the header we have
links for workouts page problems page
and read your age right now I'm already
on videos page but I can still click on
videos link and all that will happen
with the click is a refresh because I am
already on videos page my origin and
destination are same here so if I'm
representing world wide web as a
directed graph the way we just discussed
then we have a self loop here now the
next special type of edge that I want to
talk about is Multi edge and edge is
called a multi edge if it occurs more
than once in a graph once again we can
have a multi edge in both directed and
undirected graphs
first multi edge that I'm showing you
here is undirected and the second one is
directed now once again the question why
should we ever have a multi edge well
let's say we are representing flight
Network between cities as a graph a city
would be a node and we can have an edge
if there is a direct flight connection
between any two cities but then there
can be multiple flights between a pair
of cities these flights would have
different names and may have different
costs if I want to keep the information
about all the flights in my graph I can
draw multi edges I can draw one directed
edge for each flight and then I can
label
and edge with its cost or any other
property I just labeled edges here with
some random flight numbers now as we
were saying earlier selfloops and multi
edges often complicate working with
graphs the presence means we need to
take extra care while solving problems
if a graph contains no self-loop or
multi edge it's called a simple graph in
our lessons we will mostly be dealing
with simple graphs now I want you to
answer a very simple question given
number of vertices in a simple graph
that is a graph with no self loop or
multi-edge what would be maximum
possible number of edges well let's see
let's say we want to draw a directed
graph with four vertices I have drawn
forward DC's here I will name these
vertices v1 v2 v3 and v4 so this is my
set of vertices number of elements in
set V is 4 now it's perfectly fine if I
choose not to draw any edge here this
will still be a graph set of edges can
be empty nodes can be totally
disconnected so minimum possible number
of edges in a graph is 0 now if this is
a directed graph what do you think can
be maximum number of edges here well
each node can have directed edges to all
other nodes in this figure here each
node can have directed edges to 3 other
nodes we have 4 nodes in total so
maximum possible number of edges here is
4 into 3 that is 12 I have shown edges
originating from a vertex in same color
here this is the maximum that we can
draw if there is no self loop or
multi-edge in general if there are n
vertices then maximum number of edges in
a directed graph would be n into n minus
1 so in a simple directed graph number
of edges would be in this range 0 to n
into n minus 1 now what do you think
would be the maximum for an undirected
graph in an undirected graph we can have
only one bi-directional edge between a
pair of nodes we can't have two edges in
different directions so here the maximum
would be half of the maximum for
directed
so if the graph is simple and undirected
number of edges would be in the range 0
to n into n minus 1 by 2 remember this
is true only if there is no self loop or
multi-edge now if you can see number of
edges in a graph can be really really
large compared to number of vertices
for example if number of vertices in a
directed graph is equal to 10 maximum
number of edges would be 90 if number of
vertices is 100 maximum number of edges
would be 9900 maximum number of edges
would be close to square of number of
vertices a graph is called dense if
number of edges in the graph is close to
maximum possible number of edges that is
if the number of edges is of the order
of square of number of vertices and a
graph is called sparse if the number of
edges is really less typically close to
number of vertices and not more than
that there is no defined boundary for
what can be called dense and what can be
called sparse it all depends on context
but this is an important classification
while working with graphs a lot of
decisions are made based on whether the
graph is dense or sparse for example we
typically choose a different kind of
storage structure in computer's memory
for a dense graph we typically store a
dense graph in something called
adjacency matrix and for a sparse graph
we typically use something called
adjacency list I'll be talking about
adjacency matrix and adjacency lists in
next lesson ok now the next concept that
I want to talk about is concept of path
in a graph a part in a graph is a
sequence of vertices where each adjacent
pair in the sequence is connected by an
edge I'm highlighting a path here in
this example graph the sequence of
vertices a B F H is a path in this graph
now we have an undirected graph here
edges are bi-directional in a directed
graph all edges must also be aligned in
one direction the direction of the path
part is called simple path if no
vertices are repeated and if what
disease are not repeated then edges will
also not be repeated so in a simple path
both vertices and edges are not repeated
this path a bfh that I have highlighted
here is a simple path but we could also
have a path like this here start vertex
is a and end vertex is D in this path
one edge and two vertices are repeated
in graph theory there is some
inconsistency in use of this term path
most of the time when we say path we
mean a simple path and if repetition is
possible we use this term walk so a path
is basically a walk in which new
vertices or edges are repeated of walk
is called a trail if what disease can be
repeated but edges cannot be repeated I
am highlighting a trail here in this
example graph ok now I want to say this
once again walk and path are often used
as synonyms but most often when we say
path we mean simple path a path in which
vertices and edges are not repeated
between two different vertices if there
is a walk in which vertices or edges are
repeated like this walk that I am
showing you here in this example graph
then there must also be a path or simple
path that is a walk in which what
disease or edges would not be repeated
in this walk that I'm showing you here
we are starting at a and we are ending
our walk at C there is a simple path
from A to C with just one edge all we
need to do is we need to avoid going to
be e H D and then coming back again to a
so this is why we mostly talk about
simple path between two vertices because
if any other walk is possible simple
path is also possible and it makes most
sense to look for a simple path so this
is what I'm going to do throughout our
lessons I'm going to say path and by
path L mean simple path and if it's not
a simple path I will say it explicitly
our graph is called strongly connected
if in the graph there is a path from any
vertex to any other vertex if it's an
undirected graph we simply call it
connected and if it's a directed graph
we call it strongly connected in
leftmost and rightmost graphs that I'm
showing you here we have a path from any
vertex to any other vertex but in this
graph in the middle we do not have a
path from any vertex to any other vertex
we cannot go from vertex C to a we can
go from A to C but we cannot go from C
to a so this is not a strongly connected
graph remember if it's an undirected
graph we simply say connected and if
it's a directed graph we say strongly
connected if a directed graph is not
strongly connected but can be turned
into connected graph by treating all
ages as undirected then such a directed
graph is called weakly connected if we
just ignore the directions of the edges
here this is connected but I would
recommend that you just remember connect
it and strongly connected this leftmost
or undirected graph is connected I
removed one of the edges and now this is
not connected now we have two disjoint
connected components here but the graph
overall is not connected connectedness
of a graph is a really important
property if you remember intra-city road
network road network within a city that
would have a lot of one-ways can be
represented as a directed graph now an
intra-city road network should always be
strongly connected we should be able to
reach any street from any street any
intersection to any intersection ok now
that we understand concept of a path
next I want to talk about cycle in a
graph a walk is called a closed walk if
it starts and ends at same vertex like
what I'm showing here and there is one
more condition the length of the walk
must be greater than 0 length of a walk
or path is number of edges in the path
like for this closed walk
that I'm showing you here length is five
because we have five edges in this walk
so a closed walk is walk that starts and
ends at same vertex and the length of
which is greater than zero now some may
call closed walk a cycle but generally
we use the term cycle for a simple cycle
a simple cycle is a closed walk in which
other than start and end vertices no
other vertex or edge is repeated right
now what I'm showing you here in this
example graph is a simple cycle or we
can just say cycle a graph with no
cycles is called an acyclic graph a tree
if drawn with undirected edges would be
an example of an undirected acyclic
graph here in this tree we can have a
closed walk but we cannot have a simple
cycle in this closed walk that I'm
showing you here our edge is repeated
there would be no simple cycle in a tree
and apart from tree we can have other
kind of undirected acyclic graphs also a
tree also has to be connected now we can
also have a directed acyclic graph as
you can see here also we do not have any
cycle you cannot have a path of length
greater than 0 starting and ending at
the same vertex or directed acyclic
graph is often called a dag cycles in a
graph caused a lot of issues in
designing algorithms for problems like
finding shortest route from one vertex
to another and we will talk about cycles
a lot when we will study some of these
at one still go our thumbs and coming
lessons for this lesson I will stop here
now in our next lesson we will discuss
ways of creating and storing graph in
computer's memory this is it for this
lesson thanks for watching

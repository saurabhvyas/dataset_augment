That happened in Costa Rican soil
[Music]
especially when it is obvious getting chis
[Music]
I saw in my case
[Music]
albert
the girl said in youtube mac os x
use your voice
klein
[Music]
[Music]
[Music]
[Music]
bush
meanwhile
be top scorer of the team
in 2008
pizzi
They are topless and explained that following the
that
the connection with my eyes or
alone
bush
allocates only 28
8
bush
bush
the crisis
the country
[Music]
[Music]
[Music]
[Music]
[Music]
configure your important work
lot April
I am very good with an amount of
14 John River over its typical
[Music]
articles
canchi luis behind
physically active to side
their daughters
bush
[Music]
[Music]
laws
[Music]
[Music]
[Music]
although his death
[Music]
[Music]
[Music]
[Music]
[Music]
hear from officials
takes the stage and South smiley moore
[Music]
River deep in san felix
and I had no trouble
studying in France and where there are five
days later
[Music]
I saw right where forgiveness is the
project contains a foreword written
that more
the selection of topics
[Music]
[Music]
brightness or
[Music]
[Music]
It would not be good that's how we are
[Music]
do not
[Music]
efe
at last
It aims to increase
that can lead to his former partner in
crisis , but two of them
bad
or 810 photos and news at February 6 ay ccoo to
you your
santillán are taken without swing 6 x
saxophone solo pulled bank technician
66 66 agustín and which is now
the swing process and process courses
[Music]
while here
the aim is to put the walls
this is it true blood
I'm cycle as are their fans
bush
They are are connected to the alpha plus
laws
their uses
hello I'm Gavin and this is going to
work with my advisor Russell first and
let me introduce the well-known
orthogonal vectors problem we are given
a set of unfolding vectors of the
emergency and usually we consider the
American T is between logarithmic of and
unto the little of one and we want to
decide whether there exists vector U and
V such that their third product is 0 and
when this logarithmic of their simple
algorithm that runs in n square times
slogan and best algorithm bye-bye put
Williams and events in this amount of
time and the orthogonal vectors
conjecture says this problem with still
across logarithmic of the first time
unto the two- little of my of what that
means this problem cannot be solved in
sub quadratic time and this conjecture
is implied by strong eth here is the
graph of the reductions from the MP
complete word we have case in f that
problem and in the polynomial time warm
you have orthogonal vectors and here is
a here is a reduction from and the
casing access to always saying that if
the latter has a sub quadratic time
algorithm then Kasyanov set can be
solved the faster than 2 to the M and
there are a lot of reductions reducing
and for more solvent vectors to into
these problems including a distance LCS
for her distance sparse graph diameter
local alignment and longest common
substring our motivation is that the
casing offset is a complete problem in
in the mpu world and in
polynomial world we want to find us we
want to find a complete list of problems
so we showed that that there is a sparse
case of orthogonal vectors problem that
movie is a special case of this problem
and we can also show that on all these
three arrows and still hold true if we
replace orthogonal vector if we replace
parts orthogonal vectors here and we
also introduce a class of problems which
we call first-order graph properties and
it includes the problems like hitting
set click click and the K terminating
set and many other problems sports
orthogonal vectors is contained in this
class and we also show that this class
of problems are reduced to spar so V or
smart so V is complete in this class of
problems next we show was the first of
your problem means it is equivalent with
the two disjoint sets problem here we
give the input as a bipartite graph with
M edges and and on the left side artists
are the set and on the right side are
the elements if elements belong to a set
we create an edge between them and and
the input is the list of all the M edges
and output is to decide whether there
exists to the two sets that they are
disjoint here in here we consider as the
orthogonal vectors problem and to
disfranchise problems are equivalent if
we consider the orthogonal vectors of
problem in a smart way where we only
records the ones and don't post the
zeros in the input so so we say
orthogonal vectors is a special case of
two destroyers problem
the Parrot Heads of guava is very
unbalanced here the here's a website is
logarithmically small to the left side
and our question is to decide whether
the parasite graph satisfies this logic
formula where there exists set as one
exists at as to such that for all the
elements we have either when the element
is not contained in as one or is not
contained in as two similarly in we have
a sports cast of orthogonal vectors
conjecture where we replaced the time
and we replace the parameter and with
the parameter m next let me introduce
the first watercraft property by craft
we mean a bunch of binary relations plus
a bunch of urinary relations and we have
a fixed first order formula 5 with
cripples one quantifiers the quantifiers
are either there exists or for all the
input is a graph tree with n vertices
and edges notice that the input size is
the is M and we say the model checking
problem for formula Phi is to decide
whether G satisfies five here are some
examples the hidden side problem is to
decide whether there exists hitting CH
such that for all other set as their
existing elements and that is a common
element in the heating set and reset and
in the click click problem we want to
decide and if there exists k vertices
such as there is a pair of edge between
there is an edge between all pairs of
vertices and the K terminating set is to
decide whether there exists k vertices
suggest for all other vertices they're
very
at least and I drew from the dominating
set vertex to this vertex and the K
orthogonal vectors problem is to just
decide if there exists K vectors such
that for all indices at least for some
vector there isn't zero on this vertex
and on this index
by a simple recursive algorithm we can
show this the model checking for k plus
1 quantifier formula is so about in time
until the K minus 1 times m and we
conjecture it to request time I'm to
decay and this conjecture is implied by
strong age and the orthogonal vectors
conjecture and we can call it model
checking conjecture so we solve loss of
generality we can assume that the number
of input size the input size will reduce
the number of edges is near linear to
the number of nodes because otherwise we
can just use the simple an algorithm
above so we can we can always assume the
graph is sparse in this case and for the
dense graphs when Williams show that it
is decidable in time until the K minus 2
plus Omega where Omega is the matrix
multiplication exponent and when k is
greater than or equal to 9 it is
decidable in time unto the K and under
strong eth it requires time on to the
case of this bound is test where case at
least nine our main result is is we show
that the two or so the two disjoint sets
problem is hard for model hiking and
crafts under randomized reductions or we
can say the model checking conjecture is
just another name for sports orthogonal
vectors conjecture and our theorem
states that if two distances has the sub
quadratic time algorithms then the model
checking for k plus 1 qualifier formula
can be solved in time less than I'm to
the K and to prove this result we use
fine green reductions we say for problem
hi one with time t1 it is fine green
irreducible to problem hai to waste time
two if the reduction can preserve the
fine grain time complexity of problems
or we can say if ty 2 can be solved
substantially faster than t2 then PI 1
can be a source of substantially faster
than t1 we can show that for 4k plus 125
problems there is a penguin introduction
to okay 25 problems this can be done by
exhaustive search over the first
qualified variable so from to confirm
the k plus 1 quantifier problem we can
we can reduce it down to three qualifier
problems and we just need to show me the
three qualifier problems are reducible
to two disjoint sets and our steering
one says two teaspoons that is hard in
three qualified problems under
randomized fine groaning reductions
Steven Ruben is implied by theorem 2 and
0 mystery theorem 2 says there's a
reduction form for all exist for all
problems to exist exist for all problems
and the theorems recess there is a
reduction from exists exists for all
problems to two distances problem so
also possible quantifier structure our
list as are listed as follows for all
uses for all and its negation exists for
all exists by serum to can be reduced to
exist exist for all and education for
all for all exists and family rooms
three and they are reducible to two
different stats on the other hand and
this for qualified instructors are
considered as easy problems where we can
have algorithms running in time I'm to
the three halves
here are some examples of these
quantifier structures accreting set and
the Quaffle radius 2 is the first class
and the graph diameter to is the second
class and the three click and the three
independent sets are considered to be
the easy problems well the theorem 2 is
inspired by the respective is also
stated in the last talked in the
previous talk mmm that hitting set is
reducible to orthogonal vectors in the
tensile structure and we can generalize
this result to a sparse graphs and we
can show that the formulas and
quantified by for all exist for all
camping videos to exist exist for all
next we mainly show the proof of theorem
mystery we prove a theorem 3 when from
this following lemma these three
problems are equivalent under randomized
fine green reductions the first is to
destroy nests the second is set
containment and third is to set covering
and noticing this randomized the
reduction is only randomized reduction
other parts of our proof are always
deterministic here's the definitions of
the three problems which we call the
basic problem the first is to this one
says we have seen before and it is
represented by this logic formula and
this bipartite graph and the second
problem is set containment it is to
decide if there exists to stats as one
and as to such that as one is contained
in as to it can be represented by this
logic formula and we're free for all
elements if it's in as one and done is
in us too and we can we can represent
the implication
relation using the negation of you is in
as one or using us to and it can be
represented by this paragraph and to
start cover is similar East assess
whether there exists to assess which
covers the whole universe and it can be
represented by this large formula we can
observe that the first problem has to
negations and this problem has only one
negation and this problem has no
negations and this means if we could
find a way to complement all the sets
whereas to is the chosen form then we
can reduce from the first problem to the
second problem where we can can replace
the purple edges with the blue edges
then it becomes a set containment
instance similarly if we could
complement all the cells where s 1 and s
2 are chosen from then the two disjoint
sets are reducible to a to set cover
problem however we are not able to in
complement the set directly because we
are working on a sparse graph where the
number of edges is almost linear in the
number of nodes so to preserve the
sparsity of the input graph we have to
think about and other reductions our
reduction technique is to use universe
shrinking self reductions on the basic
problems for four basic problem problem
one we can use a randomized self
reduction to the same problem with a
smaller universe new prime and the
sucess as one and as to our map to two
sets as one Prime and s 2 prime then we
can complement the stairs on a small
University you so it becomes as one
prime and as the as 2 prime bar so we
can show complementing on this small
universe the country
the fun growing the complicity on the
sparse graphs here is the example of
reducing from set set containment to two
different states because the graph is
sparse there aren't too many large
degree vertices so for the large degree
vertices there are not many of them we
can do exhaustive search on them and see
if they are in the solutions and then we
remove them from the graph so the graph
has only small degree vertices then we
can stop reduced in this graph to a
smaller universe u prime and we can
complement other sites on the new Prime
and we can show because the degree is
small the error probability is small to
do the universe drinking we use a bloom
filter that randomized that randomly
maps each element of you to t elements
in you frame uniformly at random so last
hp's imprisonment Matthew function and
the funny for each test which is a
subset of you we can define a such as
form which is a subset of U prime for
the set containment problem we can see
if if as one is contained in as two then
at one prime is always contained in as
two prime but if as one is not contained
in as two and and all the elements of s
1 are unfortunately mapped into elements
inside as to frame then an error occurs
we show that the yes instance is always
knife to the yes instances and no
instances with high probability math to
no instances and the waste small
probability math too yes instances in
particular we show that when the size of
s 2 is small then the probability of pot
of false positive is exponentially small
the last step of this reduction is from
the general for from the general exists
exists for all problems to the basic
problems we define the hybrid problem
which connects the two problems in the
middle the hybrid problem is a
combination of basic problems where the
universe is partitioned to for
destroying the universes and we are
looking for a solution s1 and s2 net
simultaneous need satisfy the four
conditions the first is that on universe
you 0 they are destroying stats on
universe you one as one is contained in
s2 universe youtube as one contains as 2
and on universe you story as one and as
to work covers the universe usually we
show that we can construct an instance
of the hybrid problem of linear size in
linear time family is the conclusion we
we have we have Ruben approved the
relations showed in this graph and in
evil and if one day someone prove that
strong eth is false then the sparse
orthogonal vectors conjecture my still
hold true then we in the polynomial word
we can still have this relation finally
is open problems the first open problem
is to find a complete problem in the k
plus 1 qualifier problems we can we can
find a complete problems worse than
specific quantifier structures for
example k orthogonal vectors is complete
for in problems with que existe no
quantifier and one universal quantifier
but we don't know if this problem is
completed for general k plus 1
quantifier problems the second oven
problem is to find similar
results for dense graphs but dense
graphs are different from the sparse
graphs in the sense that if we magnify
the number of edges we can go through
all the k plus 1 variables using time
I'm to the K so we can save one variable
here but if we measure the number of
vertices intense graphs we won't have
such advantages so the design of
algorithms will be changed and the third
open problem is to extend this result to
the model checking in hierarchy
structures which we mean from web
properties to hyper graph properties the
last open problem is to find the
complete results for dynamic programming
problems the motivation is that we want
to clean up more messy in the reduction
graph so we found many strong eth hard
problems have dynamic programming
algorithms and many of them cannot do
batters and dynamic programming and we
want to know what what properties do
these problems they have in common and
we want to know if there is a complete
problem in this kind of problems
questions
versus Parsons it got much better person
yeah well in in internal turns case they
you already use the algorithms based on
matrix multiplication and in the sparse
graph most algorithms are not based on
it and in an hour example we we heavily
depend on because the graph is sparse
there are not many large degree vertices
and for some was and and forth and for
the large degree versus you can do
exhaustive search and the first multiple
we have some properties
so let's
Johnson's algorithm is a way to find the
shortest paths between all pairs of
vertices in a sparse edge-weighted
directed graph it allows some of the
edge weights to be negative numbers but
no negative weight cycles may exist it
works by using the bellman-ford
algorithm to compute a transformation of
the input graph that removes all
negative weights allowing Dijkstra's
algorithm to be used on the transformed
graph it is named after Donald B Johnson
who first published the technique in
1977 a similar e weighting technique is
also used in sewer ball's algorithm for
finding two disjoint paths of minimum
total lengths between the same two
vertices in a graph with non-negative
edge weights algorithm description
Johnson's algorithm consists of the
following steps first a new node Q is
added to the graph connected by zero
weight edges to each of the other nodes
second the bellman-ford algorithm is
used starting from the new vertex Q to
find for each vertex V the minimum
weight H of a path from Q to V if this
step detects a negative cycle the
algorithm is terminated next the edges
of the original graph re-weighted using
the values computed by the bellman-ford
algorithm an edge from u to V having
length W is given the new length W plus
h minus h finally q is removed and
Dijkstra's algorithm is used to find the
shortest paths from each node s to every
other vertex in the rear weighted graph
example the first three stages of
Johnson's algorithm are depicted in the
illustration below the graph on the left
of the illustration has two negative
edges but no negative cycles at the
center is shown the new vertex Q a
shortest path tree is computed by the
bellman-ford algorithm with Q as
starting vertex and the values H
computed at each other node is the
length of the shortest path from Q to
that node note that these values are all
non positive because Q has
length zero edge to each vertex and the
shortest path can be no longer than that
edge on the right has shown the rear
weighted graph formed by replacing each
edge weight W by W plus h minus H in
this reweighed graph all edge weights
are non-negative but the shortest path
between any two nodes uses the same
sequence of edges as the shortest path
between the same two nodes in the
original graph the algorithm concludes
by applying Dijkstra's algorithm to each
of the four starting nodes in the roux
weighted graph correctness in the rear
weighted graph all paths between a pair
s and T of nodes have the same quantity
H minus H added to them the previous
statement can be proven as follows let P
be an ST path its weight W in the re way
to graph is given by the following
expression every is canceled by in the
previous bracketed expression therefore
we are left with the following
expression for W the bracketed
expression is the way to P in the
original waiting since the rear
weighting had the same amount of the
weight of every ST path a path is a
shortest path in the original weighting
if and only if it is a shortest path
after re-weighting the weight of edges
that belong to a shortest path from Q to
any node is 0 and therefore the lengths
of the shortest paths from Q to every
node become 0 in the ROO weighted graph
however they still remain shortest paths
therefore there can be no negative edges
if a GV had a negative weight after the
rewedding then the 0 length path from q
to you together with this edge would
form a negative length path from Q to be
contradicting the fact that all vertices
have 0 distance from Q the non existence
of negative edges ensures the optimality
of the path is found by Dijkstra's
algorithm the distances in the original
graph may be calculated from the
distances calculated by Dijkstra's
algorithm in the roux weighted graph by
reversing their re weighting
transformation analysis the time
complexity of this algorithm use
Fibonacci heaps in the implementation of
Dijkstra's algorithm is Oh
the algorithm uses Oh time for the
bellman-ford stage of the algorithm and
over each of the instantiation of
Dijkstra's algorithm thus when the graph
is sparse the total time can be faster
than the floyd-warshall algorithm which
solves the same problem in time Oh
come on and I'd like to thank refers own
for inviting me to coast today to give a
seminar so our today and we talked about
our a problem list my co-authors are
senior editor and AM Kazakh about
finding larger highly connected subgraph
when we know some sort of condition
about the sparseness of the host graph
okay so here's the question so if I have
a graph of order n on independence
number alpha then how large or K you
connected subgraph can I find in ng so a
lot in terms of order and K this is our
four text connectivity so if I know the
independence number so that kind of
somehow measures that how sparse or the
graph is so it's a very natural looking
question to ask so let's take our first
observation so if we just consider
taking alpha queex
each with which they're all about the
same the same size so either feeling of
flow n over alpha and then if n is at
most n alpha K then we cannot get this
chalk then we cannot get
we cannot get a ke connected subgraph at
your soul
this is the simple example so if n is at
most n alpha K then there's no K
connected subgraph in this example and
if we have more than allocate and the
the best that we can hope for is well
the biggest is end feeling n over alpha
so in general we cannot hope to get more
than feeling of n over alpha for the
answer and the problem is only
interesting them if n is greater than
alpha K and then some more observation
so K is 1 this is trivial because you
can just take the largest connect the
components in the graph there is only
our most alpha components and also K is
2 is not too difficult you know we can
prove that by considering the block
decomposition of the graph we can remove
an end block under apply induction on
alpha so while it's true if n is greater
than 2 alpha and this cannot be we can
say 2 alpha because we can take take the
paths up with to our vertices so that's
got independence number alpha and this
sir there's no two connected subgraph so
if I have to I can assume case on these
three
okay so here's some a first result so it
sets the fallen so with K and alpha of
these two and G has more than alpha
square K for the phase and independence
number alpha then an overall feeling is
indeed the correct answer for the
largest K you connected R sub graph so
let me give a rough sketch of this proof
so we kind of go by contradiction
throughout so supposed to yourself is
not K connected so then we have a we
have a cut three four four G like this
and our a1 a2 each one non-empty we
don't have any edges between them and
the book that this is up I'm both K
minus one and it's are not too hard to
show that that to independence numbers
how to come to alpha so if alpha 1 and
our two other independence numbers
numbers then they have to come to alpha
and also the the sizes of the two sub
graphs would be approximately the
proportion alpha I over alpha times the
times n yes so about the spec
right so
so and one of these will will be
typically they're having this condition
one of these two parts will will be
large enough so it cannot be K connected
so let's say our oneness of these two
well I'm yes sir then then it's too big
so we cannot be K connected so we can
continue to split up be set here so I
can say split up a 1 so here's another
cut so a 1 and I can move this cut over
to C so this is at most K minus 1 move
the cup over to C so they're like I get
a new thing here now is that moves to K
minus 1 I got 3 and then you can guess
what happens a 1 a 2 a 3 are the 3 some
to alpha and each one so it's at the
same proportion so just after we label
it so so we get that the next step and
then we can keep going up so there's no
edges so we can keep going until we get
to our four of these assuming along the
way we do not find the required K
connected subgraph so I can continue
this process and I end up with
I just continue here with alpha of these
sets
so hopeful okay so the some of them will
come to alpha and here I will have alpha
minus one so I'm going to call this
three price and now the some of the
independence numbers is alpha so each
one must be a creek so each in each unit
furnace number is 1 so we have this
situation now so we're at this stage
here there's no edges between any pair
are so this we can show somewhere along
the way this this is needed like a full
induction phone induction argument and
this is also user because they prove
this here so each Creek has more than K
for the phase and now notice that them
for any protections C Prime the first X
has to dominate one of the creeks
otherwise we have alpha plus 1
independent vs. right so let's say it
dominates a 1 and then it follows side
side we can find a partition of C prime
say D 1 after D alpha
let's I have right there where di every
vertex of the isolates a I so to that
one that
not one okay
and now the largest of the AI Union di
will be cake if k connector than its up
big enough at least n over alpha so oh
so that's the proof so I've missed our
few technical details here but this is
the rough idea or thought this result
alright so okay so where what does that
tell us now okay so with then seen
so we imagine these are the values of n
so then below of okay the problems
trivial that's nothing to to do the prom
server we don't get the K connected
subgraph
are more than our okay we do get we have
solves the problem we do get the K
connected subgraph on at least then over
alpha undone which we've seen already
the other it's not the shop answer so
the problem is also solved are on the
side so now the interesting question is
what happens between alpha K and alpha
is square K right so let's think about
if we're little more than of okay then
can we possibly in fact get get the N
over alpha for the K connected subgraph
so maybe not surprisingly are the answer
is no we can't cannot because otherwise
the problem will be completely the hope
that I with a Finn's right now so yes
that the algorithm okay maybe it's a
rather weak lower bound on N and can we
possibly drive the lower bound all the
way close to our case so for third I'm
going to tell you that it's not
we cannot go all the way down to alpha K
because I have a construction at this
lemma says if alpha 3 I need came more
than alpha then we can have a graph G on
about a little bit more than 2 times
alpha K with no K connected subgraph on
the N over alpha so fine let me just
dive through here I mentioned that so on
so I've been actually visiting professor
Boehm talked the last two weeks and
we've been discussing this problem and
so you actually found a better example
we I think we checked them within find
any mistakes and so you managed to
increase the 2 plus epsilon up to 9/4
and in fact we don't need the K more
than over here and construction fix
person so I will be showing you in a
moment and I still reckon this might not
be optimal but it's it obviously beats
this one so I have to make it double
check this but I think it's correct
so maybe I can mention a little bit
about this one in a moment but let me
just mention this one so here's an
example for alpha is 4 so what happens
here is we have alpha minus one of these
lines here so we have every circle with
a click and every thick line between two
circles means every edge is present and
these are all the sizes and let's look
at the sole so we optimize n subject to
B plus C is K minus 1 because we the
whole graph has to be not K connected so
we choose B plus C of the cut-set right
so we need this condition here and also
our the biggest cake and like that so
cough is this click over here so we need
to optimize subject to this thing less
than n over alpha so this here is the
sum of all these and then to a bit of
optimization and we find find out that n
comes to this expression for some
constant C alpha so for example if we
have 3 &amp; 4 we end up with 6
insects on 8.8 so a little bit between 6
&amp; 8 so 2 times 3 2 hands 4 respectively
ok so just comparing back to the ninth
force here so 4 3 for example it's
better we get 27 over 4 which is more
than six point six that's six point
seven five so this is better for three
and well in general it's better than
this okay so so let me now talk up or
thinking should I show you the other
construction now I couldn't make it do
that so roughly speaking because all
under construction yeah yes so this one
know the names of ever okay we and yes
with this number for the size yes
so it's like this it's actually on I
think that they could still be some
constructions that's more complicated
but this is how this one works
so what's it then website if alpha is
three we have a g3 where from with
roughly 27 over 4 K minus 1 for the safe
and no are this side independence number
is 3a no cake so upon minus 1 or
something so so one less than that's sad
that's value no K connected
subgraph on so far I will show you what
G 3 looks like in a moment so assume for
now we have such a such a graph G 3
satisfying all these and then
furthermore there is a cut-set or figure
well okay maybe our most this is a
couple of g3 so what she just simply
that was our two art and then roughly
five for here and then and how many of
these there are there alpha minus are
three of these so now that was her
construction in fact so assuming the g3
F is this will be G alpha right and the
independence number is alpha you can
maybe check out these are all quakes and
the biggest so this would be n you can
work that off fairly easily and there is
no cater like a sub graph on n over
alpha because
why is that because
I think oh it isn't
No
think it's true if you have a cake I'll
agree so that we can add and it's only
on one side I'm linking to it is yes or
commensalism
either in TP either in here or one of
these and from them so I think that it
can be verified okay so okay so let me
now go on and talk about alpha is 2 and
alpha is 3 I'm trying if I prove the
shop results so here's alpha is 2 here's
the result so now I assume K is at least
3 and if n is at least 4 times K minus 1
and alpha is 2 then we have the answer
so we have at least n over 2 and so
here's a kind of warm-up so proof so
similar to this here if we have G is not
K connected then we have a cut-set and
now the two parts must be complete
because alpha is 2
okay and then we divide into two cases
so if both of them both creeks have at
least K then one of those that's just
observe here in the cut-set we can
partition into two parts are similar to
this here with one part dolmen every
vertex dominates a one in one part and
here every vertex dominate a two so the
bigger one of these two would would be K
connected if R this is true otherwise if
let's say a one is too small then a two
with some calculation it's big enough
and it's a creek so so a two works right
so that's really simple and this results
is sharp because here's an example where
far n is just one less than what we had
four times K minus one and then you can
check that there's no so there are four
K minus 5 for the say Sun n over 2 is 2
K minus 2 but the biggest that we can
only get is D and E here which is 2 K
minus 3 so here so if we drop n just by
one I do that the previous result is no
longer true
okay so first our equals two settled
right so now the main name of my talk
today is to try and prove the case alpha
is 3 2 right so this is going to be my
challenge today so here's our result
so KL is 3 alpha is 3 and this n is at
least this 27 over 4 K minus 1 then we
have the conclusion so now n over 3
alright now I'm going to show you this
graph so hit business actually in my g3
this graph will be my g3 as here
undercut set this business g3 this whole
graph is g3 on this color here is C 1 F
and B so if you sum these three that's
the cut set okay
yes Kim K minus 1 sum of them and so
just maybe ignore the fact that these
might not be integers like maybe a think
of K is 3 more for in this picture
otherwise we put some ceilings and
floors are and we can sort of argue
similarly so this is the cassette and
and then we to get this we users we use
this cut set us here so that's how this
works then right so then this graph we
found out that's over is 3 n is 1 less
and the biggest K connected subgraph are
there are three of them in this triangle
here or this one or this one and then
all three cases are they stay or thumb
up one less than this n over three here
so you can maybe we have believe me on
this
so within come up this randomly so it
came out from now
are the proof that I will show you in a
moment so as you can see on when we get
to Alvarez 3 we already have a very
surprising construction like so this is
telling us that dealing with the
independence now number sometimes just
sort of really strange constructions so
this is what we found on say for example
if we go to alpha is 4 this is why I
kind of maybe not expecting this to be
optimal because it's kind of relatively
simple we just add two more parts onto
onto an existing graph so so just
outputs for we know we get this here and
there might be some other construction
which is better than just adding the two
to queex
okay so let me try and prove this
results yes
so I can say in our paper this is a
10-page proof so so my plan would be to
skip part of few claims that I will just
put up but show you how all the claims
fit together into a proof ok so let me
do the setup ok so
[Music]
so firstly G itself is more connected so
I have
I hope this is my G so I have a cut-set
let's call it s and then into two parts
that the independence numbers have to be
2 &amp; 1 so this Seidler creates one so
with the click let me call this a 2
right so I'm going to fill me necessary
right 1 or 1 1 1 1 would be trivial yeah
so 2 &amp; 1 obviously right so and then
this is true under it um so this is
inventions to make it would be K
collector so a 2 needs to be smaller
than n over 3 so here so uh so here will
be roughly at least 2/3 n so 2/3 n minus
K minus 1 so this will be would be too
big so it has we have to have a cut-set
of this here so like that so I'm going
to call this a 1 1 a 1 2 both must be
creeks I'm going to call this an S star
okay so we can get these facts fairly
easily doing around the size of Crete or
how do you know the a2 is small it's a
fairly large actually so so certainly a
so we're a 2 is our small gonna say that
then then our so in front of me let me
say this first so we can um as before we
can partition the estar into two parts
where let's say s1 here star as 2 star a
nephron term they tell us before right
so if we have if this happens then then
this whole thing would be at least some
what N minus 2 so this whole part here
would be at least this so the larger one
would be at least that and it should be
large enough or otherwise are so we can
either consider if one of them if let's
say a one one is small then then a tree
will be large enough otherwise if they
both are these K then one of them is at
least this bacon but that's true right
so that that's how we can we can
eliminate
eliminators here right and then to get
that how do we do that
and
yes so so to get that a to so because a
so a to we always have fun of this so to
get that second one so if so I can I
assume our I assume this also so if if a
1 1 is of this K then the larger one of
of this one and that's one would would
be what we want our using using the
system we get our two thirds minus K
minus 1 over 2 or something that should
be big enough right or not
from Google what for
oh okay no no I I jumped one step yet
not yet so to get the second one there
so what I actually do with this so I
will define an s3 where so what done
that so s3 is defined so that induced
sub graph K connected on the s3 is
maximum okay so I take a maximum set
within s such that when I complete
together with a two I get a case unlike
a sub graph and then I cannot be fooled
design have a partition of the West
where they must dominate like this right
so then now to get that if we assume
otherwise then well because this is K
connected so if a 1 1 a sub is K then R
then the maximum of so are let's max are
of these two so are a 1 1
well this is a moose n over 3 because
it's K connected so I've got 2/3 / - yes
so that that's too big and one of these
is a would be K connected so I certainly
must have that and then this one what -
like - just some calculation yes okay I
can do that so n - n - a let's do so n -
Fay - right so this is our comfort our
this is 3 and then that that would give
you
I will give you what you want using our
function on n okay right I can I can
show you that one so if we define Bo I
have to define a few things before go on
so okay so now what I'm going to do next
is I'm going to define a fess n a
certain which is upset off s 1 and s 1
star Union of them in such a way that
okay so get rivers
right n is defined to be and I'm also
going to define H so n is a subset of s
1 Union s 1 star such that the soft of H
on a 1 to s 2 star F 2 and n is K
connected on NS maximum okay and I'm
going to define L and L star to be these
two sets here
so s 1 minus n and s 1 star minus n okay
right so now and so now a 1 1 Union how
basic start that's our claim
yes so right so with more than n over 3
because because this part K connected on
H is calculated
so the remain so they take up almost 2/3
n so the root of the rest of them has to
be more than 1/3 and therefore I can
find the cut set for for this here and
I'm going to call this up B B prime not
a be B 1 B 2 and T prime and then what
can we say notice that T Prime so that's
what I've yes notice that T Prime has to
contain a 1 1 because a 1 1 contains
dominating for the same offer of this
here right so then therefore I must have
so I'm going to copy this song here so
so here this is B intersect L whoa okay
so so we're if B we have to be just two
parts here okay you hope you can reserve
T T Prime has to contain any 1 1 because
a 1 1 contains dominating furnaces or
far of the set so therefore that B 1 B 2
so that B has to be outside of a 1 1 so
it must be there and there right
and um this sob not too hard to show we
can yes sir just from this yes but
that's the media from this just take K
minus 1 away and so so B intersect a oh
is this here so in fact this is L star
this is B in personnel and so this thing
here is non empty it's large
all right so here's a very important
fact to notice so we're a one is
actually are different here and then a
one if this whole time here so I cannot
have two for this is an a one and a
third and a 1 Union s1 s2 with the three
of them independent so if I do it's a
that's very easy to three so let's say I
have I have that and what what goes
wrong right because a two is at least K
said as are non neighbor over here and
this is so we assume this is in the pen
so we've got four for the same so it's
not that's not allowed so three here is
a very important thing to keep in mind
right so on this one is also not hard to
show us so you kind of assume assume
that the contrary and you get a
contradiction to this so it's not
terribly hard to show this one okay I'm
not going to prove these three oh I'm
sorry I haven't even defined a and a
prime okay okay so eight prime and a and
Peter okay so
a 1 Union s 1 Union s to the fest is
also back so we must have be able to
have a cut-set so
right so as as usual here for both of
them
this is almost K minus 1 and R be
pleased to have dull edges with between
them under non empty
just the usual cuts right so so then
with this definition up this is not
terribly hard to show our okay so five
six and seven
I am NOT going to prove them but I will
just assume them up takes a small bit of
work but not difficult why so so in
saying that
so this is a good thing to also keep
keep in mind so a1 the sorry a by the
way we also assume we assume our purpose
is to abide by symmetry of our a a prime
and B 1 B 2 ok so up so this is a cut
for for this here
okay so this one here is start and then
P is the rest the things outside that
the a and a prime okay so who let me now
I'm going to define into two cases so
case one so we're going to look at in
here and we asked um if there are four
types of be tuned and this set here so
the case the first cases we assume yes
there is on the case one is start there
okay so let's see
if I can complete this part so a prime
is our most K minus 1 otherwise of a
prime is up these K then you can find 3
for the space independently that will
contradict dot the the thing earlier so
so we can take off what we do so X
like so we can take us to for the face
here and if a prime is on these K then I
can take a non neighbor off of Y so
right by assumption and then the then XY
set will be independent which
contradicts our culture decks are this
fact here okay so three is P connected
okay so a bit of work I know dear ah
okay so America running very behind so
on so let me I would just side complete
this case and then case to us but even
much much more complicated let's see
this case in full and unsee what happens
okay so so we've got this we got this
here our otherwise we have a
contradiction and let's see this here so
firstly right that's that this briefing
here's at least two two thousand on a
prime it's at most K minus one so the K
minus 1 K minus 1 so that is in fact I
put max in with three five words to sing
my song
right so we have this pot and let's see
that this thing is connected so firstly
I will claim that this is a creek so we
are talking about this here so obviously
we have a creek in here so this is a
quick I know so everything here
dominance it so we just have to check
two for the sides over here so let's
check that so if we have two independent
for the phase over here then our I can
just solve so a intersect L star so X OS
so if you can see them under independent
then I can just pick up that anywhere in
a a prime so it might be here for
example and and then they have to be
independent and read again contradiction
from from that earlier observation and
finally I go to add these on so once
again so I claim that everything here
dominates this part here so obviously
that's two and then if we are missing an
edge between something here and
something here I can take I can take a Y
and so we have this ability of fact here
a prime intersect a12 is not empty so I
can take a Y in that set so it would be
like over here
so if that's said this is why and then
once again in the pendant triangle
contradiction
okay so that starts okay let me try and
complete this one I don't know
I really don't have time now so so now
this thing empty so so I cleaned up this
is empty so I have to show that would be
to here it's empty so
and yes to
I'm going to prove this first so ever
if unwise are we can pick so we assumed
only if this was non-empty so we can
pick this with an pick this by a
function and we can pick this is non
empty from back here so and then they're
independent another contradiction so at
least two of them on a one we can see
that so X X's and L star so it's and Y
is in a 1/2 so it's also an yet x and y
are on a 1 so that's not again gives us
a contradiction so therefore we have
this here because yes because a so now
this is true so a does not meet be in to
say L star and neither does a prime
because a Primus so because L star is up
here
and a prime that's down here so it
doesn't meet L star so we have this here
it must be inside T and this
yes we use we use this fuck here with
two suitable for the face to get this
here so yes you can you can believe this
and then under we do a calculation here
so what's happening here so n is the
subset of t be in say a thousand
subtlety on a 1/2 union s 2 star
consists of only two parts are apart in
the second a prime and the part in the
second key so it doesn't meet a here and
so I can split this up into an a prime
part and a teapot which can be absorbed
with these two terms are absorbed into
this key here and using this here I get
the K minus 1 so I have this upper bound
and then this okay so I might have to do
the same one more picture so
and okay so let's see what's happening
right so I'm going to find what this
fence under sets are so all these 3s -
[Music]
it's this one and T prime unit yes t k
finest defense so without without these
two circles condom P intersects I'll
start one so that new thing is so start
there so right and also our let's find
what this is here on this red thing here
is our it's this this awesome here which
is almost 2 K minus 2 times K minus 1
right unnoticed starts
in fact our publicist actually contain
not any course so what what can we say
so therefore
therefore we have far locate with some
calculations we get this here are so
anyway so okay that's the contradiction
to to this facts here right this is
obviously an S and then that this being
started suppose are two times K minus 1
and 2 / Q right so that completes case
one case two well there's a lot more
facts to prove I may need another hour
to do what I think so uh
okay so case two is this so we assume
now B 2 does not meet here so this is
only B 1 here and B 2 has to be inside
here and then we find out all these
things and then put together an argument
I might need another house but ok open
problems ok I try what to say in the
bottom limit so the first one is some so
we saw that so we had on very basically
we have a some thing here and we know
what's going on here and here so this
first problem is just asking
at which point onwards to and we able to
get the cake and like the subgraph or
not least n over alpha so where is this
function f versus optimal function f of
valve okay
right so we know that it has to be more
than variable 9/4
K alpha K roughly and then for small n
so if we let's say less than nine over
four x over K minus one ah
and we know that we cannot get the end
of alpha but we nonetheless how large
can we get instead 4k connectors across
so there are some other nice questions
you can ask but here just like two
problems here were no bound on what how
large you can you know something I don't
know anything except that there cannot
be more than n over alpha so I'm not
sure what the answer is because I yeah
for example yes something's more than
let's say end of a constant alpha for
example so that would be an interesting
question to consider okay
so come to me that thank you
[Applause]
okay so yes i'll talk about graph
parsimony utd's only the most valiant
relationship okay if you remember this
one so this is joint work with emily fox
at university of washington so that
there has been a tremendous interest
where recent years in in the study the
understanding and the modeling of
complex networks so so i'd be interested
in in so in different types of networks
for example so networks were a set of
nodes of vertices with connection
between them that maybe the right kiddo
and the right kids for example you got
directed multi graphs sonos may
correspond to two people and an edge
nature from one node to another node may
correspond for example to an email from
one person to another persons who have
also examples for example citations or
the world world wide web we also
interested in simple graphs so now you
just have nodes and undirected
connection between two nodes so the most
popular example is a social network so a
node may correspond to an individual and
and an edge to a connection between
three DVD also here to repress on the
network so the size of the node is
proportional to the degree of that note
or to the number of connection of that
of that node and actually so this this
network is a is the network of
collaborations between oxford and well
week academies so you've got oxford in
blue and well week in rates while jim
mentioned so we expect to have much much
more edges in in the near future so i
didn't want it to put names on the
under gravity because usually a please
one person with the largest node and
then I upset everyone is so so I'd be
also interested in in the last type of
network so bipartite graphs where now
you've got two different types of of
nodes so nodes of type A and nodes of
type B and you only allow connections
between those of different types so for
example here scientist authoring papers
readers or in books internet users
posting messages on forearms or if you
think about recommender systems so
customers buying items so while i'm
interested in am experiencing in
developing a statistical network model
and so why do I want to do that so I may
be interested in basically observing a
network and try to find interpretable
structure in the network so by building
the statistical model I've got some
parameter that I'm able to interpret it
and tell me something about the networks
for example some block structure awesome
Layton's parameter that may explain the
connectivity of that graph may be
interested more in in predictions so for
example I will be interested in
predicting missing edges in the network
or may be interested in predicting the
connection of new nodes so if you think
about the customer buying items so I've
observed so some subset of the customer
having bought some sort of items and
then I've got a new customer and we want
to predict the east set of connection so
we so we've got now some massive
networks which has on massive size of
just put like latest estimated thing
from from various companies for example
linkedin so three hundred millions user
facebook claims more than a billion
Twitter to 300 million in the world well
well to a billion soap so we've got very
very large network and three
we've got sort of to hem so first so we
want models that are able to capture the
large-scale properties of network and so
I'll discuss that in a minute and we
also want once we've defined this this
model we want also a scalable in France
algorithm to learn to learn the
parameters of all those owners and so in
terms of of property in these talks I
will focus on the on the on the
following properties which is a total
poverty or network which is a spa cities
what is the definition of sparsity so if
I write so an e so so think about from
now thing about a simple graph n is the
number of edges in the in the network
and n is the number of nodes so we we
call a graph dance if the number of
edges scales quadratically with a number
of nodes so it grows basically as the
same rate as the maximum number of
connection and and what I we call sparse
so there may be different definition of
a sparse graph at wellick I will call a
sparse graph is basically a graph which
is not dense okay so where the number of
edges grows sub quadratically with a
number or not so it is regularly so
admitted that most of the real world
network are actually sparse and now are
not dance okay so this is not because
the size of Facebook double that the
number of your friends will double at
the same time and so what comes with
sparsity what is related is it's also a
very heavy tell degree distribution so
the degree of a node is the number of
connections of that node and for many of
the real world network so the degree
distribution are heavy tail so whether
or not its power low it's actually a
power distribution is some there's some
matter of debate but actually it's
anyway it's very heavy 10 and what you
expect also in real world network so is
having some sort of a late
structures or some block structure were
you more likely to a more connection
between nodes of a certain type than
between nodes of of different types so
so I would like so to be able to to
build a statistical network model that
can capture those those properties okay
so just as as an illustration so here's
an example of a bipartite graph so two
types of nodes so one set of nodes is
the readers so this is from the book
crossing community network so we've got
five thousand readers represented at the
top and I've got so the second type of
nodes books so I've got three thirty six
thousand books represented at the bottom
and so again like as the as the previous
picture so the size of the node is
proportional to the degree of that node
so on the top the larger nodes
correspond to readers who have read a
lot of books and at the bottom so larger
nodes correspond to books right by a lot
of Raiders so what you see here so I
don't know if you know some of the books
while animals da Vinci Code so those are
the most popular books in in the data
set so what you see obviously so that
the graph is quite sparse with only a
fraction of the potential possible
connections and there's a high variance
in the degree of the of the node so for
both the leaders and end the books so
some of the videos have a very very high
degree but most of the readers actually
have only read one or two books and the
same for the books and so if you if you
represent the degree distribution so the
distribution of the connection for both
so for the readers and for the books in
your present it on the log-log scale
okay so here I've got the proportion of
reader having read one book two books
etc and near the same sub proportion of
books
right by one-way dirt orders etc so
you've got this sort of a linear trend
so indicating some sort of heavy tail
power low degree distribution so I would
like to be able so to to to build a
model that can capture such such
behavior okay so I so I took now but so
network modeling and the classical
approach route network modeling and I'll
focus now on simple graphs okay so I
don't have any direction so so
undirected graph without self loops and
so typically so we're interesting in
defining the model for the adjacency
matrix so we're with entries binary
entries xij where xij indicate if
there's a connection between load I and
no Jay and so a reasonable assumption
which which is made is an assumption of
of joint exchangeability so intuitively
just means that when you define your
statistical model the order the the way
you order the nose does not have any any
sort of importance so mathematically
what you assume that the distribution of
this infinite matrix xij is invariant
over permutation joint tran permutation
of the hose and the column ok so if you
apply any permutation of the node you
obtain the same the same distribution so
interestingly so there's there's some
representation theorem by I'll dues and
n Hoover so it takes the following form
for for exchangeable binary matrices so
if you make this exchangeability
assumption then you've got this
representation for the matrix so you got
xij given some uniform unenviable you
are you J and some function W from the
unit square 201 so xij just
from a Bernoulli distribution with the
probability of connection w of you I you
J ok so there's some function call it a
literature the grifone and so xij given
this function and give a uniform
variable it's just been we distributed
with probability of connection given by
W of uiuj and so so as noted in a recent
review by all bonds and hoy so you've
got several network models of asian
version of the network model that fit
into these frameworks for example the
erdos-renyi stochastic block models mix
membership stochastic block models by
Jean nonparametric version of stochastic
block model etc so that's fine so does
those model can capture a lot of
structure within the network okay so
however so that there's that there's
only one limitation which is that if you
make this exchangeability assumption
then you've got one consequence which is
that graphs that are represented by an
exchangeable matrix they are either so
trivially Mt all they are tens okay so
either you've got the empty graph or you
obtain necessarily dense graph and so to
quote so the the recency of a paper of
all bonds and also made this this this
remark so the theory clarify the
limitation of exchangeable models so it
shows for example that most asian models
of network data are in army specified so
so me specified in the sense that they
necessarily lead to dance graph and we
believe that most of the real world Rafa
actually as possible so how can we enter
these them so so some people what do
people in the literature do so of course
so you can still work with you
with your adjacency matrix and just give
up infinity exchangeability because if
you have infinite actually beauty then
you've got densities for example you can
work with non-exchangeable generative
models such that the bar Abbas I'll bear
preferential attachment models which is
some sort of a rain falls young process
you you can also what people do so in
the frequentist literature is to to use
so to consider sequences of finitely
exchangeable models xij of n so maybe
the most common is to use some sort of a
specification of the of the glovephone
so you now assume that for sample size n
your xij of n are drawn from a Bernoulli
distribution so where the probability of
connection is this graph on the value of
uiuj times some scaling parameter ho n
where this ho n goes to 0 as n goes to
infinity and so by doing that you obtain
sequences of graph which are actually
sparse so the limitation of using that
so now you've got sequences of model so
so those model are finitely exchangeable
but they they are not consistent under
marginalization okay so it does not
define a proper generative network model
so if you interested in making
prediction what is the distribution of a
connection of a new node given the
previous one so you cannot it doesn't
really make sense to use to use this so
the approach we propose is liz is
different so we we we don't use this we
don't define a model on this district
structures on the matrix so now well we
will do so we use a different
representation for the graphs we will
represent the graph as a marked point
process on the plane and so so what is
interesting that we we we've got sort of
the similar property
as in the in the algebra framework so in
particular there's also representation
theorem in this for this point process
there's a representation by Karen before
jointly exchangeable pawn processes on
the plane and our construction fits in
this in this framework so we've got some
construction based on on completely
under measures and with the following
property so we've got a change ability
in this point process setting we can
have depending on the values of the
parameter we can generate graphs or data
sparse or dance with every 10 degree
distribution and an interesting point is
that so the model we define so we can
derive scalable in France algorithms so
how do we represent the graph so our
represented as a point process on on the
plane ok so I assume that each node has
some location theta I ok theta I in our
place and there's a point at location
theta I theta J either the connection
between ing ok so here this represent a
set of connection with this node theta I
connected to this node theta G and so
you so for the moment so you don't need
to care about the location actually sort
of the the the connectivity will be
actually independent of the location of
the object and I won't try to learn
those this location so what is the
definition of exchange ability for this
fall for this process so basically if I
take any regular grid on the plane of
length H basically and I count the
number of points within each box so it
gives me an infinite dimensional match
eggs then these infinite dimensional
matrix has to be exchangeable in the in
the previous as in the previous
definition ok so if I do any joint
permutation of the walls and the colon I
need to have the the same the same
distribution it has to be invariant and
this has to be valid for any value of
little H so as I said before so if I
make this assumption is that if I've got
this assumption of John tech gbg are
also have some more presentations around
you to calendar ok so now I'll describe
so this is how the process would be
represented so i will have these
properties and so now we describe our
what is the the network model how do i
generate a network so as I've said so we
will assume that the nodes they are
embedded at some location theta I in our
place so don't worry bother but those
location we won't try to learn them and
they do not tune the connectivity and so
I will assume that each known as some
sociability parameter WI so the larger
this parameter the more likely to notice
to connect to to other nodes and the way
I will I will generate so those WI tita
I so I will assume that those WI fita I
they they arise from a completely on the
measure so basically you've got a pure
jump here point process so where the
levy measure new of DWD teachers who is
decomposes as some lady measure that
will tune the here the germs and another
part that will just tune the location of
the of the nodes so here lambda is just
the lebesgue measure ok so the important
part is 0 here that will tune the
distribution here on the on those
sociability parameters and they are so
too important
case is to distinguish so the first one
is when the integral from 0 to infinity
of Worf DW is infinite then so we set an
infinite activity completely on DiMaggio
so we in any interval you will have an
infinite number of germs and if this is
bounded then you will have almost
release or finite number of germs in any
interval 0 T so if I if I think about my
network model so i will have basically
two cases so i observe a graph a set of
connections ok and so in the first case
basically I I have an infinite
population of any 3d population everyone
as his own sociability parameter but
only a finite number of them have a
sociability parameter that is large
enough so that they actually connect to
anyone ok so so I have only a finite
number of people who have connected to
someone and the an infinite number of
the others of no connections at all an
English in the second case I will have a
finite population so and so what we will
see that actually this difference
between a finite and infinite activity
will translate into a sparsity and
density of the of the royalty network ok
so this is the so each node has some
locations heater is on sociability
parameter WI and the way I construct the
network is just the following so the
probability that there is a connection
between node I noj is just so if I is
different from j is 1 minus exponential
minus 2w I WJ ok so the larger WI or WJ
the larger probability of connection
between two nodes and so I've
represented here the process so
graphically so here are represented so W
ok so with the germs that represent the
suitability of the different nodes and
so those two
individuals that we will connect with 5
t 1 minus exponential minus two so WI x
WJ and I've got a connection at this
point okay so so remember if my
complexion measure is infinite activity
i may have here an infinite number of
germs over any interval okay so so this
is the the model so now I'm interested
in in the in the properties of that of
that model and so so i defined the model
on the plane and so how will I grow the
size of the network so basically if you
look at any box here at 0 1 Phi square
you have almost surely a finite number
of connections okay even if you have an
infinite number of germs so basically
the size of the network increases when I
increase the size of this window 0 alpha
square and so I can have represented so
at the bottom this is my point process
represented the set of connection and so
here I've represented so the n alpha the
number of nodes depending on alpha and
an alpha e the number of edges okay so
an alpha and an alpha he so as I
increase the size here of this window so
I increase the value of the number of
nodes and number of edges and so I've
got here to counting processes and so
now so I'm interested in what is the
what is the behavior as alpha tends to
infinity of n alpha e of the number of
edges corresponding to the number of
nodes and so so interestingly so if you
if you assume that soho that shows the
distribution of other germs is non zero
and and you have got some basically that
the the expected value of the sum of the
sociability parameter over any interval
is is bounded then you've got the follow
aggressors so that the number of edges
scale quadratically with the number of
nodes if I've got a finite activity
completely on a measure and its scale
sub quadratically so I've got a sparse
graph if w is infinite activity okay so
in a sense if you use a sort of a
Beijing non parametric model where
you've got an underlay infinite number
of parameter you're in the space regime
otherwise you're in the UN the dentary
and so so to make it slightly more
precise oh so if you I can take so what
I use for the experiment is the
generalized gamma process where you got
two parameters parameters Sigma and the
parameter to hear so this is the levy
intensity and this parameter Sigma in
particular is the most important so you
can take the range between minus
infinity and one and this process so is
infinite activity for Sigma greater or
equal to 0 and so you obtain basically
the oil range of behaviors for Sigma
greater or equal to 0 you obtain sparse
graph and and the graph becomes parser
Arcilla increases and you obtained ends
graph when Sigma is is below zero so
interesting for these changes
interestingly for this class of
processes so you can sample exactly the
graph via some young process and you
obtain also a power law degree
distribution for for Sigma strictly
positive so here are just some
simulation from from the models on the
top left is the just for comparison
elder chronograph with 1000 clothes and
probability of connection of 0.05 so
where you obtained some sort of a dense
graph and here's a simulation from the
model so here for gamma process so when
Sigma is equal to 0 and here you you
increase the value of Sigma Sigma is
equal to 0.5
0.8 so as you increase the value of
Sigma you obtain grabs at our spacer and
spacer and with a view of you the same
degree distribution okay and so and if
you look at now at the degree
distribution for Sigma strictly positive
you obtain those poverty redistribution
so you can look at those three curves
here the the red curves i correspond to
a generals game a process with sigma is
equal to 0.2 0.5 0.8 so this is the same
type of picture as I've shown before so
on the log log plot the degree
distribution and so basically the value
of symbol will tune the the exponent of
the of the polar degree distribution so
that the exponent be higher as you
increase the value of of Sigma and the
parameter 2 is also interpretability
will tune to the cutoff in the tears of
the degree distribution okay and so this
is a simulation from the model so as you
so we stop from the upper left corner
and so here I'm just generating from the
model on the left and on the right so
our represent the associated graph okay
so here I've got the sociability
parameters the set of connectivity and
so there will be a hub appearing at some
point with a hike on purgatory so here
goes along
okay so this is so generated from with
the generalized gamma process with Sigma
which is a strictly positive so you
obtain let's pass sparsely connected
graph okay so it was for the father
model and and and and its property so
now what do we want to do with that so
if you want to make in France so what do
I observe so I just observe obviously
the set of connections between
individual I don't observe location and
what I want to what I want to learn so I
want to learn the sociability of each of
the individual for which have observed
at least one connection and so I may as
I've said I may have an infinite
population of individual with no
connection at all and so I want to learn
w star which is the sum of their sochi
abilities okay so i want to learn the
stability of the person for which
absorbs self connection sociability of
all the rice and some set of parameters
so in permit in particular so alpha
which tunes roughly the size of the
network sigma which will tune the
sparsity and this parameter to that some
exponential cut off and so the way we we
derive the sampler so we introduce a set
of a latent count variable so one for
each edge and we derive so three step
Markov chain Monte Carlo sampler so
we're in the first steps we update the
weights the sociability weights given
the rice using a Hamiltonian Monte Carlo
then we update the total mass so this is
just a scalar ND I parameters using
metropolises things and then we update
the latent counts given the rice is just
using your tongue cated possible
so the two bottlenecks are one and and
three and actually those steps can be
implemented very efficiently so here
you've got the gradient is very cheap to
compute and so here are some
illustration so I i simulated a graph
with this set of parameters and I
obtained and network with 14,000 nodes
and 76,000 edges and so I run mcmc some
players with three chains and 40,000
iteration and just to say that ad it
scares relatively well so here it takes
just 10 minutes on on standard desktop
with with matlab and with not a lot of
engineering and so here are the some
sort of trace plot of the photo
parameter alpha parameter Sigma 2 and
the sum of the sociability parameters
for all the other nodes or showing that
we managed to recover quite well the
unknown parameters and so okay and so
here so some posterior credible interval
for the for the nodes on the left it's
the node with the top top 50 with the
highest degrees okay and on the right
the nodes with the lowest degree so
because the graph is actually power low
all of those nodes are optically one and
so here's on the log scale so just to
show that we managed to to get quite
accurately those those credible
intervals so both for the I degree nodes
and for the load agreed on okay so we we
use this model to to try to assess the
sparsity of real-world network so what
we do so we we use our generalized gamma
process and we tried we learn this
parameter Sigma and we report so the
probability that this parameter Sigma is
greater or equal to 0 so based on a set
of absurd connection and we do that so
for 12 different network so where the
is the number of nodes or ranges from a
thousand to three hundred thousand nodes
and up to a million edges so here just
here examples I don't think I'm good so
it is the largest is the set of a world
wide web connection you got internet
Hooters the animal email data set etc so
there's a large variety of networks so
just to say that in terms of time so so
it's very very quick for the for the
lowest for the small network so it takes
12 up to 10 minutes 44 30 year thousand
nodes and here two hours for for the
largest network and so we report here
the probability that Sigma that the
sparse is graph under the model and some
confidence callable intervals here for
four Sigma so what we are 95 so we are
on tify sparsity for so we we identify
that some of the networks are out then
so for example this one is a small
social network so you expect that it's
quite densely connected this one is a
network of political blogs in the US so
again you expect that there are a lot of
connections there's four others so you
obtain clears path cities for example
for the a neuron data set and here for
the world wide web data set so we've got
quite surprising results for example for
the for this the internet network where
we obtain a sigma between minus 0.2
minus zero point 17 so here clearly so
the internet network is believed to be
sparse oh there there there must be some
misspecified in our model that you that
and we cannot really capture this this
network okay so I've present in that so
a new class of statistical network model
that builds on exchangeable Honda
measure so that can capture so sparsity
power properties for which we derive
scalable inference
algorithms so at the moment the mother
is very in a sense is very simple it
only captures or sparsity because you
only have one parameters that Thun the
sociability of a node so what we what I
currently want to do is to thwack Stan
so within this framework to extend to
most structure models so using some sort
of no- factorization block model version
of this class of model introducing
cavalia dynamic networks etc so here is
the plot of a preliminary work on now
introducing you don't have a single
parameter legends of stability but you
would have a set of three parameters or
presenting maybe your interest in three
different categories etc and so you
could model some sort of a block
structure in the network so the end the
code is a available online if you want
to to try it thank you okay we're
running a bit late has anybody got any
any questions here I mean one thing that
I personally was sort of be interested
in it was the fact that you way that
you're setting this up do you have this
redundant of theta there is really quite
an interesting parameter in fact it's
got some sort of metric there you can
sort of start scaling up so if you've
got the units being you know if I get a
secretary to do an order for me most of
the time so so there will be a sort of
you could build lean as in into these
systems something which which which
actually had another process which was
associated with with clustering the
Poisson type process stuff that you've
got there which has a completely
different interpret
in terms of scaling up which might I
mean okay will be quite interesting okay
I didn't look at if it would be too much
yeah the frame that you've got ya 1 1
direction won't eat well is to introduce
covariate would be to to make now the
sociability dependent on the location so
the location would be a cetera yeah
covariance
now we'll look for at Dijkstra's
algorithm for finding shortest paths in
graphs with non-negative edge weights
Dijkstra is a famous figure in computer
science yeah and you can amuse yourself
by reading some of the things that he
wrote he was a very prolific writer and
kept notebooks and here really some of
his famous quotes now you need to maybe
know something about old languages to
appreciate some of these for example
COBOL was one of the first business
oriented languages programming languages
and it tried to make it so that programs
kind of read like English language
sentences but purists in computer
science such as Dijkstra really didn't
like that language as you can tell the
use of COBOL cripples of the mind it's
teaching should therefore be regarded as
a criminal offense actually when I first
came to Princeton was setting up the
department here one of there was a big
donor who actually I wanted to have me
fired and the Department closed down
because we wouldn't teach COBOL I didn't
know about Dijkstra's quota at that time
I wish I had and there's some other
opinions that Dijkstra had you can amuse
yourself on the web reading some of his
writings but he was here's another one
that's actually pretty relevant today
object-oriented programming is an
exceptionally bad idea which only could
have originated in California Dijkstra
worked in Texas University of Texas for
a while and of course came from the
Netherlands okay but Dijkstra did invent
a very very important algorithm that's
very widely used for solving the
shortest paths problem it's the one
that's in your Maps app and in your car
and many many other applications so
let's take a look at how that algorithm
works it's a very simple algorithm we
consider what we're going to do is
consider the vertices in increasing
order of their distance from the source
so in the way we're going to do that is
take the next vertex add it to the tree
and relax all the edges that point from
that vertex so let's talk about it in
terms of a demo and then we'll look at
the code so our mandate is to consider
vertices in increasing order of distance
from the source so our source is 0 in
this case
so what vertices are closest to the
source well we can look at the edges
that point from that vertex well start
out with vertex is 0 is that this and 0
from the source so we pick it then we
add that vertex to the shortest path
tree and relax all its edges and
relaxing all the edges pointing from 0
in this case is just going to update the
this to an edge two entries 4 5 8 &amp; 9 so
in this case this says that the shortest
path from 0 to 1 is to go along 0 1 and
its distance 5 from 0 to 4 its go along
0 4 and that's just since 9 and 0 to 7
let's go take a look 0 7 that's distance
8 and the edge weights are positive
we're not going to find a shorter path
to any one of what the sorry the edge
weights are positive and those are the
shortest paths that we know so far to
those vertices now the first key point
of the algorithm is take the closest one
if you take the closest one in this case
it's 1 then we know we're not going to
find a shorter shorter path to 1 the
only other way to go out of 0 is to take
one of these other edges and
they're longer so and all the edge
weights are positive so we're not going
to find a shorter way from 0 to 1 then
to take the edge there I want so that
means that edge 0 ones on the shortest
path tree that's what the algorithm says
take the next closest vertex to the
source and in this case it's 1 and then
we're going to add that edge that vertex
to the tree and relax all the edges that
point from that so now both 0 and 1 are
on the tree so now let's look at the
edges pointing to that and we have to
relax each one of those so in this case
if you go from 1 to 4 then we look at 4
we know a path of length 9 and 1/2 4
doesn't give us a shorter path so we
leave that edge alone
1 2 3 gives us a shorter path to 3 which
is going to be 20 because we went from 0
to 1 and then went to 3 is of length 20
and 1 to 2 also we didn't know what way
to tube and now we know now we know 1 or
better 1 is 17 so that completes the
relaxation of all the edges pointing
from 1 so now we have 0 and 1 on the
tree and we would consider next the next
closest vertex to the source so what we
have in this 2 is the shortest path to
all the vertexes vertices that we've
seen so far
and this one says that the and we've
been to 0 and 1 so that our next closest
one is 7 which is distance 8 so we're
going to choose a vertex 7 and again
that's the shortest path we've seen so
far we're not going to find a shorter
one because to get to every other vertex
is longer and so we know it's on the
shortest path tree and now we're going
to relax all the edges pointing from 7
in this case both of them the one from 7
to 2 gives us a shorter way to 2 then
what
new before and the one from 7:00 to 5:00
gives us a shorter way to 5:00 than we
knew before well we hadn't been to five
before so that relaxes seven and now
sevens on the shortest path tree so now
not it forced the next closest path
vertex to the source from the edge zero
four which is of length nine so that's
the one that we're going to pick next
for the tree we're not going to find a
shorter path there and we're lack we
relax along all its edges in this case
we find a shorter path to five than we
knew before and a shorter path to six
while we visit six for the first time
okay so that's four so now we just have
to worry about two three five and six
and five somewhere there so we select
vertex v relax along its edges in this
case those edges both give us better
paths than we knew before so now we're
left with three vertices and two is the
winner this is the two from the source
is 14 to 3 it's 20 into six is twenty
six we relax its edges and it gives us
again new shorter paths to both three
and six and then the last is next to
last step is to pick three and that does
not give us a new way to 6 and then
finally we pick 6 and then we now know
the shortest paths to all the vertices
from vertex zero if we just take the
edge two edges that's from two one you
take zero to five to take two and so
forth you get the shortest path tree and
that gives the shortest way to get from
the source to every vertex that's a
simulation of Dijkstra's algorithm
here's a demo of Dijkstra's algorithm
running in the large digraph so the
source is the vertex in the center with
the open circle and you can see it
considers the vertices in the graph in
order of their distance from the source
now you can see this is maybe like a
lake in the middle of the graph and so
it's not going to move on from those
vertices it's going to take a little
while to get around the lake and again
if this visualization is produced by our
code and it gives a very natural feeling
for the way that the algorithm works
this in principle and I think in fact in
many cases is what your car does when
you turn the map system on computes the
shortest path to everywhere and then
it's already when you ask for a certain
place know how to get there
so here's just starting from another
point in the graph you have if you
happen to live at a corner of the earth
and it's going to be a slightly longer
to get to the other corner and a nice
feeling for how the algorithm gets its
job done again when it gets into those
blank areas it's it takes a little while
to get over to the other side and of
course if we had Islands if we had
little roads in the middle there that
were not connected there'd be no way to
get to them from the source and we
wouldn't see them and that's fine not
for the way our algorithm works we just
leave that out of the demo and the
proofs to avoid adding an extra comment
about that for every algorithm
that's a visualization of Dijkstra's
algorithm on a large graph so so how do
we prove it's correct well essentially
we prove that it's an instance of the
generic algorithm so first thing is that
every edge is relaxed exactly once we
every time we put a vertex onto the tree
we relax all the edges that come from
that vertex and we never reconsider the
vertex and what does relaxation do
relaxation ensures that after the
relaxation either way that was before
afterwards you have the distance the W
is less than or equal to the distance to
V plus e to the plus the weight of the
edge either it's equal because we just
made it that way or it's less because it
was before and the edge is not relevant
and this inequality is going to hold for
every entry and for every edge
corresponding to every edge for this two
entries corresponding to every edge
because number one the this two values
are always increasing
we never I'm sorry are always decreasing
the only thing we ever change only
reason we ever change just two w is to
make it smaller if if we found an edge
it would make it bigger we ignore that
edge so it's always decreasing so when
we change this to W we're not going to
make that inequality false now we're
just going to make it better and this 2v
is not going to change at all once we
relax an edge from a vertex we're done
with that vertex we're not going to
process it at all so then when we're
done
we have the optimality conditions hold
that exactly is the optimality condition
and not only that we have a path from
the source to every vertex so that's a
correctness proof for Dijkstra's
algorithm
based on the optimality conditions
here's the code it's similar to code
that we've seen before
we're gonna use the indexed priority
queue that allows us to implement the
decrease key operation and we have our
edge two in just two arrays that are
part of the shortest paths computation
and the goal of the shortest path
computation so we initialized the
constructor initializes those arrays
including the index minimum PQ and we
start out with all the distances
infinity except for the distance to the
source is zero we put this source on the
priority queue and then what we're going
to do is take the edge that's closest to
the source off the priority and that's
on the priority queue off and then relax
all the edges that are adjacent to that
so using our standard iterator to get
all the edges that emanate from that
vertex and relax them and then the
relaxed code is just like the code that
we showed when describing relaxation
except that it also updates the priority
queue if the vertex that that edge goes
to is on the priority queue it gives a
new shorter way to get to that so we
have to decrease the key on the priority
queue if it's not a priority queue we
insert it and that's it that's a
complete implementation of Dijkstra's
algorithm using modern data structures
now this algorithm might seem very
familiar if and if you've been paying
attention it's essentially the same
algorithm as friends algorithm the
difference is that in both cases we're
building what's called a spanning tree
of the graph but in prims algorithm we
take a vertex that that's not on the
tree using the rule of let's take the
vertex that's closest to the tree
anywhere on the
three closest to some vertex on the tree
for Dijkstra's algorithm we take next
the vertex that's closest to the source
through a path that goes through the
tree and then into a non-tree vertex
that's the difference and that the
differences in the code have to do with
the fact that prims algorithm is for an
undirected graph Dijkstra's algorithm
for a directed graph but essentially
they're the same algorithm and actually
several of the algorithms that we've
talked about are in this same family
they compute a spanning tree I have you
you have a tree that takes care of where
that records where you've been in the
graph from every vertex back to where
you started and they use different rules
for choosing which vertex to add next
for a breadth-first search you use a cue
for depth-first search you use something
like a stack and then you just have to
decide what to do if you encounter
vertex you've been to before
but many graph algorithms use this same
basic idea so in particular when we're
talking about what the running time of
Dijkstra's algorithm it depends on what
priority queue the implementation we use
and it's the same considerations as for
prims algorithm we have v insert
operations every vertex goes on to the
priority queue V delete min every vertex
comes off the priority queue and for
every edge in the worst case we could
compute a decrease key operation
so the original implementations of
Dijkstra's algorithm used an unordered
array which would mean that it would
take time proportional to V to find the
minimum to find the vertex closest to
the source so the total running time be
proportional to V squared that's not
adequate for the huge sparse graphs that
we see in practice today like the map in
your car so the binary heap data
structure
makes the album makes it feasible to run
this algorithm and that's where all the
operations take time proportional to log
V we have to use the indexing trick that
we talked about last time to support
decrease key but still we get a total
running time of elog V which makes it
feasible and just as with prims
algorithm by using a implementation of
the priority queue that can do a faster
decrease key you can get a faster
algorithm and in practice something like
a 4-way heap is going to give quite a
fast algorithm that's a more expensive
to in certain to insert but much faster
to a delete min and decrease key and
again there's a theoretical data
structure that's not useful in practice
that gets the running time down to e
plus V log V of course if your graph is
dense and again the examples I use
they're not the array implementation is
optimal you can't do any better you have
to go through all the edges you might as
well find the minimum at the same time
but in in practice people use binary
heaps for sparse graphs I may be going
to four-way if the performance is really
critical bottom line is we have
extremely efficient implementations for
the huge graphs that arise in practice
nowadays
hello friends my name is Tushar and
today we're going to talk about how to
find all simple cycles in a directed
graph using Johnson's algorithm so what
is the cycle a cycle is a path in a
graph such that the first and the last
word existed what is the simple cycle a
simple cycle is a cycle in which no
vertex is repeated twice except the
first and last vertex so in this graph
here we have bunch of simple cycles
example h98 or 1 2 3 1 so the idea is to
find all such simple cycles using
Johnson's algorithm so there are five or
six other algorithms to find simple
cycle but I found out that Johnson's
algorithm is the fastest of them also
for Johnson's algorithm I expect my
beavers to know how strongly connected
components work in another video I
already talked about how to find
strongly connected confident using cosas
Rajas algorithm and I am also going to
create another video about how to find
strongly connected component using
Tartans algorithm but just for revising
it quickly so a strongly connected
component is a subset of vertex in a
graph such that all the vertices are
reachable from each other so in this
graph here we have three strongly
connected component one is H 9 because 8
&amp; 9 are both reachable from each other
another one is 7 and the third one is
consisting of the remaining vertices
because all these vertices are reachable
from each other also for the purposes of
Johnson's algorithm I have numbered
these vertices going from 1 up up all
the way to 9 it is not strictly required
but this is how the algorithm is
described in the paper so this is how I
am going to explain it in this video so
next let's see how a Johnson's algorithm
works first thing we do is we divide
this original graph into sub graphs
where each sub graph consists of
vertices and edges which are for a
strongly connected component since we
had three strongly connected component
there we have three sub graphs here so
why we do that because a cycle is always
going to be restricted with a strongly
connected component there is never going
to be
cycle it stands for multiple strongly
connected component so for example this
edge here is never going to be part of
any second why because if it was it
means that either from 8 or 9 there is a
way back to 1 and which which would have
matched that 8 and 9 would be the parts
a strongly connected component as the
rest of this vertex and since they are
two different strongly connected
components which tells us that there is
no way back to one which means that
which means that edges like this this
and this would never be part of any
cycle so once you have done divided into
three sub graphs then we look for the
least number H least number what X so
the least number what X here is one so
first we are just going to work on this
sub graph this is this strongly
connected components of graph also I
have stack blocks set and block map map
data structure and as we run through the
algorithm I'll show you how this data
structures are you so what started what
X will be 1 which was the least numbered
vertex in this strongly connected
component so basically we are looking
for all the cycles which start and end
at 1 so then we do a DFS starting at 1
so we add 1 to the stack and 1 to the
block set and now you're going to
explore neighbors of 1 so 1 neighbor of
1 is true who is not say the start
vertex which means that we have not
found a cycle 2 is not part of the block
set which means that we can explore 2 so
we put 2 into the stack and 2 into the
block set then be able to explode
neighbors of 2 so 1 neighbor of 2 is 3
so again 3 is not same as start vertex
which means that we have not yet form a
cycle and 3 is also not part of block
set so we can explore 3 so we put 3 into
the stack and 3 into the block set then
they explode any course of 3 1 neighbor
of 3 is 1 and that table is same as
start vertex which means that we form 1
cycle and the content
the cycle is the contents of this tab so
let's record this cycle so the cycle
will be 1 2 3 1 then we come back to
three and then we explore other
neighbors of C 3 and see if we can find
more cycle so one neighbor of 3 is 2 but
2 is already part of this blocked set
and since we are just looking for a
simple cycle anything other than 1
cannot be repeated so we cannot continue
in this path of 2 so we come back to 3
and then explore another neighbor of 3
so one more neighbor of 3 is 4 so 4 is
not going to start vertex 4 is not part
of block set so we are going to add 4 in
woods tag and block set and explore
neighbors are for
one neighbor of 4 is 5 so 5 is not same
I start vertex and 5 is not in block set
so we will add 5 into stat and block set
and then explode anywhere is a 5-1
neighbor 5 is 2 not 2 is not same a star
vertex but 2 is in this block set which
means that we again hit a road black
road block and we cannot proceed in that
direction because to gain on what X 2
cannot be repeated again in the same
cycle so we're going to come back to 5
and explore other neighbors of 5 but 5
have just 1 over 2 so what that means is
all the way are going to recur start
from 5 I am NOT going to remove fry from
the block set why because the way things
says stack is set up correctly there is
no way in the current DFS traversal that
you could find a cycle which starts
which which has 2 in it and also which
goes to 5 because 5 is going to h2 but I
am going to record that if we ever
unblocked to then I am going to unlock 5
so basically if we look for
neighbors of five and add five to their
unlock to the block map so if two ever
gets unblocked at that point of time
island block five so as you see I left
five in the block set then from five we
come back to four four has no other
neighbors and we did not find a cycle in
this current path so similarly just like
before we leave for in the block set
will remove it from the stack and once
you find the neighbors of 4 which is 5
and add 4 to the blocked map so just
saying like if five ever got unblocked
then be able to also unlock 4 and then 4
we come back to three now three has one
more neighbor 6 so be able to traverse
there 6 is not same a start vertex and
sticks is not same as block side so we
add 6 to both of them now 6 has one
neighbor for now 4 is not saying a start
Ward expert 4 is times this block set
which means that we should not explore 4
and this is how we do optimization since
we did not remove 4 from this block set
which means that we save this acceptable
cell where we go from 4 to 5 and 4 5 to
2 and then realize the 2 is already in
the current stack and then come back so
by not removing 4 and 5 from the block
set we provided this optimization so
anyways but we have to note that if
forever gets unblocked then we should
also have lock sets so 6 has nobody else
will we go back to 3 so we remove 6 from
the stack but not from the block set now
equal to 3 so 3 is done exploring all
its neighbors and it did find a cycle in
one of its neighbors so 3 be able to
block 3 because there is a possibility
that there could be a future cycle going
from 3 to 1 since 3 found attached to 1
it means that there could be some other
path which could lead from 3 to 1 so we
have to
three so I had removed three from the
stack and I'm also going to unblock
three and then I'm going to check this
dog Mac to see if by blocking three do I
have to block any other vertices and
none because there is three is not part
of any key so then from three we go back
to two because that's where it came from
now two has no more neighbors so now
since we did find a cycle from 2 all the
way to 1 which means that we have to
unlock two to open up the possibility
for future cycle so we will go to unlock
two and since now that we've unlocked -
we have to go and check is anyone else
waiting to be unlocked because of two so
five was so we go to 5 and see if anyone
else was waiting to be unblocked because
of 5 and 4 was to recursively go to 4
and if it was anyone else ready to be a
block because of 4 6 was and no one was
waiting for 6 so first we remove 6 from
here unlock 6 and remove this particular
entry then you go to 4 remove flow from
this unblocked block set and then delete
this particular entry and then we go to
2 and 2 is already removed from the
block set but we also got to remove 5
and then remove this particular entry so
as you can see by the time we were done
with 2 and then also going to move two
from the stand so by the time we were
done by 2 now we have to unblock 4 5 6
all this other vertices why because now
that 2 is no one in this tab there is a
possibility of a cycle going through 6 5
and 4 and leading to 1 so now we come
back to 1 and 1 has another vertex 5 so
we are going to add and find it's not
part of this block set and why because
since to garden not removed 5 which
means that we open up a possibility for
another cycle going through 5 and now
that 5 is not part of block set we can
explore
five so we add fire to the stack five to
the block set and then you have an
exploding where's a five so neighbor of
five is two and now we'll repeat the
process like before so neighborhood five
is two and two is not Paris attack and
true is not same and start word X we add
to here to here and then here of 2 is 3
so 3 + 3 + 1 aber of 3 is 1 it means
that we found a cycle and and the
contents of the cycle will be this 1 5 -
3 1 so we found cycle here so now we
explore other neighbors of 3 1 neighbor
of 3 is 4 &amp; 4 is not exploding 4 is not
saying start node X so we add 4 here
another neighbor of one neighbor of 4 is
5 but 5 is already colored blocks set
which means that we cannot exploring the
direction of 5 because 5 is already part
of this time and it's a simple cycle we
are looking for so we go back to 4 leave
for in the block set but mark that if 5
ever gets unlocked then we will unlock 4
then 4 for big and then we go for from
this tack but not from the block set
from four we go to 3 3 has another
neighbor 6 we add to the stack and to
the block set 6 has another neighbor for
but 4 is already the block set so we did
not find a cycle from 6 all the way to 1
so but we are going to mark that if
forever gets unlocked then we will
unblock 6 so leave it says in the block
set and remove 6 from this tag and go
back to three now three has explore 3
has another neighbor 2 but 2 is already
caught a block set so we come back to 3
so now 3 has explored all its neighbors
and it did find a cycle going so it did
find a cycle leading to 1 so we have to
unlock 3 if you never find a cycle so
ever find a path leading to the start
vertex yes that block that vertex to
open at the possibility for future work
future cycles so we remove 3 and you can
got 3 and then we check doesn't doesn't
one else needed blocking because of
three no one else does so we go to two
now since we found a cycle in this path
from two so we able to unblock to remove
two from the stack and see does anyone
else need Sun blocking and no one does
so from two we go back to five and we
unblocked five and remove fry from the
stack and also see if anyone else needs
unlocking because of five yes we do
because for was dependent at fight then
locked which means that we will remove
four from here and then we will see who
is dependent in four and six was so we
will remove six from here and we'll also
remove these entries so finally we come
back to one and now one has no more
neighbors to be explored so basically
after all this we found all the cycles
which starts and ends at one and so
there are two cycles with start and ends
with one so after we are done dealing
with one the next thing we do is we
remove one from my original graph as if
one never exists existed before so we
remove one we move all the outgoing and
incoming edges from one and just delete
one and acts like one ever existed and
then repeat the entire process again so
let's do that quickly now that one is
not part of the graph we still have
three strongly connected component we
again look for the list numbered vertex
which is two so where new start vertex
will be two and will repeat the entire
process again so let's do that very
quickly
so two into the stack two into the block
set neighbors of two will be explored so
definitely three three will be go here
will be going here and three will go to
the block set neighbors are three will
be explode one neighbor is 2 so it is
say the start vertex so we found one
cycle which will be two three and two
then you go back to three and then
explore another neighbor of three which
will be four so we add flow to the stack
and four to the block set and explore
equal to 4 which will be 5 so we add
five stack and
and then fry the end up at two so again
you find another cycle and the contents
that cycle will be two three four five
two and then be records back from five
since you found the cycle from five
there's a possibility of future cycles
if we remove from the block set and
stack and then you go back to four since
we found a sec on this path we remove
four from both blocks set and block and
stack and then we go back to three three
has another three has another vertex
another neighbor 6 so we explored that
so we add 6 to here and 6 to here and
from 6 we go to 4 and 4 is not a block
set and now you can see why the moving
forward was so critical because if you
had not removed four from the block set
we would have never found this this
cycle going wire 6 so we remove the S 4
is not X is not a block set to be high
it we add it then for from for a good 5
it's not there so we added and then 5s
neighbor is 2 so we found one more cycle
so the content of that cycle will be 2 3
6 4 5 &amp; 2 &amp; then makers back so we found
a cycle so we unblocked 5 for the
possibility of future cycle then we go
to 4 then block 4 for the possibility of
future cycle then we go to 6 we unlock 6
for the possibility of future cycle then
we go to 3 so at this point of time we
have explored all the neighbors of 3 so
we're and since we found cycles be able
to unblock we haven't unlocks 3 and we
will remove from the stack and then we
end up at 2 and now to has no more
neighbors so again we are done finding
all the cycles which starts with 2 which
is 2 3 2 and this one and this one so
now again we go back to this
you know original graph act like to
doesn't exist and then repeat the entire
process again let's do that very quickly
with two gone these are the remaining a
strongly connected components here so
there is no reason to pick a sub graph
with just one vertex because there is
the possibility of the cycle because
we're assuming there are no self loops
in this graph so then next we pick the
least number what X which has at least
two vertices in the sub graph and there
will be eight so with eight we'll repeat
the same process so it goes into the
stack and blockset and explode neighbors
of it which is 9 so 9 goes here and here
and then explode any person mine which
is one of them is 8 and it is also
starting vertex so we found another
cycle and the second will be 8 9 8 and
then and then we removed them from the
stack as where occurs back and then we
are done with it
so once you're done with it will will
act like it doesn't exist in this graph
so then we will just be left with then
we'll just be left with a strongly
connected components or with the sub
graphs whose size is just one at this
point of time we are done with the
algorithm so in this graph here we found
six different simple cycles let's
analyze the time complexity so the time
complexity is o of e plus v into c plus
1 where c is the total number of cycles
in the tour number of cycles in the
graph so we can find strongly connected
component in a plus we time which is why
we can guarantee that between every
cycle the worst time which will pass
will be e plus we but remember we can
have exponential number of cycles which
means that the total time complexity can
still be exponential so but the only
guide is that
recycle the worst time which will pass
will be II trust V and the space
complexity also will be e plus we so
next let's look at the code for this
let's first look at all the data
structures used in this algorithm one of
them is block set which is a set of
vertices which when blocked will not be
explored another one is blocked map
which is a map of vertex to the set of
vertices so when this vertex in the key
is unlocked then all the vertices in the
values should also be unblocked and then
stack holds the current DFS tag and then
all cycles is a list of list of vertices
so in this inner in this inner list is
is going to is representing one cycle
and then the outer list is the list of
all the cycles so basically all cycle is
going to store my final result the main
function is simple cycles it takes in a
directed graph and then first we are
going to do is initialize all the data
structures and then our start index will
be one and you remember or the index
indices or the vertices are valued
numbered from 1 all the way till Hauser
many vertex we have then we are going to
use stargell algorithm to find strongly
connected component
so while start index is less than the
total number of vertices we have which
initially will be true first we do is we
create a sub graph from the original
graph such that it only consists of
vertices which are numbered greater than
or equal to start index so anything less
than start index will be ignored so
initially start index is 1 so we will
include the entire graph then in this
sub graph we are going to find all the
strongly connected components using
target's algorithm then we are going to
pass this SCC s and the sub graph to
least index SCC so basically finding for
the least index vertex among all the
strongly connected components so this
method here first creates a graph for
each of the strongly connected
components and then among them looks for
the least index vertex also it ignores
any graph which has just one vertex in
it because it will never have any cycle
so this might or might not return a
this might or might not be present so if
the least index vertex is not present
then we break and then we are done with
algorithm and return all cycles
otherwise if it is present then we get
the least index vertex clear the block
set and block map because they may be
populated from the previous iterations
and then find all the cycles in this
particular in this particular strongly
connected component graph which list
index least vertex is part of so this is
the start vertex and then this is the
current vertex and then finally we look
at this method in a bit and then finally
we increase our start index to be at
least least index plus one so when we go
back here and then when we create a sub
graph next time we are going to ignore
everything which is less than start
index and this is also we discussed
previously in the video so let's look at
find cycles in s e-g which is the main
part of the algorithm so here the first
is the start vertex and second is the
current vertex so first we do is we do a
DFS here phone cycle is false we push we
push the current vertex into the stack
and then we also push the current vertex
into blocked set then we are going to
explore all the neighbors of current
vertex so we get one neighbor at a time
if this neighbor is same as start vertex
it means that we found the cycle so we
are going to push the contents of the
stack into this list cycle and add this
list cycle into all cycles because all
cycles is storing our final result and
then we are going to mark the found
cycle as true because we found a cycle
in this particular path by going wire
this current vertex
otherwise if neighbor is not same as
start vertex and if also if neighbor is
not in the blocked set then we can
explore this neighbor so we again go
into the recursion with fine cycle in se
G keeping the start vertex s same and
for the current vertex passing neighbor
and then we and then we what and then it
repeats the same process and then if we
did find a cycle in this
then we then we sent the found cycle to
be true so basically if for a current
works a current vertex if any of its
neighbors finds a cycle in its path then
found cycle will be true for the current
vertex so once we are done with the for
loop and all the recursion
if found cycle is true then we unblock
the current vertex and this unblocking
is not just an drawing the current
vertex but it is also checking the
blocked map and recursively unblocking
all the other vertices which are waiting
on the current vertex so let's look at
unblock very quickly so we remove vertex
u from the block set and then records
and then getting all the vertices from
the block map which are waiting on you
and then going through one each of them
and removing and recursively and
blocking them as well so as we discussed
in the video this is recursively
unlocking all the vertices so once we
are done and blocking if then we are
done unlocking and if the found cycle is
not found so basically if there is no
cycle in this path then we are not going
to end block this vertex this current
vertex but we are going to add we are
going to find all the neighbors of the
current vertex and add this current
vertex to the blocked map so this is
because if they ever got unblocked
then we are going to unblock the current
vertex as well and this is also we
discussed in the video before and after
this we are just going to remove the we
are just going to remove the current
vertex from the stack and then if cycle
was sawn divisionary will turn to and if
cycle was not found then widget Falls
and this happens recursively
so finally fine cycle comes here and
then we come back to here and then the
increment over start vertex and all and
then all cycles will have the final
result let's quickly run this code so
this is the same graph as we discussed
previously in the video so let's look at
the result of this graph so here this is
the list of all the cycles found in this
graph so I can totally endure
and this video is little in this code is
little involved so I would recommend is
you copy this code and run it and see
how the entire code is working so
finally the time complexity for this
algorithm is o of e plus we into c plus
1 where c is the total number of cycles
the reason we have a strong bound here
is 2 because 1 we can find strongly
connected component in o e e+ we time
and second because we are doing all the
blocking of the vertex and unblocking it
only when required
we are saving on the useless fruitless
and needless searches and also the space
complexity is o of e plus v so this is
all i have to talk about johnson's
algorithm please like this video share
this video comment on this video check
out my facebook page facebook.com to
sharra 25 and check out my github link 8
sub-commission peace interview wiki
thanks again for watching this video
we have a wonderful program coming up
I'm David Eisenbach I'm very pleased to
inch to welcome you to this mathematical
horizons 12 wonderful speakers and I'm
looking forward to their talk first is
Sarov Chatterjee he was a student of
persi diaconis at Stanford then i'm
happy to say he came to Berkeley
unfortunately he left Berkeley again
after a while and went to New York and
then came back to Stanford but at least
he's our near neighbor and a frequent
collaborator and visitor here he won the
rollo davidson prize in 2010 and most
recently lavey prize in probability in
2013 and i'm looking forward to his talk
on nonlinear large deviations Seraph
thank you for the gracious introduction
and thank the organizers for the
invitation so so this is a something
that I have been working on for the last
few years so I was asked to prepare the
talk at the level of incoming graduate
students so that's what I did I don't
see that tiny Gavin graduate students
here up so I'll do all the senior people
you know please don't feel insulted if
it's too easy so I'll try to okay so so
let me start by saying a few things
about the area of large deviations you
know it's a technical part of
probability theory and there are two
main goals of large deviations theory
one is to study the probabilities of
rare events so events that are very rare
but probably is a very tiny but you want
to want to understand how I need is and
you want to understand the conditional
distributions given that some really
vendors have as occurred so so the thing
is the world doesn't look the same if
something unlikely has happened so
things change so you want to understand
how things change you've given the Sun
likely happen so you know a silly
example here so like what is the chance
it will win the lottery and the second
thing is how is a life kind of change if
you win the lottery so that's the
conditional distribution part so often
the second question is more interesting
than first because the first is just a
number it's some small number is a
number the second question is more
complicated you know you don't know how
things are going to be affected if
something very unlikely happens but it's
usually essential to answer the first
question before you want to you know you
you can understand the second so you
want to you have to know the probability
of the event before understanding and
then there are technical reasons for
that why that is nice actually so a
simple example in all the the lottery
example is some real life thing which is
too complicated to capture by
mathematical models but a simple example
starts a fair coin 10 times where
innocent large number and under normal
circumstances this is what you'd expect
it you'd expect to get approximately n
over two heads okay since it's a fair
coin and you also want to know the
number of times should get two
consecutive heads so head and head you
know so so roughly n over four pairs of
these you'll get consecutive heads okay
so that's some simple prob lista
computation that you can do so suppose
the following rare event occur so that
instead of n over two heads you get to
and over three okay so if you're tossing
the coin a thousand times instead of
getting 500 head so you get 666 or more
heads and there are some general purpose
tools I'm going to briefly describe how
these computations are dunce of this
event you can write down the probability
is approximately asymptotically it's
like e to the minus n times some number
okay and moreover you can make
conclusions like the following that if
this happens then this this thing the
number of conservative hads pairs of
consecutive helps that you get instead
of being n over
or it will likely be four and over nine
so it'll increase so you can say this
kind of conditional thing that given
this rare event has happened you you get
you see other things also changing other
things that were very likely before
become unlikely now and things that were
unlikely before become likely now so so
this is a very simple thing you can do
it in many ways if you know if you're
familiar with Sterling's formula and all
that you can it there are 100 different
ways you can do this but you know as in
math there is only one way we
generalizes ok so there are many ways to
do this but there is only one way which
becomes abstract and generalizes to a
more complicated settings so the next
slide will be a little bit of math and
then I'll you know move move back to
this mode where I'm not showing so so
just this one slide of math I have so so
this is how you do this so how is this
estimate obtained this this Pro
ballistic estimate that i wrote down so
this is how you do it's a little bit of
undergraduate exercises so suppose x1 to
xn are independent random variables each
is 0 or 1 it prob'ly a half then a
number of heads in entasis is same as
the sum of these random variables and
then you do this little competition here
so you want to find the chance that you
get at least two and over three heads so
you take some theta you write this event
as saying e to the theta s n is bigger
than it would to theta n over 3 so these
two are the same things now this is the
general purpose markov inequality that
you have in probability that probability
x bigger than t is less than expected
value of x over T so you just write this
down now the nice thing about this one
is that it's a product of independent
random variables so it breaks up as a
product of expectations and each of them
is easy to compute this these are just
soooo one variable so you can write this
down so use markov inequality use
independence and then you optimize this
bound over theta and you get some upper
bound on this probability ok so this is
less than or equal to now the
interesting thing is that this upper
bound is actually the correct asymptotic
and that that requires a different idea
now both this technique and this other
idea which i am not showing here they
generalize to an abstract setting
this is sort of the beginning idea in
the theory of large deviations this is
how you compute large deviation
probabilities in most cases so so you
see there are several things here
they're independent random objects their
summing up to give something and then
you apply this Markov inequality and the
main the key point is that this
expectation breaks up as a product and
then you optimize and you get a bound
and it's unclear why this bound should
give you the right answer but it does
and so this example has this built-in
linearity which allows you to compute
this expectation and this idea
generalizes this is there is a fast
generalization of this this simple idea
and the corresponding idea for the lower
bound and this is the main technique
behind the theory of a classical theory
of large deviation so so you go from the
zero and random variables to more
complicated objects you go to function
spaces and you generalize and you you
get you know amazing things so your
literature spanning 50 years so but the
thing is there are no general tools for
non linear functionals okay so there's
this linearity here okay but there are
no tools for nonlinear functions I'll
give you an example in the next slide
okay what i mean by nonlinear and so you
see you understand how linearity helps
in in the previous slide now you know in
this expectation breaking up as a
product because you get a sum of
independent things or you know any
linear combination would you can do the
same thing but it nonlinear you cannot
do the same trick okay so so here is a
real life example sort of so analysis of
real world network so this is becoming
more important these days and you know
lots and lots of papers now really the
thing is rarely vents on networks large
deviations or networks are often
nonlinear in nature okay so consider the
following simple model there are n
individuals and any to our friends with
a certain probability P and not friends
with probability 1 minus P and
friendships are all independent okay so
this is known as error shiny model and
this is not realistic mainly because
this assumption that friendships are
independent which is not true in reality
but there is a first step to
understanding real networks okay so so
in graph theoretic terminology and
individual is called a vertex and a
friendship is called sorry it's called
an edge so so you this is a model of a
graph and you have you have these edges
in the graph and you consider a simple
object which is the number of triangles
a number of you know Triplets of people
who are all friends with each other and
there's an easy computation you can
compute that the expected value of the
number of triangles in this graph you
can exactly write it down so this isn't
you know n choose 3 is a total number of
triangles and each of them is actually a
triangle in that graphic probability P
cubed and so on so that's so now the
large efficient question is what is the
probability that there are K triangles
where K is some number much bigger than
this expected value so what is the
chance that there are many more
triangles than what you would expect ok
so that's a that's a basic question that
you can ask about the graph and what
does a graph look like if such a rare
event happens so if if you observe many
more triangles you know you observe you
know three people being all friends with
each other many more times than you
would expect just by chance now what
does a graph look like so this is an
example of a nonlinear problem because
the number of triangles is a nonlinear
function of the adjacency matrix they're
using the matrix of the graph is you
know make the matrix with which which
has zeros are ones depending on whether
there is an edge between I&amp;J or not and
this is a Phi entries are all
independent and this is a nonlinear
function of Zen tree so this is a very
simple basic nonlinear question that one
of the simplest you can think of however
the strange thing is that you know this
was open so this was completely or
people didn't know how to do this this
large so so this this thing about a
number of heads in n coin tosses this
was a basic linear question and you know
it is very old peep
knew how to do this for a very long time
but this this question this you go one
step higher you just take a random graph
it all independent edges and you look at
the number of triangles and what is the
chance that you know there are many more
triangles and you expect or what is a
what does the graph look like if so by
many more I mean maybe two times or you
know 1.5 times more triangle so if it
was just a little bit more people
understood that you know very small
increasingly more people understood but
if it was a fraction more you know there
was no understanding of firm you know
how to approach how to even approach
this question so this was done so I you
know together with raghav are done at
NYU where we did this in 2011 and so the
so this theory brought together things
from various areas so it's a bunch of
large deviation techniques and they were
you know semi his regular dilemma and
then there was the graph limit theory
developed by loafers and quarter so
various things came together which was
which was pleasing so so here is an
example the kind of results so i'll give
you in this slide i'll give you a very
counterintuitive sort of result that you
get from this this investigation so we
call this model you have n individuals
any to our friends with probit be
independently and TB the number of
triangles let ET be the expected number
of triangles and so you want to know
what is the most likely structure of the
graph if this rare event happens that
the number of triangles is a fraction
more than the expected value so so there
are various possibilities one is that
all the extra triangles are contain a
small subset of vertices which i have
high connectivity among themselves so
the graph can look like that there is a
small bunch of vertices which are highly
connected they sort of click of people
so that they give all the triangles or
it can happen because there is a nexus
number of edges spread independence just
that there are many more friendships
than you would expect so just uniformly
distributed so what you know what is it
you know what is what is the most likely
scenario
so surprisingly the theory implies that
both scenarios can happen okay so that
is that was a surprise to us so more
precisely it's the following so I'll
cite in words in the next slide but just
let me cite in math in this slide there
exists a delta 1 and Delta two two
numbers so that if if this Delta here is
between Delta Delta 1 and 0 are bigger
than Delta 2 then conditional on his
event the graph just behaves like an
addition any graph so it just behaves as
if there were an external extra number
of friendships or edges spread out
uniformly all over okay on the other
hand if Delta is between delta 1 delta 2
then the conditional structure is
different so then there is a clustering
okay so that you know so these and I
should say that then you know that there
was this very nice work of Lubezki and
Ja afterwards where they've found this
formulas for Delta 1 and Delta too you
know so there's this symmetric and
broken symmetry regime and they found
the exact boundaries between so they had
a phase diagram in the language of
statistical mechanics okay so so in
words what this means is the following
that if the number of triangles exceeds
the expected value by a little bit or by
a lot then the most likely scenario is
that the excess number of edges spread
uniformly okay so you have this graph
you observe a nexus number of triangles
if the excess number is a small fraction
more or is a large fraction more then
you would expect the error shiny
behavior then you would expect that the
all the edges are spread uniformly
whereas if the if it belongs to middle
range then there is a clustering of
edges then you can say that with high
probability there is a clustering of
edges and the exact nature of this is
still not fully understood so in the
sense of this graph limit theory which I
you know don't have time to go into now
but it comes as a solution of
variational problem and we cannot solve
the problem but you can show that there
is a clustering okay and the thing is so
this is some of this demonstrates the
value of mathematics because these
results cannot be guests and I still
don't know what's the intuition that you
get this sort of uniform behavior
if you have a little more of a lot more
but somewhere in between you get a
clustering so you know I you know we
don't have any understanding of why this
happens it just comes out of of the
calculations that earring on you know
now we have tools by which to calculate
things and these calculations can yield
these results which you know you you
cannot you know justify by other means
any questions now yes it is a phase
transition it is a phase transition so
so you know I I wasn't you know wanting
to go into that so it's in a statistical
mechanics language there is an order
parameter which is what is called a
graph on in the graph limit theory and
this order parameter so there is a
variation problem whose solution can
either be a constant function which is a
symmetric regime or it can be a non
constant function so is a broken
symmetry region of broken symmetry so
and and there can be more than you know
so we identify these two phase
transitions that happen there can be
more happening in between and we still
don't know whether whether it happens
yes there can be you know we don't know
it doesn't it doesn't yeah yeah yes
I know they depend on they depend on P
so they don't depend on n because it's
not synthetic result so so this result p
is fixed but n is going to infinity okay
so so they depend on p delta 1 and delta
2 so this was a very you know clever
piece of work where they actually found
this in a wing we tried very hard arago
and i tried very hard to get this but we
we couldn't pictures what pictures of
graphs representing the cluster know so
the thing is there is a days of
difficulty doing that because you have
to okay so this is another value of
large deviations you have to simulate
conditional or rare event now the event
is rare right so so you know the chances
are extremely small so you may have to
wait for many years before you get
something by these simulations but the
thing is rare events do happen you know
hundreds of millions of people by the
lottery and somebody wins so okay so so
rare events do happen okay so then there
are some other applications I there are
these exponential random graph models
which are you know widely used in the
Earth's of real social networks and you
know there are we can say things about
those also now there is an
incompleteness of this theory so this is
large equation 3 4 random graphs it's I
know it was fairly well developed in in
the pin in our paper and subsequent
papers what has one very serious
shortcoming which is that it applies
only to dense graphs so what does it
mean so a graph is called dense if most
vertices are connected to a sizable
fraction of other vertices okay so for
instance in the editor shiny graph where
n is 10,000 and peas point 3 so there
are 10,000 people and any two are
connected to chance thirty percent then
each person has approximately three
thousand other friends okay which is an
unrealistic scenario so so if you have
10,000 people in a real social network
you know even if the editor any model is
true p would be like point zero one you
know 100 people 100 friends so the real
networks are usually sparse so this
dense case we did just as a first step
to understanding the thing now
the problem is the main issue so there
is a deep in a graph graph theoretic
issue here which is that the graph
theoretic tools that you used for this
analysis samara's lemma and graph limit
theory of lavas and co-authors they are
useful only for the dense case okay and
and this for instance you know
developing a satisfactory version of
Somalia's regularity lemma so this is
you know this is an open problem for 40
years so and i'll tell you why that is
so so i'll tell you why what's what's
the underlying reason why this is this
is challenging so so the main issue is
the following that the problem is to
anywhere the number of graph of the
given set of properties for instance you
have to understand how many grafts are
there with em edges and k vertices okay
this kind of this is the kind of thing
that you have to understand now for
dense graphs this approximate counting
is possible with semi radius lemma so
how is this so i won't tell you know i
don't have time to tell you what this
this thing this lemma is but what it
does is the following it classifies the
set of all dense graphs and n vertices
in a bounded number of types and the
number of times depends on the desired
accuracy of approximation so you want to
approximately count something and and
you want you know approximation up to
error epsilon so depending on epsilon
you can classify the graphs into into
into type so it's sort of a compactness
thing you can you can and the number of
types will just depend on epsilon and
within each type you can count using
classical large deviation techniques so
that's that's the combination of large
deviations and summer a dilemma that
comes in and the graph limit theory will
allow you to take a limit to infinity so
the embed the whole thing into into an
infinite object so the regularity lemma
is inapplicable in the in the sparse
setting we do not know how to classify
sparse graphs in two types so I'll tell
you a little bit more on how this means
so what this means so for instance an
excess number of triangles in a sparse
graph can occur because there are extra
edges that are distributed uniformly or
a small number of vertices that are
highly interconnected or a small number
of vertices that are high connectivity
to the rest and these small things
happen in very tiny regions in sparse
graphs so in a dense graphs very tiny
regions don't matter so you can just
forget about them but in sparse graphs
even very tiny regions can give rise to
a serious anomalies and this
classification you know what are the all
possible structures of sparse graphs
this is not yet captured by anything any
tool that we have in graph theory okay
so we really don't understand this now
so we tried hard to get around this
problem and then you know so with me
Denbo at Stanford I have a paper which
is the same title as a dialogue to start
nonlinear large deviations which goes
beyond this graph theoretic setting in
and bypasses a regularity lemma so I
look at it from this different viewpoint
going beyond the graph theoretic setting
and then using this I'll tell you one
outcome of this of this new theory is
that there is this more recent work of
lubinski angele where they show the
following you considered the editor
negraph GNP you look at the probability
that the number of triangles exceeds the
expected value by a fraction Delta now
Delta is fixed but here n is going to
infinity but and P instead of remaining
fixed is going to 0 but okay we cannot
we have there's this barrier so the
theory is still incomplete but going to
0 slower than this then you have this
explicit formula of the probability it's
like e to the minus N squared P squared
log 1 over P times a constant which is
the minimum of these two things and and
and then you know there is a follow-up
papers I'll tell you about that so so
this problem is open for a very long
time this upper tail problem so there is
there is a paper of Swantee handsome
called infamous upper tail this upper
tail problem for triangles so initially
they didn't even know what's right what
are the right exponents on N and P and
then that was fixed and then there was
you know this log factor was missing and
then you know I was one of the people
who fixed that there's a log factor but
this constant dependence on Delta was
was completely open and now now this is
known because of this work which uses
this and this you know this
comes because of some limitations of
this theory of the nonlinear large
deviation theory and this has been
generalized by these four others more
recently where the go beyond triangles
they go to other sub graph counts and
they can handle these these objects okay
okay so here is a here is a summary so
the main problem is that we do not yet
understand the nature of sparse graphs
the thing is we understand some sparse
graph structures but not all possible
structures in totality so the seminary
lemma gives you a classification of all
possible structures that a dense graph
can have so somehow it gives a complete
classification of that you know as a
compactness theorem so it tells you that
it's a compact essentially it's it's
embeddable in a compact space and you
can identify the row correct metric and
so on but we do not understand the
sparse graphs we don't know how to embed
sparse graphs in a compact space and and
you understand the structure by that
method the problem can be solved by by
you know if somebody can prove a
suitable version of summit of the
regularity lemma for sparse graphs but
you know this would have many
consequences including various famous
things of recent times such as the green
tout theorem that but you know this
there is no hope for a solution as far
as I know so so instead what we did is
you know circumvented the problem using
this these tools of nonlinear large
deviations which unfortunately I don't
have the time to tell you what the main
results are and what the things are but
well it they already have several
breakthroughs and the theory is still
not complete you search hard a 10 to the
minus 1 over 42 and you know that should
go to n to the minus a half so that
needs improvement and from the applied
perspective there is a gap between
networks that apply appear in the real
world versus networks with large
deviation properties can be
theoretically analyzed so so many many
networks appearance in the real world
and you would want to understand what
happens if something where
the main problem with rare events
relevant analysis is that there it's
hard to just do a simulation and okay
you don't need to do math you just do a
simulation ok you can wait forever
simulation will never converge so so you
cannot do a rare event analysis using
simulations you need to do the mat and
we don't know how to do that so that's a
problem that needs to be bridged okay
thank you thank you i think we have time
for some questions if people would like
to ask yes yes i get out of your way
yeah laughs no no it doesn't so facebook
you know the first of all the structure
of facebook it's hard to do mathematical
modeling is mainly because the data is
not available they will give us the data
so so in and I don't see any chance of
that so so we I don't know what what's
their interest you know they you know
they have their they have their own
people who are trying to increase
revenues or that that's mainly mainly
what yeah so yeah the question for the
mic okay yes yes oh sorry are there
examples of dense graphs in real life
that we might be familiar with it not so
much so you know for instance if you
consider very small networks like there
is this example of network of students
at Cal Tech which are like 700 some some
students and and in that it's not dense
but let's say if we consider all people
who are friends of friends okay so then
that it's an artificial example of a
dense network so people have some
artificial examples of that I've heard
some other examples which come up
in from biology and so on but not not so
much so real-world graphs are almost
always sparse question there this is a
microphone if one writes down
similarities theorem in a quantitative
way the quantitative balance are
notoriously a kind of weak does that
have any shadow any effect on your
theory so the thing is that was the main
reason why we needed to develop this new
theory because the quantitative bounds
will let you let P go to infinity only
as well you can use a weak weak
regularity lemma and even if you use a
week regularity lemma that the peak and
go to zero only like some negative power
of log N and that's too slow so we
wanted to break that barrier and get to
n to the minus something and you know
that's why we had to develop this new
theory and the graph theorists think
that you know this this theory may have
some bearing on on how to break the
barriers that cannot be cannot be broken
by semi design but you know we aren't
sure about that so you know so but but
if you just use the qualitative
regularity lemma it won't give you
anything like n to the minus some
negative power of n you won't get that
all right thank you very much that was
lovely
you
hello everyone my name is pleasure yes
Fraulein today I'm going to explain this
Johnson's algorithm for all play pairs
shortest paths so first of all we will
let us look into this definition then
representation so what is a graph or
graph is basically a set of vertices and
edges as you all know an undirected
graph is a pair of V and E V is what
this is at ease the edge and where V is
fine a set of points call what this has
an e is a finite set of edges so an edge
e belongs to e capital e is an unordered
pair so pair of you and we you and we
are both belongs to the set of it so we
have one point as U and one point as V
and the edge which are joining this both
the points R is known as e so E is a
belongs to edge family in directed graph
what happens H E is an ordered pair of U
and V so it is nothing but a vector edge
and edge UV is incident from victor left
for vertex V U and is incident to a
vertex V so an edge which is moving from
u to V is basically a directed graph
basically a directed edge and that graph
is known as directed graph a path from
vertex V to a vertex u is a sequence of
v-0 v1 v2 and all dot V caves so path is
basically a when we move from one vertex
to another so how fast is our sequence
of the vertices vertex of where v-0 is V
and VK is equals to u so V 0 is the
initial point in VK is the final corner
so VJ a VI comma VI plus 1 belongs to e
is a set of H belongs to e for l equals
to 0 to K minus 1 the length of the arc
is defined as the number of edges in the
path so we can define the length of the
path as the number of edges in that path
so the next definition and
representation so how we represent a
directed and undirected graph so
interacting graph is basically
represented with the outer word
vectored edge so the edge is simple and
it is not defined or it is not directing
to any other vertex so a directed is
basically opposite to that other so it
it is directed to some other u to V so
it is incident to you as well as V so
introduction first we will introduce all
pair shortest form like any
communication link like in the word
network or one of the most natural
algorithm question is how we determine
the shortest path from one point to
another so in this project we basically
deal with one of the most fundamental
problems of graph theory so how we wanna
solve it so basically there are three
distinct Sangam to face this problem
which is like the floyd-warshall
algorithm and all pair shortest paths we
have multiplication matrix
multiplication so we gonna means write
all the cost of the edges and then we
used to do some matrix multiplication to
solve that and the Johnsons algorithm is
basically what we are going to deal with
minimum spanning tree is so first we
define the minimum spanning tree a
spanning tree of undirected graph is a
sub graph of G that is a tree containing
all the vertices of them so in the
spanning tree what we do is like we have
all the vertices vertex and and a single
edge which is minimum as well and which
connects to all the vertex so innovating
graph the weight of the sub graph is the
sum of the weight of the edges in the
sub graph a minimum spanning tree is
like a for a weighted undirected is a
spanning tree with a minimum weight so a
vertex are there and the edges are
connected to all the vertex and and the
minimum one is the minimum spanning tree
so next are we going to use this one
minimum spanning tree so like we can see
yeah there are basically 1 2 3 4 5
vertex and each and every vertex is
connected to each other with edge and
each edge is a weighted edge so in order
to find the minimum spanning tree you
basically takes the minimum
H which is connected between the two
vertex so in order to find that we use
several algorithms like this truss and
others so finding the shortest path and
the problem divides into two related
categories and the basically one is
single source shortest path problem and
second one is all pair shortest path
problem so we here in this video we are
dealing with all pair shortest path from
single source shortest path problem is
directed graph consists of determining
the shortest path from fixed one source
vertex to all other vertices so in this
one these are directed path and we
determine the shortest paths from a
fixed vertex to others and all pair
shortest path is this is the distance
problem is that of finding the shortest
path between all pair of Firdous says oh
a graph so in next one all pair shortest
path a PSP SV its acronym given to all
pair shortest path is given her weight
and directed graph G when G contains a
set of vertex and edges and with a
weight function W weight is here's the
cost given to each and every edge that
map's edge to real value weight we wish
to find for every pair of vertices so
this is basically how to find we gonna
find it all pair shortest paths
so given mate graph G we Iike why W so W
is nothing but an integer variable in
decent value which is given to each and
which is the like the cost of each and
every edge the all pair shortest path
problem is to find the shortest path
between all the pair of vertices a
number of algorithms are known for
solving this phone so in Johnson's
algorithm is one of the basic algorithm
which is used for solving this one so it
finds a shortest path between all pairs
and in Big O of v square log V Plus ve v
square is the vertex and E is the edge
set of all the edges and V is the set of
all the vertex house fast graph it is as
simple e better than either repeated
squaring of mattis's or the
floyd-warshall cycle so it is partially
better than this in sparse graph in
floyd-warshall algorithm ITER returns a
matrix of shortest path weight for
of disses or reports at the Indies a
graph contains a negative weight cycle
so in this one what do you basically
implement two different algorithms side
by side like if in here we will
basically see a edge which contains some
negative values so in order so we cannot
directly apply this ra's al ghul so in
order to apply this curve we have to
change it to positive side so we first
apply bellman-ford algorithm then the
edges values will change to positive
then we apply this draws algorithm find
the shortest path so thank you for
watching this video michael eager will
continue with this one thank you hello
everyone my name is Vikram and I am
going to be continuing this presentation
on the johnson's algorithm and before we
get started let's talk about what the
johnson's algorithm is not about the
johnson's algorithm is an algorithm to
find the shortest path between all pairs
inner-directed weighted sparse graph
having a time complexity of B spare log
V Plus ve where V stands for the number
of vertices and E stands for the number
of edges for sparse graphs it is
asymptotically better than either the
repeated squaring of matrices technique
or the floodwall algorithm this
algorithm either returns a matrix of the
shortest path weights for all pairs of
vertices or it reports that the input
graph contains a negative weight cycle
Johnson's algorithm uses two algorithms
as its subroutines they are the Dijkstra
algorithm and the bellman-ford algorithm
now let us take a look at the steps
involved in this algorithm the first
step node Q is added to the graph
connected by zero weight edge to each of
the other nodes in the second step the
bellman-ford algorithm is used starting
from the new vertex tube to find for
each other vertex the least weight of a
path from u to V if this disturb takes a
negative cycle then this algorithm is
terminated next the edges of the origin
or graph re-weighted using the values
computed by the bellman-ford algorithm
and edge from u to V having length W of
UV is given the new length W of UV plus
H of U minus H of V finally for each
knot s the Dijkstra algorithm is used to
find the shortest path from s to each
other vertex in the riveter graph now
let us consider this following example
the first step we add a source s and add
edges from s to all of the worst reese's
of the original graph in the diagram
there s stands for for the second step
we calculate the shortest distances from
four to all of the vertices using the
bellman-ford algorithm the shortest
distances from 4 to 0 1 2 &amp; 3 are 0-5
minus 1 and 0 respectively as shown in
the figure above once we get the
distances we remove the source vertex 4
and we wet the edges using the given
formula that is W of U V is equal to W
of UV plus H of U minus H of V now since
all these weights are positive now now
we can implement the Dijkstra algorithm
to find the shortest path between all
pairs of nodes now we can now in to
check the time complexity of this
algorithm in different graphs is as
follows for dense graphs the algorithm
is ve log V when sparse graphs it is v
square log V which as can be seen is
much better than the floyd-warshall
algorithm in case of sparse graphs and
in case of the Dijkstra algorithm it has
a time complexity of L of V but when the
graph is passed
there is no e and it becomes v square
thank you for watching this presentation
Johnson's algorithm is an algorithm that
improves on iterated bellman-ford and
iterated Dijkstra's algorithm for
finding all pairs shortest paths
ironically it does that by using both of
those algorithms we're going to study
Johnson's algorithm in Pearl and Hermes
atoll again which is a suitable location
because you need to search to find the
shortest paths and this a toll here
we're towing a buoy that measures water
conditions and weather conditions and
sends it to a satellite year-round we
have to replace an old one with this new
one and as you enter the atoll it looks
like you can just go straight across the
water at one point we could see the buoy
we wanted to replace and our able-bodied
seamen tried the head straightforward
but you can't do that in this at all
because it's actually a reticulated maze
of reefs and you keep running into reefs
so to find the shortest path from where
you are to the buoy you actually have to
follow a GPS track that is essentially
tracing your way through a graph so
let's look at how Johnson solved the
problem that we just saw in the previous
screencast so we've been using a weight
function W that map's e or V cross V to
real numbers how to make that symbol
that's the real numbers I guess for each
edge in the graph and it will give the
value infinite otherwise if the edge is
not in the graph and the problem is that
we want to make a modified weight
function we're going to call it W hat
that enables us to run Dijkstra's
algorithm by getting rid of the negative
weight edges so the problem is we we
ignored one requirement in that example
that I gave in the previous screencast
the requirement that we met with the
example in the previous screencast was
that we want to have the case that for
all edges u V in E the the W hat will be
non-negative so we did that but it
didn't work and it didn't work because
we neglected another requirement that
were going to meet here and that is the
requirement that essentially if we find
a shortest path in the graph with the
modified weight it'll also be a shortest
path in the original
so if a path is a shortest path from u
to V using W we want that to be true if
and only if it's a shortest path from u
to V using W hat
in other words finding a shortest path
and transformed graph finds it's a
shortest path in the original graph
tried a naive approach to coming up with
W hat and it didn't work
Johnson's insight was a better way that
meets both of these conditions to
understand this we need to start with
something called the rewedding lemma the
rewriting lemma will start with a graph
G V that's weighted and say let H be any
function that map's vertices also to the
real numbers limit doesn't care what
function H is now the lemma defines W
hat as follows it's W plus h of the
start vertex minus H of the end vertex
and keep in mind this H can be any
function that max maps vertices to
numbers now let's look at any path in
the graph we call it P it's a path from
V naught to BK the waiting lemma then
says that this meets conditions one
which will write out as follows just
mirroring the definition above it's also
going to have another conclusion which
will be that the graph G has negative
weight cycle under W if and only if it
has it under AB u prime so that's going
to let us test in the modified graph or
a negative weight cycle and it will tell
us whether there's one in the original
graph so how do we prove this first
we're going to show that this definition
this property here which was defined on
edges also applies to paths I'm going to
do so as follows
let's define W hat of the path to be the
obvious sum of W hat of all the edges on
the path
so this just says sum up for Michael 1
the k:w have of each edge you know I - 1
being 0 so that's starting at V sub 0
and so on at the V sub case that's just
applying a definition of the sum of the
weights of the paths but now let's apply
our definition here expanding w hat of
UV for each of these terms and in the
sum expanding using this here so we're
replacing w hat with W which is what
this definition does up here and we're
adding in the h's adding and subtracting
the HS that is now the next thing to
notice is that this some telescopes for
example when you've added in V of I
minus 1 you're going to then subtract it
out in the next increment as I
increments up you're going to subtract
it out with this term so the only
remaining terms are the first in the
last H of V 0 and minus H of V K so this
is equal to this so that's this term
here but then what I'm going to write
following is no longer inside the scope
of the summation so we've taken these
things out of the scope of summation
because of the telescoping sum only the
first in this survived the first in the
last survived and here they are but this
is precisely the definition of WP right
here plus h of v-0 minus h of VK so
therefore any path from V to V 0 to V K
as the W hat wait on that path is going
to be the W wait on that path plus the
number H assigns to the first vertex of
the path minus the number H assigns to
the last vertex of the path but how does
this show that if he is a shortest path
from the knot the VK under W it's also
shortest path under B not B K under w
hat well the only difference between
these two is the the H of the knot and H
IV K but those don't depend on the path
those are just the start in the end
position so anything if you have two
different paths and it has one path has
a lower weight than the other one under
this definition here it's also going to
have a lower weight than the other one
this definition here because it doesn't
depend on the path so this shows that
this part here is true what about this
bit about the cycles here well let's let
a cycle cycle is just like a path except
that II not is the same as VK so that's
going to say the cycle is like that path
okay so let's now write AB u hat of the
cycle is going to be from what we just
showed here W of the proposed cycle plus
h of V naught minus H of the K but these
two things are equal they cross out so
it's just simply equal to the weight
under W so the weight under any under W
hat of any cycle is going to be exactly
the same as the weight of the cycle
under W so therefore if we find a
negative weight cycle under W it's got
to be the same as a negative weight
cycle under W hat that proves a lemma
but what does it do for us well bringing
back in these two properties we need to
meet we've just shown that it meets
property one down here and it's
remarkable that that this lemma holds
under any assignment of H H can assign
any way to the vertices and the shortest
paths and negative weight cycles will be
preserved and since we have that
flexibility we're going to use that
flexibility to get property two under
Johnson's
other insight which brings us to his
choice of what H is we've shown property
1 now we just have to pick an HIV that
meets property 2 which is that it will
always be greater than or equal to 0 w
hat we defined is w UV plus h of u minus
h of v and that's got to be greater than
equal to 0 so how does Johnson do it now
the motivation for how Johnson got this
insight is from material we haven't
covered so we're just going to have to
take this as an insight out of the blue
and then we will show that it works
we're going to define a new graph and
we'll have a vertex set V prime which is
V with a new vertex S added and it'll
have an edge set E prime which is the
old edge set plus we're going to add an
edge from
ask to every vertex in the set of
vertices and finally here's how we're
going to define W from s to V so note
that no edges enter s so therefore G
Prime has to have the same cycles as G
including any negative weight cycles if
they exists so before we go on let's
make sure we understand what this means
if we start with this as our original
graph then we're going to add a start
vertex s it's going to have an edge to
all the vertices and that weight is
going to be 0 and that will look like
this so here's the original graph but
now we've got a new vertex and we've
added 0 weight edges to all the other
vertices and here's the crucial step how
he defines H of V H IV is going to be
the distance of the shortest path from s
to V so in terms of that example we just
looked at again here we have the
original graph the vertices are labeled
with their labels on the inside but then
when we augmented it here we've moved
the labels outside so it's 1 2 3 4 5 or
the labels outside and the shortest path
from the start vertex to each vertex
isn't now indicated inside the vertices
of course usually that would be 0 but in
some cases where there's negative weight
edges in the graph it's not 0 so here
the shortest path is 0 but here we can
get to this vertex by negative 4 because
that's cheaper than going the direct
route by 0 so notice that they're either
going to be 0 or they're going to be
negative they can never be positive
because I always get to a vertex by cost
to 0 from the start vertex so what we
need to show is that this definition of
H V gives us this property property to
that it's always going to be greater
than 0 so our claim is that if we define
W hat to be W plus h of u minus H of V
under this definition then it will
indeed be greater than equal to 0
proof let's take the triangle inequality
then triangle inequality the distance
from s to B shortest distance from s to
V has to be less than or equal to the
shortest distance from s
to you plus the weight from u to V now
if we defined HIV to be Delta SV we can
substitute that in here for both V and u
and we get this expression and now if we
subtract HIV from both sides we're going
to get zero w u the plus h of u minus H
of V which is exactly what we wanted to
prove w UV + hu - HV is greater than
equal to zero therefore we've met
property two so what this means is that
we can use HIV to define W hat and that
adjusts the weights in the graph weights
between the edges are adjusted in this
manner and that's guaranteed that if we
find a shortest path in the new graph it
will be the shortest path in the old
graph and furthermore if we find cycles
in one negative weight cycles and one
there will be negative weight cycle
corresponding negative weight cycle in
the other and we should add that this is
equal to W hat of U V so how does this
work well let's get back to this graph
here here's the Augmented graph and
let's look at this negative weight edge
you want to get rid of we're going to
give it a new weight so the w hat weight
will be the old way plus the H of its
start vertex and minus H of its end
vertex remember the H is equal to the
shortest path from this s vertex so the
shortest path here was zero in the
shortest path here was negative four so
the value is in the circles are the H so
we're going to take W UV which is
negative four plus h of u which is zero
minus H of V which is negative 4 so
negative 4 minus negative 4 negative 4
plus 4 this is going to become 0
similarly we got this edge here weight
of 3 we are going to add H of U which is
0 and we're going to subtract H of V
which is negative 1 so this 3 will
become a 4 and so on and so if you do
that we will get this updated weight
graph which will look like this so here
we have the negative weight edge has
been converted to a zero this has been
converted to a 4 we
rid of the other negative weight edge
and so on well that was pretty
complicated what we finally found our
way to where we want to be we've got the
old we here in the new one and the
diapers go down and start replacing it
as usual under the watchful eye of the
local bosses some pretty mean looking
dudes who aren't very happy so we've
developed the theoretical background
behind Johnson's algorithm but let's
take a look at the algorithm itself here
we can see in the beginning we compute G
Prime that's the Augmented graph where
we add the new start vertex s and then
we add from s to every other vertex in
the graph we add an edge so we're
augmenting e G prime of e is this new
set of edges including the ones from s
to other vertices and we initially set
the weight from s to other vertices to 0
and here's an interesting thing we're
going to run bellman-ford on this
augmented graph and if that returns
false remember that means it has a
negative weight cycle we aren't allowed
to have a negative weight cycle because
that means that there's a potentially
infinitely negative path length from one
vertex to another in the graph we can
have cycles with negative weights on
them they just can't be negative weight
cycle so that will lead us to exit
because the shortest paths are undefined
otherwise we're going to now define the
H value and the H values are defined by
the shortest path that we found up here
in bellman-ford when we search from the
start vertex s to all other vertices
because remember the H value is defined
to be that shortest path from s to all
the other vertices so we're going to set
H to be to every vertex to be those
values and remember that H is now used
to define the new W hat by taking the
original W and adding H of the start
vertex and subtracting the H of the
target vertex so this will require
iterating over
we're making it essentially we're making
a table you know this is a function but
we're making a table that says for each
edge this is the value of this function
for this edge we need a new matrix and
an N by n or V by V matrix to record the
results and then because this is all
pairs shortest paths we're going to run
for each vertex u being the start vertex
we will run Dijkstra's algorithm
starting from that start vertex using W
hat the you know the new weighting
function that gets rid of all those
negative weights so we can run
Dijkstra's algorithm and again it gets
rid of the negative weights in a what
manner that does not cause the problem
that we saw in the first screencast
where at some password were penalized
more than others that computes shortest
paths under this we'll call it Delta hat
which is based on the W hat so it gives
the shortest path distances under this w
hat function but once we found those we
need to convert them back into what the
path links would be under the original W
so whereas here we took the original W
and we added H of the start vertex and
subtracted the H of the N vertex we're
going to reverse that to get the actual
distance by subtracting the H of the
start vertex to undo our adding it here
and then adding the H of the end vertex
to undo our subtracting it here and
finally we returned this matrix of
distances between all pairs so before I
go on I just want to say what is the
point of this is it it's not just for
you to learn Johnson's algorithm it's to
learn some things about how when when a
problem doesn't seem to work with the
algorithms you have you can change the
representation of the problem work with
the algorithms you have so change of
representation is an important concept
here and also the interesting idea that
you can construct more complex or
sophisticated algorithms by combining
existing algorithms like we did here
with Belvin Ford and bike stris
bellman-ford had the property that it
can handle negative weight cycles but
it's not as efficient so we run it once
to discover the negative weights
cycles and bail out of there there at
the same time we run it to get the
shortest paths from s to all the other
vertices that defines this age function
and that enables us to run the more
efficient Dijkstra's algorithm from each
start vertex using our newly defined
wave function w hat but what about the
efficiency of this the runtime to
compute G prime here is going to be
order of e to go over all those vertices
and put in edges from s to compute
bellman-ford is going to be order of V
times e now here the runtime to compute
the H values is going to be for each
vertex so that's also going to be order
of e here we have for each edge we're
going to define the new weight function
for each edge in the graph so that's
going to be order of e if we initialize
this matrix the values in this matrix
that's going to be order of V squared
but all of this is dominated by the next
thing remember Dijkstra's algorithm here
is order of e log V but we're doing it
for each vertex so this whole line here
is going to be order of D for each
vertex times e log D and not
surprisingly this is the same as
iterated Dykstra's Dykstra is the log V
we're iterating at the x so it's not any
more efficient than iterated Dijkstra's
so what have we accomplished well we've
accomplished is we can now handle
negative weights so this is the most
efficient approach especially on sparse
graphs it is possible to improve
performance using Fibonacci heaps as
usual - or V squared log b + v e see the
CLRS text for that but we're going to
use this result here as the dominating
term for Johnson's algorithm
so let's recap with an example here is
our original graph augmented with s and
edges of length of 0 to all the vertices
and the vertices are now labeled with a
shortest path from s so for example how
do we get negative 4 here where you go 0
negative 4 how do we get this negative 1
here where you go 0 2 here negative 5
here 4 here which gives you a negative 1
and so on so that gives the H values in
in the vertices or all the H values that
are going to be used to adjust the
weights and now let's go ahead and
adjust the weights so now you can see
that we have adjusted all the weights
we've got no negative weights anymore
we've either either got 0 or positively
and so now we do the search we throw
away this Augmented part at this point
we just had to use the S and all the
extra links in order to find the H
values and now we throw them away and we
do runs from every vertex so here for
example is a run from vertex 2 we found
all the shortest paths using Dijkstra's
algorithm in this graph the numbers on
the left are the distances in this graph
the number on the right are the adjusted
differences distances in the original
graph so let's see how this works
of course it's 0 distance to itself now
in this graph it found the link of
length 0 to give us a path length of 0
to node 4 but to get the cost in the
original graph remember we have to
reverse the adjustment that was done in
computing w hat so we now have to
subtract the source vertex weight source
vertex weight is negative 1 from the
previous figure and we have to add the
target weight which was 0 so that gives
us this value here of 1 for a more
complex example let's look at node 5 in
this graph we get a path length of
length 2 so we've got 0 to 0 but now to
get the cost in the original graph we
are again going to subtract the source
vertex weight and add the target vertex
weight we don't have to subtract and add
all the weights along the path
because of that telescoping some deal
all we have to do is subtract the source
vertex weight which is negative one so
essentially we're adding one and add the
target vertex weight which was negative
four and that gives us negative one in
the end because that's two minus
negative one which is three plus
negative four makes it negative one so
you can figure this out for all the
other numbers in the graph you should
run an example it's kind of hard to
figure out without actually running
examples and of course we just did this
example we're going to Johnson's
algorithm we'll run it from all the
other vertices as to start vertices and
construct the matrix of all these
shortest paths solutions well that's the
end of a fun day at Berlin Harry's at
all working on Johnston's algorithm this
is David Lynch fogger and Susan
Middleton who are photographers working
for National Geographic and they were
taking specimens that the the fish
people brought back and had a complex
photography setup in the wet lab and
here is the octopus that they
photographed that became the cover of
their book you can google it on
amazon.com so we're all done with all
pairs shortest paths Johnson's algorithm
we just need to take a look at a dynamic
programming alternative
The following content is
provided under a Creative
Commons license.
Your support will help
MIT OpenCourseWare
continue to offer high quality
educational resources for free.
To make a donation or to
view additional materials
from hundreds of MIT courses,
visit MIT OpenCourseWare
at ocw.mit.edu.
PROFESSOR: Today
we start a-- we're
going to take a little break
from parametrized complexity
and talk about something called
exponential time hypothesis,
which I've mentioned a few
times but we haven't really
talked about its many
cool consequences.
I'll abbreviate it ETH.
And the idea is
think about 3SAT.
and the claim is it has no 2
the little of n time algorithm.
So of course you could
make analogous assumptions
about KSAT set for various K.
But in general, the hypothesis
is that for any fixed K, there
is no 2 to the little l of n
time algorithm.
So 3SAT is the smallest
one where that should hold.
2SAT, of course, is easy.
This is not the
maximization version.
And our evidence
for this is merely
that we haven't been able
to find such an algorithm.
There are better than
2 to the n algorithms.
The obvious algorithm
is 2 to the n,
try all possible
assignments times n.
But we won't worry about
the non-exponential part.
The best algorithm to
date is 1.31 to the n.
That's just from
three years ago.
So it's an active
area of research.
Many people have tried to
find better SAT algorithms.
But conjecture is
you cannot get, like,
2 the square root of n, the sum,
and be complete problems have 2
to the square root of
n time algorithms--
we'll see some today.
I'll mention some.
But seems like 2 to
some constant times
n or some constant to
the n power equivalently
is the right answer for 3SAT.
Now one issue to talk
about very briefly--
it won't actually
matter-- is what is n?
Normally n is the
entire problem size,
which would be the
number of clauses,
essentially, because each
clause has three variables.
But the usual
assumption is to say
this is the number of variables
and m is the number of clauses.
But these turn out
to be the same thing.
So one assumption is equivalent
to the other assumption,
so henceforth we don't
need to worry about this.
In general, m is
at most n cubed,
because each clause can have
three different variables,
so there's only n
cubed possible clauses.
But when we're worried
about this little o thing,
we actually care
about polynomials,
because we're in the exponent.
So it turns out you can
sort of think of m and n
as being linearly related--
that's OK-- by something called
sparsification lemma.
It says that if you have a
formula with a superlinear
number of clauses,
you can convert it
into some vaguely reasonable
number of formulas
with a linear number of clauses.
It's, like, 2 to the epsilon
n different formulas,
each with a linear
number of clauses.
And so the end effect
we need to know
is that these two
are equivalent.
We can think of situations
where clauses and variables
are linearly related.
So that's the hypothesis.
Of course, we don't
know how to prove it.
If we proved it, it would
imply p does not equal np.
This is sort of a
strong form of p
does not equal np from
the SAT perspective.
Because we have so many
reductions from SAT,
you can turn this lower bound.
If you assume this
lower bound about SAT,
you can prove
corresponding lower bounds
about other problems we've seen.
While I'm here, let
me just tell you
another version of ETH
called strong ETH, which
is that CNF SAT has no 2 minus
epsilon to the n algorithm.
So the idea is
there's some ideal
constant to the n for each KSAT.
But as K goes to infinity,
that constant goes to 2.
So if you have
really large clauses,
you can't be 2 to
the n, roughly.
This is for all
epsilon greater than 0.
So I won't talk about anything
that needs this assumption,
but there is some work on-- you
can get very specific bounds
on what the constant should
be if you assume strong ETH.
Let's take a review
of past reductions.
And I'm going to start with
3-coloring a graph for here.
So this was our proof
that 3-coloring was hard.
There was a clause
gadget, variable gadget,
this thing of constant size.
And the point is, if I
start with some 3SAT--
this is a reduction from
3SAT-- if I start with a 3SAT
instance that has n
variables and m causes,
the number of vertices
and edges over here
will be some constant
times n plus m.
So 3-coloring, we get order
n plus m vertices and edges.
So what does that give me
in terms of a lower bound?
Well, if I assume ETH that
there's no 2 to the little of n
and no 2 to the little of
m time algorithm for 3SAT,
then I get a corresponding
thing that there's
no 2 to the little of n
time algorithm for graphs
with number of vertices and
number of edges linear in n.
So I state it this way
because for graphs again,
there's the issue of
sparse versus dense graphs.
I could just say
for sparse graphs
here, which each have a
linear number of edges.
So this is, I should
say, ETH implies this.
So while I thoroughly
believe p does not equal np,
p does not equal np, ETH is
a little bit more unknown.
So I'm going to
explicitly mention
ETH teach every time I use it.
And more generally if we look
at various past reductions,
all we need to measure is
something called size blowup.
Remember an np
reduction Karp style,
you start with some instance x,
you make an instance x prime.
And the size of x prime--
this was our reduction
f-- the size of x prime should
be polynomial in the size of x.
What's that polynomial?
That's your blowup.
So if we have n, which is
the size of x over here
and n prime, which is
the size of x prime,
this was polynomial in n
and that is the size blowup.
And so in particular, if
we have linear blowup,
like in this example
of 3-coloring,
then we preserve
the ETH statement.
No 2 to the little o
of n time algorithm,
meaning if there was no 2 to
the little o of n algorithm
for problem A, then
there'll be no 2
to the little o of n
algorithm for problem B,
because you could just convert
to only get a linear size
blowup, run that
algorithm if there was a 2
to the little o of n algorithm.
And then that solves A in 2
to the little o of n time.
So the usual argument,
but now we're
giving explicit bounds
on the running time.
More generally, if you
don't have linear blowup,
let's call this function b of n.
Let's say size of x prime is
always at most some function
b of n, b for blowup.
Then if there's no 2 to the
little o of n algorithm for A,
then there will be no
2 to the little o of b
inverse of n algorithm for B.
So for example, if you have
quadratic blowup, b of n
is n squared, then you
will say that there's no 2
to the square root of
n, no 2 to the little o
of square root of n algorithm.
You can imagine how that goes.
So here are some nice examples
of reductions we've seen.
This one was from lecture seven.
Then from lecture 10, we had
dominating set-- oh, sorry.
I have one more
before dominating set.
This was in the
context of proving
planar vertex cover was hard.
First, planar 3SAT was hard
and then a reduction from 3SAT.
But ignore the planar
issue, because things
are going to be a little
different for planar graphs.
But for general graphs, this
was a reduction from 3SAT.
And there was a constant
number of vertices and edges
for each clause.
And then also here, we had
to make a copy of variable
for each occurrence,
but the total number
of recurrences of all
variables is linear,
because there's only three
occurrences per clause.
So the total number of vertices
in these variable gadgets
is also linear, so the
whole thing is linear.
Size blowup is linear.
And so vertex cover is another
example of this type of result.
Assuming ETH, there is
no 2 to the little o
of an algorithm for graphs whose
number vertices and edges is
order m.
AUDIENCE: Is there a
name for that class?
PROFESSOR: That
should have a class,
but-- I can make
one up, if you want.
We could call it an
ETH-style graph problem,
say sparse graphs no 2 to the
little o of n assuming ETH.
So it's sort of saying it's
linearly related to SAT,
but as far as I know it
doesn't have a proper name.
It should, because
I had to write it
many times in my notes.
Another one was dominating set.
I don't have a slide for
this, because there never
was one, because the
reduction was so simple.
It was if you have an edge
in the vertex cover instance,
you convert it into a subdivided
edge and the original edge
for dominating set.
And then you have
the same problem.
It's the domination.
So it's the same
thing as vertex cover.
You never want to put this
thing in the dominating set.
You might as well move
to one of the neighbors.
So that's again, a
linear size blowup.
And so dominating set
is also in this class,
who shall remain nameless.
And another one
is Hamiltonicity.
We saw a couple of
different proofs.
This one is from lecture seven.
This was ostensibly
for directed,
thought it also claims to
work for undirected graphs.
Linear?
Maybe I'll jump to the
next one, because it's
a bit of a stronger result.
This was maximum degree
3, Hamiltonicity directed.
This was lecture eight.
It's also linear.
I mean, the main
diagram is this.
It's linear if you're not
aiming for planar graphs.
And then there's no
crossover gadget here.
And so the total complexity
of all these things is linear.
So that's cool.
So Hamiltonicity
is another example
of a graph problem with no 2
to the little o of n algorithm,
assuming ETH.
And from this proof-- this was
for directed max degree 3--
and it was also bipartite.
And then we reduced that
to undirected-- sorry.
We reduced from planar
directed max degree 3
to planar bipartite
undirected max degree 3.
And that's of
course also linear.
So all these things
we get for free.
We did them in
different contexts.
That was for planar graphs.
This one was for APX
hardness for independent set.
But we use the same proof.
And we have this biclique
for the variable.
Now here you start to worry
this could be quadratic blowup.
But this was actually a
reduction from 3SAT-3.
So that we didn't have a
very large clique there.
It was actually only
three nodes in it,
so it's more like two edges.
But in general,
3SAT, any constant
would suffice for that proof.
You also have to check that this
reduction from 3SAT to 3SAT-3
is OK.
But it's, again, linear blowup
because the total number
of occurrences of all
variables is linear.
So that was a whole bunch of
free results-- independent set,
3SAT-3.
That's not a graph problem, so
I won't write it in this list.
Now, normally independent set
is the same thing as clique.
In this universe,
that's not quite right
because we're talking
about sparse graphs.
For clique, it's still
the case that there's
no 2 to the little o of
number of vertices algorithm.
But the number of edges used to
be linear for independent set
and becomes
quadratic for clique.
So you have to be a little
careful with clique.
So all is good as
long as we're talking
about non-planar graphs.
What about planar graphs?
Well, this is not true
for planar graphs.
In general, you tend to get
2 to the square root of n
algorithms, and that's
tight, assuming ETH.
So for example, planar 3SAT
we had a crossover gadget.
And in the worst case,
there are a quadratic number
of crossings.
And so the blowup
in our problem size,
because we spend
some constant number
of vertices per crossover,
the blowup is quadratic.
And so for, say,
planar 3SAT, ETH
implies no 2 to the little o
of n or 2 to the little o of m
algorithm-- sorry,
with square root.
So with 3SAT, it's
a little annoying
because we have to think
about variables and clauses
separately.
So the size blowup is not
quite as uniquely defined.
But just analyzing number of
variables, number of clauses
separately, the blowup
is quadratic in both.
So that's the
lower bound we get.
And then I have planar
3-coloring, vertex cover,
dominating set, Hamiltonicity,
independent set.
All of them have the
property that ETH implies.
No 2 to the little o of
square root of n algorithm
for planar graphs.
Now planar graphs
are always sparse,
so I don't need to worry about
how many edges versus vertices.
n is within a constant of both.
How you prove that?
Exactly the same proofs
that we just looked at.
They were all actually proofs
for the planar problem,
but they all had some
kind of crossover.
Either they started from
planar 3SAT, in which case
they were already quadratic.
Like this one was
from planar 3SAT
and it was a linear
blowup from that.
So it's only quadratic overall.
This one was again, from planar
3SAT and linear after that.
This one was from 3SAT.
And then there was a
custom crossover gadget,
which I don't have
the slide for here.
But for each of
these crossovers,
we paid something, so we
get quadratic from 3SAT.
And that's linear, of course.
And this is not a planar
independent set reduction,
so I don't have one here.
You have to fill in your own.
And one other one was coloring.
We did the planar
3-coloring gadget.
Again, you pay constant
for each crossing.
So quadratic reduction
from 3SAT-- all of them
end up being quadratic overall.
Independent set's the only
one I haven't shown you.
And-- cool.
So this is a sense in which
even though the planar problems
are np hard, they a
little bit easier.
Question?
AUDIENCE: So you mentioned
that was [INAUDIBLE].
PROFESSOR: Yeah.
So I think-- I
should double check.
I'm pretty sure all
of these problems
have 2 to the square root
of n time algorithms.
I'm confident enough that
I will write it down.
I think the general approach
is Lipton Tarjan separator.
But that's about the level
of detail I remember.
Oh, yeah-- also,
all planar graphs
have tree width order
square root of n.
And generally, that will
give you such an algorithm.
So that was-- question?
AUDIENCE: Are there
any of these problems
that you can, in a sense,
preserve the difficulty
in a planar graph?
PROFESSOR: Yeah,
that's a good question.
We might get to some.
I'm about to shift gears
into parametrized complexity.
And in that setting-- I
would say generally no.
But there are certainly
some exceptions
where you can encode
a non-planar problem
into a planar structure.
But most natural problems
tend to be like this.
But there definitely
are examples.
We might even see one.
This is sort of-- this could
have been in lecture two,
and maybe it should have been.
But ETH is nice
because it gives you
a bit more of a
quantitative sense
of how much running time
you should expect out
of your algorithms.
It gives you
motivation for going
for linear blowup when
possible, or at least
minimizing your blowup and lets
you distinguish between planar
and non-planar problems.
But we're in the middle of
parametrized complexity.
And I mentioned all
this in particular
because it has an
even bigger impact
on parametrized complexity.
So let's shift
over to that world.
Now first of all, we get two
sort of trivial consequences
just from these statements.
They're trivial,
but in some cases
they're actually interesting.
So they're easy to prove, but
actually give tight answers
for a few problems.
So for the natural
parametrizations,
a vertex cover is a vertex
cover size at most k.
Longest path, which
is the optimization
version of Hamiltonicity,
is their path--
in the parametrized
version, is their path
of length at least k?
Dominating set of size k,
independent set of size k
upper bound and lower bound.
In particular, there can't be
a 2 to the little of k times
polynomial in n algorithm,
because there's no 2
to the little o of n algorithm.
This is assuming ETH.
Because in particular,
kb could be n.
Now this is not exactly
what we care about.
What we care about is whether
there's an f of k times
polynomial in n algorithm.
But this at least gives
you a lower bound on the f.
So in particular, for dominating
set and independent set,
this is not a very
interesting result,
because in fact we will
show, assuming ETH,
these do not have FPT
algorithms at all.
There's nothing of that form.
But for vertex cover,
it's interesting
because that is FPT.
And there is a constant to
the k times polynomial n in n
algorithm.
We saw a 2 to the n
times n algorithm.
And this shows
that that's tight.
So there's no better-- it
gives you a bound on f.
For vertex cover, c to
the k is the right answer
for some constant c.
And if you assume
strong ETH, you
can actually figure out what
the-- well, you could try
to prove with the constant is.
We don't know the right
answer for vertex cover.
Some of these problems, we do.
Longest path, same deal.
It's FPT, so it's easy
to find short paths.
And the algorithm is
like 2 to the order k
times polynomial in n.
And similarly for planar
problems, if we have ETH,
there's no 2 to the little
o of square root of k times
polynomial n for those same
problems on planar graphs.
For clique, actually
this should also work.
Clique is OK because
k is the number
of vertices in the clique.
And so even though the
number of edges is quadratic,
this would still hold.
For a planar clique, of
course it's polynomial.
So I can't put clique down here.
The maximum clique
size is 4, so there's
an n to the 4 algorithm.
Again, this is interesting
because for dominating
set, independent set, vertex
cover and longest path,
there are 2 to the square root
of k times polynomial in n
algorithms.
So this is actually
a tightness result.
There exists 2 to the order
square root of k n to the order
1 algorithms for planar
graphs for those problems.
This is called subexponential
fixed parameter tractability.
And there were a bunch of those
results in the early 2000s.
And then a theory called
bidimensionality kind
of characterizes when it's
possible, or gives you
a big set of examples
where it is possible.
But that's algorithm, so
we're not covering it.
So all well and good.
So for planar or dominating
set, that's interesting.
But for general
dominating set, we
know dominating
set is w2 complete,
we think that means
there's FPT algorithm.
Independent set in
clique, our w1 complete,
we also think that
means no FPT algorithm.
Assuming ETH, we can
actually prove that.
So let's say there's
no FPT algorithm
for clique/independent
set assuming ETH.
So that's a theorem
we will prove.
If you believe in ETH,
then w1-- these problems
are complete for w1--
w1 does not equal FPT.
These are the FPT problems.
And in fact, we can prove
a much stronger bound.
Very non-FPT-- these
algorithms generally
have an n to the order k
algorithm, or if they're in xp,
then they have some n to the
k to some constant algorithm.
But we can't even
reduce that exponent
below k for any-- for clique
and independent set, let's say.
And if you reduce clique
and independent set
to some other problem,
you can, just like we've
been doing over
here, you can keep
track of the parameter blowup.
And if it's a quadratic
blowup, then you'd
get that there's no n to the
square root of k algorithm.
We'll actually do that in
a moment for planar graph
problems.
But for general graphs,
clique and independent set,
no f of k for any
computable function f times
n to the little of k algorithm.
So this is much stronger
than FPT does not equal w1.
And this is a result from 2006,
so fairly recent by Chen et al.
So let's prove it.
It is essentially a
reduction from 3-coloring.
But it's unlike
most reductions we
think about,
because-- well, it's
unlike parametrized reductions.
Question?
AUDIENCE: Sorry, so is this
[INAUDIBLE] from claiming
that the xp hard these problems?
PROFESSOR: Yeah. xp hard
is really way up there.
None of the problems
we've talked about
are xp hard, unless something
happens with p versus np.
AUDIENCE: But xp [INAUDIBLE]
problems that you have--
PROFESSOR: These are
in xp, but they're also
in-- these problems
are actually in w1,
which is much smaller than xp.
Yeah, I mentioned xp because of
the n to the k to some constant
is related in the same vicinity.
But it's not directly about xp.
So normally when we do
parametrized reductions,
we start from a
parametrized problem
and we reduce to a
parametrized problem.
Here, we are reducing from
an unparametrized problem.
3-coloring has no parameter.
And we are going to
reduce to clique,
which has a parameter, namely
the size of the clique.
So it's a little
weird, but you've
got to get started somehow.
So we're going to
introduce a quantity k
and set it how we want to.
So here's the idea.
We are given an
instance of 3-coloring.
We're given a graph.
We're going to split the
vertices into k groups, each
of n over k vertices.
And remember, we know
that 3-coloring has no 2
to the little of
n time algorithm.
That's what I just
erased, assuming ETH.
So I'm going to choose
k in a little bit.
But let me first tell
you the reduction.
So we're going to
create a new graph.
Let's call it g prime, with
k groups of not n over k,
but 3 to the n over k vertices.
Why 3?
Because we are going to think
about all possible 3-colorings
of those n over k vertices.
So it's corresponding.
For every group up here,
we're going to just write down
every possible 3-coloring.
So obviously, n over k
has to be quite small,
every possible 3-coloring of
those and n over k vertices.
So the intent is that
in our clique problem,
that we want to choose
exactly one vertex from each
of these groups.
So k is supposed to be
the size of our clique.
That's why I wrote it this way.
So at the moment, I have
vertices but I have no edges.
Each of the groups is going
to be an independent set,
so that means you can
only choose at most one
vertex from each group,
to make a clique.
And we are going to
connect to colorings
if they're compatible.
by an edge nG prime
if compatible.
So the idea is, here is
one group, size n over k.
Here is another group
of size n over k.
And if you color these
vertices some colors--
I'm only using one color--
and you color some colors over
here, now it's coloring
within the group,
but there are some
cross edges between here
which may be incorrect.
They may be monochromatic.
And so we check
whether the coloring
of this and the
coloring of this is
consistent with all the cross
edges between those two groups.
If it is compatible, if it's a
valid coloring of both groups,
we connect them by an edge.
This coloring corresponds
to single vertex in G prime
and this coloring corresponds
to a single vertex in G prime.
And we add an edge if it's OK.
We don't add the
edge it's not OK.
And if we're looking
for a clique,
that means we need to choose a
coloring for each of the groups
where everything is
pairwise compatible.
And that represents
all the edges.
Every edge is either within
a group, in which case
it was taken care
of at this stage.
I guess I should say is at
most, 3 to the n over k.
I only want valid 3-colorings.
Or the edge crosses
between two groups
and then it will be
considered when we think about
whether there's an edge.
In a clique, there
are pairwise edges
and so everything is
pairwise compatible.
So never mind the claim.
You should be convinced this
is a valid reduction, in terms
of a correctness standpoint,
from 3-coloring to k
clique for any k.
The construction depends on k.
So what do we set k to?
Here is a setting
for k that will work.
I don't have a ton of
intuition for this, other
than the algebra works.
Essentially, we need
to set it just--
we want to set k to, like, a
tiny, super-constant thing.
So just a little bit,
little omega of one.
And I need to give that
little of k a name.
So I'm going to say, let's
say k clique could be solved
in f of k times n to the k
over s of k time, where s of k
is some monotone, increasing,
and unbounded function.
I need that s goes to infinity.
That's the meaning
of little of k
is that you can
divide by something.
And you can assume
without loss of generality
that the something is
monotone increasing,
but in particular it
should go to infinity
as k goes to infinity.
It might go there very slowly.
It could be, like, 1 over
2 to the k or something.
But something little--
s is little omega of 1.
But now I have this quantity s.
And I'm going to set k as large
as possible so that f of k
is at most n and k to the
k over s of k is at most n.
My goal here is to
make k a function of n.
And so one choice of k is
basically f inverse of n.
So f, remember, was the
dependence on k, so here.
This will turn out to work.
So you can think of k
just being f inverse of n.
But there's actually
another constraint.
It's another inverse thing.
I want k to be at most, the
inverse of this relation.
So I'm basically taking the
min of these two functions of n
that will be a function
of n, which is growing.
I mean, you can check.
If you set k to be a constant,
of course this is true.
If you set k to be a constant,
of course this is true.
So you can set it a
little bit superconstant
by inverting this relation.
That gives you some value of
k that would satisfy this,
some function k equals k of
n that would satisfy this.
And I want to take
the min of those two.
Still a growing function of n.
We'll need that in a moment.
And I get these
two inequalities.
Now it is just a computation of
how much running time I have.
So I want to plug
this algorithm-- this
was an algorithm for k clique.
I have this instance
of k clique, which
looks a little weird
because it's got
potentially a lot of vertices.
I'm just going to plug that in.
This is my n prime, the new--
well, the number of vertices
is this times k, because
they're k groups.
So a number of vertices
in G prime is k times 3
to n over k at most.
So we just plug that
into this running time.
And we get f of k times that
number of vertices, k times 3
n over k at most.
So less than or equal to
the power k over s of k.
And now we do some manipulation.
We know that f of
k is at most n.
That will be enough
for this term.
This is at most n times-- I'm
going to split this apart.
So we have k to the
k over s of k power.
And then separately, we
have 3 to the n over k
to the k over s of k.
Again, k to the k
over s of k, that's
something I get to assume
is less than or equal to n.
So this is less than
or equal to n squared.
And then the k's cancel.
And we're left with 3
to the n over s of k.
I'm going to remind you
k is a function of n
that is unbounded and
monotone increasing.
So this is 3 to the little
o of n, also known as 2
to the little o of n.
So I just needed to choose k
to be slightly superconstant.
And I wanted to get rid of these
terms, so I made them at most n
and took those
inverses, took them in.
And boom, we get
a contradiction.
This contradicts ETH.
This implies ETH is false.
So if you assume ETH, running
backwards you get that k clique
cannot be solved in
such a running time.
And so we get this very
strong lower bound.
There's no f of k
times n to the little o
of k algorithm for k clique.
So in particular, k clique is
not fixed parameter tractable.
I think that's pretty neat.
And henceforth, you care
about parameter blowup.
I mentioned it
briefly last class.
But in general, you map some
problem x with parameter k
into a new instance x prime
with parameter k prime.
And k prime just has to be
bounded by any function,
any computable function of k.
But if it's a linear function,
you preserve this strong bound.
If it's quadratic function, then
you get there's no f of k times
n to the little o
of square root of k.
If it's an exponential function,
which is fair game here,
you get a weaker bound.
You still get that
there's no FPT algorithm.
But you don't get a nice--
not a very impressive bound
in terms of n on the right.
AUDIENCE: Is there a name
for this type of reduction?
PROFESSOR: I don't have one.
There's only one
that I know of, so I
don't think I will try
to come up with a name.
Once you have this,
you can-- last
class we reduced k clique
to all sorts of things.
And so we get a
lot of-- for now,
you just reduce from
clique or variations.
And so you get
lots of good stuff.
What do you get?
Last time we covered a
reduction from clique
to multicolored clique
and independent set.
And if you look at that
proof, k prime equals k.
We had a quadratic blowup
in the problem size,
but the parameter
didn't change at all.
So this is good news.
That means this problem,
or these two problems,
has no-- assuming ETH,
there's no f of k times
n to the little
o of k algorithm.
And also, we covered a
reduction to dominating set.
Even though dominating
set was w2 hard,
we still reduced from
multicolored clique
to dominating set.
And then from dominates set,
we could reduce to set cover.
All of these reductions
preserve k exactly.
All we need is that they
preserve it linearly.
But then we get
this kind of result
for all of those problems.
We covered a reduction
for partial vertex cover.
But I think the reduction
we covered was not linear.
But there is one.
So I'll just state
this as a result,
but this is another one
where we covered a reduction,
but it wasn't the
most efficient one.
I think we lost a
quadratic amount.
Any questions?
Yeah?
AUDIENCE: Do we happen
to know if FPT and w1 are
separated assuming
only [INAUDIBLE]
and not assuming ETH?
PROFESSOR: We do not know that.
The best classical
assumption is ETH
implies w1 does not equal FPT.
I also don't know offhand
whether FPT does not equal
w1 implies p does not equal NP.
I think there's this
result along those lines,
but I'm not sure if
that's literally true.
So intuitively it's stronger,
but-- other questions?
AUDIENCE: So this is
strictly better lower bound
than those over there?
PROFESSOR: Right.
Good question.
So before we switched
to parametrized land,
we said-- like over here,
we had there was no 2
to the little of n algorithm.
Here we're getting that
there's no f of k times
n to the little of k algorithm.
I think that is stronger
than the old bound.
Though I guess you have to think
about it problem by problem.
It depends on how k could
relate to n in general.
I think for these
problems though, it
is a stronger result.
Because k is at most n.
And k can be close to n.
So the next topic-- just
taking a little breather.
This is all good for
non-planar graphs.
For planar graphs, we actually
haven't seen any w1 hardness
results yet.
And that's because a lot
of planar problems are FPT.
There are, in fact, 2 to
the square root of k times
polynomial in n algorithms for
a ton of planar graph problems.
But there are some that are
hard, some that are w1 hard.
And as you might expect, this
k becomes the square root of k
because we get a quadratic
blowup, for some problems--
not quite all of them.
So this comes back
to Jason's question.
And maybe I'll go up here.
Let me briefly
mention in general
if k prime of x prime is at
most some g of k of x, this
was part of our--
in the definition
of parametrized reduction,
then if there's no f of k n
to the little of k
algorithm for problem A,
then there is no f
prime of k prime times n
to the little o of g inverse
of k prime algorithm for B.
So I think the
analogous statement
was up here for size blowup
and regular np reductions.
But for parametrized reductions,
I mean, the dependence on k
is just an arbitrary
computable function.
So that doesn't change.
But the exponent
changes correspondingly.
So if you square k, we get the
square root in the exponent.
So let's do some
planar problems.
I'm pretty sure all
of the w1 hardness
results for planar problems
are within the last five years.
So it's a pretty
recent direction.
And the key insight is a
problem called grid tiling,
which I have a slide for.
So this is a problem invented
by Daniel Marx in 2007.
And so the input looks like this
and the output looks like that.
So in general, you're given
a k by k matrix of sets.
Each set has some number
of 2D coordinates.
These coordinates
range between 1 and n.
So it's k by k, small
matrix, but each cell
has a ton of stuff in it,
up to n squared pairs.
And your goal is
to-- in this example,
all the numbers are
between 1 and 5.
So n equals 5.
And it's 3 by 3, so k equals 3.
Your goal is to choose
exactly one element
from these sets, such that
if you look in any column
the first coordinates
are all the same.
Here, the first
coordinate are all 1.
Here, first
coordinates are all 2.
And in any row, the second
coordinate is the same.
Here they're all 4.
Here they're all 2.
Here they're all 3.
So that's a valid solution.
As you might expect,
this is np complete.
But furthermore, it's w1 heart.
I should say sij--
just gives you
some notation-- is a subset
of 1 up to n squared for all i
and j in 1 up to k.
That's what the
input looks like.
The squared means you
have ordered pairs.
And then your goal is to
choose an xij out of each sij
so that in any row, the first
coordinates match any column.
Sorry.
In any row, the second
coordinates match
and in any column, the
first coordinates match.
So claim is this is w1 hard.
And also now, we know w1
hardness is not the most
we could hope for.
We also want to know what
the parameter blowup is
and how we can relate
it back to ETH.
So here we will
get the same bound.
There's no f of k times n to
the little o of k algorithm,
assuming ETH.
So here we will not
lose a quadratic thing.
But notice that this
thing is k by k.
So, while we've defined
the parameter to be k,
there's kind of a quadratic
amount of stuff going on.
You're selecting k
squared different things
in the solution.
That's still OK.
I mean, k is still the number of
rows or columns of the matrix.
But typically, the
reason this problem
is interesting-- this is, like,
a starting point for planar
graph problems,
because you can replace
each of these
cells of the matrix
with the gadget that
somehow represents
all the things in that set,
but that now these constraints
that all the things
in the column
agree in the first
coordinate, you can
think of as a local constraint.
Because really, you just need
that the guy you select here
has the same first coordinate
as the guy you select here.
You only need to
constrain adjacent cells.
Because if his adjacent
cells are equal,
then the whole
column will be equal.
And if adjacent rows have
equal second coordinates,
then the whole column will
have equal second coordinates.
So as long as you
can build gadgets
that just check with
their neighbors,
that will give you a kind of
planar graph structure, or a 2D
geometry structure if you're
doing a geometric problem
and it lets you work with
things kind of locally.
But when you do that, of course,
typically k becomes k squared.
And that's where you get the
square root of k up here.
But it won't disappear yet.
So how do we prove this?
We're going to reduce from
clique because that is
our favorite w1 hard problem.
And it has this kind of bound.
And so it's going to
be a linear reduction.
In fact, k prime will equal
k and n prime will equal n.
n here is the maximum coordinate
values in the original problem.
n prime is the number of
vertices in the clique.
And I'm going to write
down the reduction
and then show you a picture.
It's hard to actually
draw the full reduction.
It's easier to write it
down generically and then
show you kind of a little
slice of a real example.
It's a little bit
confusing, because there are
four parameters lying around.
There's which cell are you
in, which I'm denoting by ij.
i is which row you're in. j
is which column you're in.
So this is the set of
things at row i column j.
But then separately,
there the coordinates
that are inside the cell.
And here I'm denoting
that by vertices, because
for us, what this says is
that the vertices map 1 to 1
with coordinate values.
But these coordinate values
are different from the ij
coordinate values.
The ij's are always
between 1 and k.
These coordinate values
are between 1 and m.
Probably should have a term
for those, that distinction.
But such as it is, ij
is between 1 and k. v
and w are between 1 and n.
So there's two types of cells.
There's cells on the
diagonal of the matrix
and cells off the diagonal.
This is for i not equal to j.
On the diagonal, you just
have pairs of the form vv.
So that's supposed to
represent the vertex.
And so basically what you
choose on the diagonal
is going to correspond to
the clique that you want.
Because the diagonal has size
k, and so each diagonal item
is going to have
to choose a vertex.
It's going to turn out that
vertex will be in a clique.
Why will it be in a clique?
Because the off
diagonal entries are
going to force that
there are edges
between corresponding vertices.
So the off diagonal
entries will have,
for every edge-- and we're
assuming no loops here--
for every edge, we put vw
an item in the set of pairs.
And if this is an undirected
graph, we'll put vwnwv.
And so in fact, all of the off
diagonal entries look the same,
I guess.
And all of the diagonal
entries look the same
in terms of the s sets.
So let's look at an example.
Suppose you choose this 2-2
diagonal entry to be vertex i.
Didn't assume very much.
But from the constraints
of the grid tiling problem,
we know that the whole row here
has the same second coordinate.
And the whole column here has
the same first coordinate.
So if you choose
the vertex i here,
that forces i to appear
throughout there.
And if you look at some other
vertex, vj on the diagonal,
then same thing happens.
You have j second
coordinate here
and j first coordinate there.
I see there's a slight
typo on these slides.
This should be a j.
That should be an i.
The colors are right, though.
So just look at the colors.
Now, for this to be
in the set, there
must be an edge
between vi and vj.
And this is true for all i and
j; therefore you have a clique.
Now one thing that's important
is that vi is distinct from vj.
Otherwise, you could just put
vi in all the diagonal entries,
and everything is vi vi.
But because we said
v does not equal w
for these sets, the fact that
there is a valid choice here,
the fact that vi vj is a
valid thing in this item
means that vi does not equal vj.
So these are all
distinct vertices.
There's exactly k of them.
And so this problem has
a solution if and only
if there was a k clique
in the original graph.
Clear?
I guess these entries are
technically correct if you
view these as unordered pairs.
Because we're in an undirected
graph, everything is flippable.
So that proves that grid
tiling is as hard as clique.
And it was a linear reduction.
We started with value k.
We ended up with a thing
whose parameter was k.
AUDIENCE: Say
something again like,
it just seems like
you've just redefined
your n and k to be square root
of what you might normally.
PROFESSOR: Yeah,
so if you prefer,
you could define k to be the
number of cells in the matrix.
And then what you would get
here is there's no f of k n
to the little o of square root
of k algorithm, assuming ETH.
It's just a matter of
what you define k to be.
You're going to either lose
a square here or later.
And I think-- so I'll
show you why in a moment,
why you might
define it this way.
Because here's going to be a
planar graph problem, kind of,
that does not blow up k at all.
Turns out, sometimes
you don't have
to blow up-- this k turns
out to be the correct k.
So let's do that example.
It's called k outer
planar list coloring.
There's two things
I need to define--
list coloring and outer planar.
Let's start with list coloring.
So in list coloring, given a
graph and for every vertex,
you're given a list
of valid colors.
And your goal is
to color the graph.
Again, no edge should
be monochromatic.
And the color you
choose for vertex v
must be on the list Lv.
So this is a generalization
of k coloring.
k coloring is the
case where Lv equals 1
through k for all vertices.
This is, of course,
a harder problem.
And turns out, it's quite hard.
For example, it's NP
hard for planar graphs.
That's not surprising,
because 3-coloring is NP
hard for planar graphs.
And size of Lv is less
than or equal to 3
for all v. The hardness
of planar 3-coloring
gives you that.
So there's also no
natural parameter here,
because you can't parametrize
by number of colors you have,
because even when it's
three, this problem is hard.
So we're going to parametrize
by something else, namely
a quantity called
outer planarity.
If you know what tree width
is, you can think tree width.
But tree width is a
bit messy to define,
so I'll stick to
outer planarity, which
for planar graphs is
within a constant factor.
So outer planarity-- if
you have a planar graph,
this would be an example of
a graph of outer planarity 2.
Let me draw you an example of
a graph of outer planarity 1.
Suppose all of the vertices
are on the outside face
of your planar graph, or all
the vertices are on one face.
Then that's an outer planar
graph, or 1 outer planar graph.
If you have a graph
where there are
vertices on the outside
face, and if you remove
all of those vertices
from the outside face,
the remaining vertices are
all on the outside face,
this is 2 outer planar.
In general, if you have
to remove the vertices
on the outside face
k times before you're
left with no vertices, then
your graph is k outer planar.
And that k is your
outer planarity.
So this is an example of a
problem that's-- there's no
natural parametrization because
it's not an optimization
problem.
So we're going to throw
in a parametrization that
often works out quite well.
Usually if you take
planar graphs parametrized
by outer planarity, they are
fixed parameter retractable.
For example, k
coloring, parametrized
by outer planarity, is FPT.
But list coloring is not.
Question?
AUDIENCE: Doesn't
the outer planarity
depend on the embedding?
PROFESSOR: It depends only
slightly on the embedding.
I think only by an
additive of 1 or something.
So it won't matter from a
parametrization perspective.
Definitely within
a constant factor.
Good question.
So what we're going to
show is that this problem
parametrized by
outer planarity--
so one note is it is in XP.
There is an n to
the outer planarity
and to the k algorithm using the
bounded tree width algorithms,
which I won't go into.
But we will show that is
w1 hard, and assuming ETH,
there's no f of k n to
the little of k algorithm.
So here's an example of
a planar graph problem
where we do not get square root
of k, which I think would also
answer your earlier question.
And this is the reduction.
It's a reduction
from grid tiling.
So the idea is if you
have a k by k grid,
we're going to make something
like a k by k grid graph.
Now we have to represent
the choice here.
We're given a set of pairs
for each of these grid
cells, which we're now
representing as a vertex.
But conveniently, we have
a choice aspect here.
So this is not-- this a thin
[? veal ?] for grid tiling.
We have this list, Lv, for
each vertex of valid choices
you could make for that vertex.
So we're going to
let L uij-- so this
is reduction from grid tiling.
L sub uij equals sij.
So that's the
choice that happens.
For every vertex, we
have to choose one color.
That mimics the fact that
every grid cell in the matrix,
we have to choose
one item from sij.
So most of our work
is already done.
Now we have to constrain.
If you look at two
adjacent vertices,
if they're adjacent
in a row, then they
must have the same
first coordinate--
I can never remember.
They should have the same
second coordinate, sorry,
if they're in the same row.
And if they're in
the same column,
they should have the
same first coordinate.
So these vertices,
which are called
v for the vertical
connections and h
for the horizontal connections,
are going to achieve that.
And it's a little bit
tedious to write down.
Basically, let's say
between uij and ui plus 1j,
so those two vertically
adjacent vertices,
we're going to add a vertex
vijcd for two colors c and d,
with list of size 2 c,d for all
colors not agreeing on first
coordinate.
Again, this is hard
to draw in the figure,
but easier to write down.
So there's a lot of these
vertices in between two
adjacent ui vertices.
There's going to be a bunch.
They're parametrized
by two colors,cd.
Remember, colors
are pairs of things,
but colors correspond to the
items that are in the sijs.
But don't worry about
that so much here,
except there are colors that
are compatible, that they
have the same first coordinate.
And there are colors
that are incompatible.
For any two incompatible
colors, which is most of them,
the ones that don't agree
on the first coordinate,
we are going to add one of these
vertices whose list is exactly
cd, the two incompatible colors.
What that means is, suppose
this vertex chooses c.
Well, then there's a
vertex here with list cd.
It cannot choose c, because
then that edge would be
monochromatic.
So it must choose d, which means
this vertex cannot choose d.
So overall, what this means
is that these two vertices
must choose a compatible
color, because we rule out
all the incompatible pairs.
So list coloring
you can do a lot,
but in particular
we can simulate grid
coloring-- sorry, grid tiling.
We're not exploiting a ton of
the structure of grid tiling,
but we get a nice result
here and it's tight.
I didn't write down what
you do for the horizontal,
but it's symmetric.
So that's nice.
This is one of the few
planar problems where
you don't get a square root.
The next two, you will
get a square root.
But before I get to
the actual problem,
here's a variation
of grid tiling, which
is a little tricky
to prove hard,
but is just as hard
as grid tiling.
Here we need that in every
row-- here's the input,
here's the output.
Easier to look at the output.
In any row-- sorry,
that's a column.
In any column, the
first coordinate
is monotonically increasing.
Doesn't have to
strictly increase,
but you have a less than
or equal to constraint.
This is less than
or equal to this.
This is less than
or equal to this.
4,4,4.
Here they happen to be equal.
2,3,3.
Here they increase a little
bit, similarly in every row.
2,3,5-- they're
monotonically increasing.
1,2,2, 2,2,3.
So this is a valid solution
to grid tiling with less than
or equal to.
That's how it's called.
I will not prove
that this is w1 hard.
I do have a figure for it.
It turns out to be
a linear expansion.
If you have a k by k grid,
we're going to end up with a 4k
by 4k grid.
That part is important.
So we get the same kind of
hardness result, no f of k
and a little o of k, because we
only expand k by a factor of 4.
And this is a description
of what the sets are.
It's kind of messy,
but it effectively
forces that the
choice on the left
is actually equal to
the choice on the right,
even though it only has the
ability to specify less than
or equal to.
But you-- equal is something
like modulo capital n, where
n is some really large number,
like 10 times little n--
something like that.
The details are kind of messy,
so I will instead-- I mean,
we could just take
this as given.
This is, like, a core
problem to start with.
And you can use it to represent
lots of nice planar graph
problems and 2D problems.
So I have one example of each.
These are all from this
upcoming book on fixed parameter
tractability.
So you saw it here first.
This book will come
out early next year.
So here's a problem.
It's called scattered set.
This is a generalization, in
some sense, of independent set.
So let me define it over here.
I would also naturally
call it d independent set.
It might even be called
that in the literature.
So in this problem,
you're given a graph.
And you're given two numbers,
two natural numbers, k and d.
And you want to find k vertices
with pairwise distances
greater than or equal to k.
Sorry, greater
than or equal to d.
So if d equals 2, this
is independent set.
Independent set
says the distance
between any pair of chosen
vertices should be at least 2.
There's no adjacent
ones of distance 1.
d equals 1 is not interesting.
d equals 2 is when this
problem becomes hard.
Now interestingly, there are
FPT algorithms with respect
to k and d.
So if k and d are small,
this problem is easy.
And for planar graphs, there's
a subexponential FPT algorithm
as well.
But when d is unbounded,
if it's not a parameter,
only k is, then this problem
is hard even for planar graphs.
So planar is w1
hard with respect
to k, only when d can be
part of the input arbitrarily
large function of n.
And also ETH implies there's
no planar algorithm with time
2-- sorry, f of k n to the
little o of square root of k.
And in fact, for
scattered set, there
is an n to the big O of
square root of k algorithm.
I don't think it's
been published,
but it's mentioned in the book.
And there is-- this is tight.
This says there is no n to the
little o of square root of k
algorithm, even when you get an
arbitrary computable function f
of k in front.
So this is nice.
We're planar, so we
lose the square aspect,
but it's actually a pretty
easy reduction from grid tiling
with less than or equal to.
So you can see the grid here,
k equals 3 in this example.
And here, n equals 5, just like
our earlier example in fact.
All the information is here, as
represented by these little red
sticks.
So we have a 3 by 3 matrix.
In each matrix cell,
we have a set of items.
In this case, there
are two items.
In this case there
are three items.
The items are encoded by
where the red sticks are
in this subgrid.
This is an n by n subgrid
within a k by k matrix.
So for every present
pair in this set sij,
we just add a stick.
Now, the stick is
a really long path.
It's 100 times n.
So we have this n by
n grid, and then we
have these hundred n paths.
Now this is still planar.
You can put that path in there.
And also, this red path is
100, and these are length 1.
Black edges are length 1.
So what's shown here is actually
satisfying assignment where
I choose one of these vertices.
In scattered set,
our goal is to choose
vertices that are very
far away from each other.
How far?
How far is 301 times n plus 1.
Roughly, three red
sticks plus one traversal
of a subgrid plus 1.
Roughly three red sticks.
So if I choose this vertex and
I want to choose one in here,
it's got to be at least
three red sticks away.
So I'm going to get one red
stick here, one red stick here,
and one red stick there.
So that's good.
But that just says
I choose exactly
one out of each of these things.
Once I choose one
of these endpoints,
I certainly can't
choose another one
because it's only
two red sticks away.
I can only choose
one per subgrid.
But then also, I want the
lesser and equal to constraint.
And that's the plus n over here.
So I have three
red sticks plus n.
That's the 1.
Because I have plus n, n is the
width, let's say, these guys.
So once I choose
this guy, I have
to be three red sticks--
1, 2, 3 red sticks away.
But I also need to be
an additional n away.
And here, I am that because
I have 1, 2, 3, 4, 5, 6 away.
I'm actually one
more than n away.
And there's a plus 1 over
there, so that's good.
I'm 6 away.
I need to be 6 away.
And that corresponds
to this being
in the fourth column and this
being in the fourth column.
In other words, it corresponds
to the second coordinate
of this guy being less than or
equal to the second coordinate
of this guy.
So it's exactly the grid
tiling with less than
or equal to constraint
horizontally and symmetrically,
vertically.
Because the distance between
a point here to point
here is going to be go straight
down, jump, use the red stick,
and then go teleport
left right and then
go straight down from there.
So that distance
corresponds to exactly when
all the less than or equal
to constraints are satisfied.
If and only if, so this
a reduction from grid
tiling with less than or
equal to to scattered set.
[INAUDIBLE] is w1 hard.
Now here, notice k
prime is k squared,
because we are choosing
one vertex per matrix cell.
And they're k squared cells.
So here we are losing
the quadratic blowup.
Questions?
One more similar example.
There aren't a ton
of hardness results
here, so not a lot
to choose from.
There are some multi-way
[? Kant and ?] other things.
But among simple
examples, here's
another simple example--
very similar looking.
This is a graph in 2D
plane, so to speak.
You can also think of it as
a unit disk graph problem.
So a unit disk graph is I take
some points in the plane, 2D
coordinates, let's say
given by rational values.
x and y-coordinates
are given by rationals.
And if I look at
any two vertices,
if they live-- if the
distance between them
is less than or equal to
1, then I add an edge.
So here, I might
have a graph-- this
is going to be a lot of
edges with that notion of 1.
You can have big cliques
in a unit disk graph,
but it's kind of planar-ish.
Especially if you
have distant vertices,
they're not going
to be connected.
So you have these local cliques,
but they're kind of connected
in a planar-like way.
Definition clear?
Edge if and only if
distance at most 1.
So what about independent
set in unit disk graphs?
We know independent
set in general's hard.
Independent set in unit disk
graphs is almost as hard.
Again, there's a quadratic
loss in the parameter.
But problem is, w1 hard
and has the same kind
of hardness as scattered set.
By similar kind of
structure, here I'm
actually giving
you the grid tiling
with less than or equal to.
Probably also corresponds
to the last example,
but here it is in the
independent set unit disk
problem.
Independent set in a unit
disk is the same thing
as choosing some
vertices, like this one,
as the center of
a radius one half
disk and then those
radius one half disks
should not intersect each other.
Because these two things
will have distance at least 1
if and only if radius one
half disk and a radius
one half disk here
do not intersect.
So it's really about
choosing centers
for these disks that don't hit.
And so again, what
we're going to do
is imagine an n by n subgrid
within each cell of the matrix.
But not all of those points
are actually in the set,
only the ones that
are in-- in this case,
s1,1 these pairs of guys
as written here, so 1,
1 is in there.
I guess that's that guy.
2,5 is this guy.
3,3 is that guy.
Those points we'll actually
put in the problem.
These tiny dots are
just place markers.
There's no actual point there.
Then we construct the unit
disk graph on the structure.
And again, if we
set the unit right,
and these are super tiny,
in the same way that we
had the red edges,
which were, like,
100 times longer than the
small things over here,
we're going to have, let's say
this thing is 100 times smaller
than that distance, enough
so that these circles act
kind of like squares.
If you look very close to
here, this looks straight.
So these are very
compressed, so it's probably
going to be more like
a factor of n smaller.
These effectively
act like squares.
It's just a matter of
whether the horizontal extent
of this disk hits the
horizontal extent of this disk.
And that is a less than
or equal to constraint.
Once you choose something
in column 2 here,
the next one has to
be column at least 2.
Here, there there's a gap
because we chose column 3.
Here, there's a gap
because we chose column 5.
But for example here, we
chose column 2, column 2
and these guys are
almost touching.
But barely not touching.
And as long as you have
the less than or equal to
constraint on the columns,
then you'll be OK.
The disk won't intersect
and it's an if and only if.
So again, we
represent grid tiling
with less than or equal
to and independent
set or disk packing
problem in the plane.
It's kind of cool.
Questions?
All right.
Well-- oh, more fun facts.
So we have for independent
set in unit disk graphs,
we have that there
is no f of k n
to the little o of square
root of k algorithm.
There actually is
an n to the big O
of square root of k algorithm.
We also get-- and I
won't go through this.
I think it's pretty
trivial, based
on what I said last class.
But we get that
there's no efficient p
test for this problem
unless-- sorry.
This we definitely
get from last lecture.
I said if you're w1 hard
or FPT-- if you're w1 hard
and FPT does not equal
w1, then you are not FPT.
If you're not FPT, there's
no efficient p test,
no f of 1 over epsilon
times n to some constant.
In fact, if you assume ETH,
you get an even stronger form
of that.
And in this example, we
get there is no 2 to the 1
over epsilon to the 1
minus delta power times
n to the order 1 1 plus
epsilon approximation.
This is the original result
from the Daniel Marx paper
that introduced-- this was
his motivation for introducing
grid tiling.
So again, you get-- out
of all these lower bounds,
you also get results about
inapproximability, which
is another reason to care.
Even if you don't care about
parametrized complexity,
these are the only ways known
to prove lower bounds on how
slow your p test has to be.
Because there are p
tests for these problems,
but only so efficient.
That's it for fixed
parameter tractability.
Next class, we'll do something
completely different.
The following content is
provided under a Creative
Commons license.
Your support will help
MIT OpenCourseWare
continue to offer high quality
educational resources for free.
To make a donation or
view additional materials
from hundreds of MIT courses,
visit MIT OpenCourseWare
at ocw.mit.edu.
PROFESSOR: Let's get started.
Welcome back to 6046.
Today, we start
an exciting series
of algorithms for graphs.
We've done a lot
of data structures.
We're starting to get
back into algorithms
with dynamic
programming last week.
And today and the
next few lectures,
we're going to see lots of
cool algorithms about graphs.
First a bit of
recall-- we're starting
with shortest
paths, which you've
seen in 6006 in the context of
single-source shortest paths.
So typically, like you do a
Google Maps query, you think
of, I want to go from A to B.
But what you solved in 6006
was a harder problem, which
is I give you a point A-- here
it's called S for the source.
I give you a source vertex.
And capital V is a
set of all vertices,
capital E is a set of all
edges, remember graph notation.
Let's say it's a directed graph.
You've got edge
weights, like the time
it takes to traverse each road.
And you want to
know how long's it
take me to get from
S to V for all V.
So this is from one given
point to everywhere.
Today, we're going
to solve a harder
problem, which is all-pairs.
I want to go from all A to all
B. But what you saw in 6006
was single-source, where I just
give you one of the vertices,
and I want to know how
to get to everywhere.
The reason you saw this version
and not the A to B version
is because the best way
we know to solve A to B
is to solve this problem.
So at least from a
theory standpoint,
we don't know how to
beat Dijkstra's algorithm
and Bellman-Ford's algorithm
for the A to B problem.
So you get a little bit
more than what you asked
for sort of for the same price.
So let me remind you in a
few different scenarios what
algorithms we have,
and how long they take.
So the scenarios of interest
are the unweighted case,
a non-negative weighted
case, the general case,
arbitrary weights,
positive and negative,
and DAGs acyclic graphs.
These are some
interesting special cases.
And you should have seen in 006
algorithms for each of them.
Let's see if you remember.
So what's a good algorithm
for single-source shortest
paths in an unweighted graph?
BFS, good, Breadth-first
Search, that takes how long?
V plus E, good.
That's-- for graphs, V plus
E is considered linear time.
That's how long it takes
to represent the input.
So you got to look at the
input, most algorithms,
and the BFS is
optimal against that.
But we're going to start
getting worse as we--
well, for these two situations.
So for non-negative edge
weights, what do you use?
Dijkstra.
Ah, everyone's
awake this morning.
That's impressive.
And that takes how long?
This is a tricky question.
V log V plus E, wow, nice.
So this answer kind of depends
on which heap structure
you use, but this
is the best we know.
If you use a Fibonacci heap,
which we don't actually
cover but it's in
the textbook, you
achieve log V for extract
key and constant amortized
for each decreased
key operation.
So sorry, this is
for extracted min,
and this is for decrease key.
And so this is the
best we know how to do
with a Dijkstra-type approach.
If you use other heaps,
you get slightly worse,
maybe you get a log factor here.
But this is good.
This is almost as good as V plus
E. For moderately dense graphs,
if E is bigger than V log
V, then these are the same.
But if your graph is
sparse, like E is order V,
then you lose a log factor.
But hey, it's just a
log factor, not too bad.
We're going to get worse.
So for general weights,
what do you use?
Bellman-ford.
OK.
Which takes how long?
VE, that's the usual statement.
Technically, you should
assume VE is at least V
for this bound to hold.
But that's the way
to think of it.
So this is not nearly as good.
This is a lot slower.
If you think of-- we can
think of two situations.
One is when E is theta V, so a
very sparse graph like a tree
or planar graph or something.
And we could think of
when E is quadratic.
That's the dense case.
So here we get, whatever,
V and V squared for BFS.
For non-negative edge weights we
get V log V in the sparse case.
And we get V squared
in the dense case.
And for Bellman-Ford, we get
V squared in the sparse case,
and V cubed in the dense case.
So this is like a V
factor, a linear factor
larger than non-negative
edge weights--
makes a huge difference.
And finally for acyclic
graphs, what do you do?
STUDENT: Dynamic programming.
PROFESSOR: Dynamic programming
is one answer, yeah.
That works.
In some sense all
of these algorithms
are-- especially Bellman-Ford
is a dynamic program.
We'll see that little bit.
Another interpretation?
Topological sort, and
then Bellman-Ford,
yeah-- say, one round
of Bellman-Ford.
So Bellman-Ford actually
works really well
if you know the order you
should relax the edges.
And if in an acyclic graph,
you can do a topological sort,
meaning you visit
all the vertices,
so that whenever you visit
the right endpoint of an edge,
you've already visited
the left endpoint.
If you do Bellman-Ford
in that order, then
you only have to do one
pass and you're done.
Whereas normally, here,
you had to do it V times.
So the total cost of
this is just linear.
Good thing to remember,
especially on quizzes
and so on.
If your graph is acyclic,
you can achieve linear time.
But in the general
case, Bellman-Ford
is your answer
for single source.
Now, these are the
best algorithms
we know for each of these cases.
So I'm not going to
improve them today.
You saw the state
of the art 006.
But for all-pair shortest
paths, we can in some sense
do better, sort of.
So let me just quickly
define the problem,
and then tell you all
of the results we know.
And also, the results
we're going to cover today.
I didn't remind you of
the delta definition.
I want to go over this briefly.
So delta of s comma
v is the weight
of the shortest path from S to
V. The weight is well-defined.
Even though there may
be many shortest paths,
there's one best weight.
But there's some special cases.
It could be infinity,
if there's no path.
That's sort of by definition.
Say, well, it's infinite costs
to get-- if there's no path,
then we said there's
infinite weight one.
And it could be minus
infinity in the presence
of negative weight cycles.
So let's say, if there's
a negative weight
cycle on the way, if you
could reach a negative weight
cycle from s, and then still
get to V from there, then
the best way to get there
is to go to that cycle loop
around infinitely many
times, and then go to V.
OK, so the algorithms
you saw probably
didn't actually compute
correctly in this case.
They just said,
negative weight cycle--
I don't know what to do.
But it's actually not that hard.
With a little bit
more effort, you
can figure out where the
negative infinities are.
We're not going to rely
on that, but I'm just
throwing it out there to make
this a well-defined definition.
Once you have the
shortest path weights,
you can also store
parent pointers,
get the shortest path
tree, then you can actually
find shortest paths.
But again, we're not
going to talk about here.
We'll focus on computing delta,
but with the usual techniques
you saw in 006, you could
also reconstruct paths.
So for all-pairs shortest
paths, we have a similar set-up.
We have a directed graph, V,E.
And we have an edge weight
function w-- in general,
could have negative weights.
And our goal is to find delta of
u comma v for all u and v. OK.
Single-source shortest
paths is the sort of thing
that you might want to do
a few-- just given a graph,
and you want to find a shortest
path from A to B. I said,
this is the best way we know
how to do A to B, essentially.
But all-pairs
shortest paths is what
you might want to do if
you're pre-processing.
If you're Google
Maps, and you want
to be able to very quickly
support shortest path
queries between major
cities, then you
may want to first compute
all-pair shortest paths
for all major cities, because
road networks don't change
very much, the large scale.
This is ignoring
traffic and so on.
Pre-compute this, and then
given a query of two vertices,
come in-- probably
get a million queries
a second-- you could
very quickly know
what the answer is.
And this is the basis for
real world shortest paths.
Typically, you don't compute
shortest paths from A
to B every single time.
You use waypoints along the way.
And you have pre-computed
all-pair shortest paths
between waypoints.
So that's the motivation.
Yeah, I guess in some
sense, internet routing
is another situation
where at any moment
you may need to
know the shortest
path to get to-- the
fewest hop path say
to get to an internet site.
You know the IP address.
You need to know where to go.
You don't need to
know the whole path.
You need to know the next step.
But in some sense,
you're computing
all-pair shortest paths.
That's a more dynamic situation.
OK.
So here are the results we know
for all-pair shortest paths.
I think I'm going to cheat,
and reuse this board.
So same situations,
except I won't think
about acyclic graphs here.
They're a little
less interesting.
Actually now, I'm
curious, but I didn't
intend to talk about acyclic.
And so the obvious thing to
do to solve all-pairs shortest
paths is just run the single
source algorithm V times,
once from each source.
So I could do V times
Breadth-first Search, V times
Dijkstra, V times Bellman-Ford.
And now, I just need
to update my bounds.
OK so VE becomes
V squared plus VE.
If you're a little bit clever,
or you assume E is at least V,
that becomes VE.
If I run Dijkstra
V times, I'm going
to get V squared log
V plus V times E.
And if I run Bellman-Ford
V times, I get V squared E.
OK.
And over here,
everything's just going
to increase by a V factor.
So a little more intuitive is
to think about the sparse case.
I get V squared, V squared
log V, and V cubed.
Check that those match
over there-- 1 equals V.
And over here, I get V cubed,
V cubed, and V to the fourth.
OK.
So pretty boring so far.
The interesting
thing here is that we
can beat the last result.
The last result, which
is the slowest one,
could take as long as V
to the fourth time.
We can shave off
a whole V factor.
So a better general
case algorithm
is called Johnson's algorithm.
That will be the last
algorithm we cover today.
And it achieves
this bound, which
is the same as running
Dijkstra V times.
So it's between V squared
log V and V cubed.
And that's cool, because
this is the best algorithm
we know for all-pairs,
non-negative edge weight,
shortest paths, just
running Dijkstra V times.
Not very smart-- but it's the
best thing we know how to do.
And what this says is, even when
we have negative edge weights,
actually we can achieve the
same bound as running Dijkstra.
This is a bit counter intuitive
because in 006 you're always
told, if you have negative edge
weights, can't use Dijkstra.
Turns out, in the all-pairs
shortest paths case,
you kind of can.
How can that be?
Because this is
a harder problem.
If you could solve all-pairs
shortest paths, of course
you could solve single-source.
And that's actually the luxury.
Because it's a
harder problem, we
have this VE term
in the running time,
which lets us do things
like run Bellman-Ford once.
And running
Bellman-Ford once will
let us run Dijkstra V times.
That's the reason we
can achieve this bound.
But we won't be seeing
that for a while.
For starters, I want to
show you some connections
between all-pairs shortest
paths, and dynamic programming,
and matrix multiplication,
which turn out
to give-- for
dense graphs, we're
just achieving V cubed
in all situations.
So our first goal is going
to be to achieve V cubed time
for general edge weights.
So we're going to first
achieve this bound.
That will be a lot easier.
And then eventually, we
will achieve this bound.
So the Floyd Warshall
algorithm and some of these
will get very close to V cubed.
All right.
So we're going to start
with our first approach
to solving all-pairs
shortest paths-- that is not
using an existing single
source algorithm-- is
dynamic programming.
Someone mentioned that already.
It's a natural approach.
Shortest paths is kind
of dynamic programming.
In fact, most
dynamic programs, you
can convert to single-source
shortest paths, typically
in a DAG-- not all,
but a lot of them.
So we could try
dynamic programming.
Now, I'm going to preach
to you a little bit
about my way of thinking
about dynamic programs.
If you watched the
006 OCW version,
you've seen the five easy
steps to dynamic programming.
And if you haven't, this will
be new, otherwise, a reminder.
First thing I like to think
about in a dynamic program
is, what are the subproblems?
The second thing I like to think
about is, what am I guessing?
I'm going to guess some
feature of the solution.
Third thing is, I want to
write a recurrence relating
subproblems solutions.
Then, I'm basically
done, but there's
a couple wrap up
things, which I'm
going to have to use
another board for.
So number four is, I need
to check that I can actually
resolve these subproblems
in some order that's valid.
Basically, this is saying
that the constraint
graph on some problems
should be acyclic.
Because if there's a cycle
in the constraint graph,
you take infinite time.
Even if you memoize, if you do
infinite recursion-- bad news.
You'll never actually
finish anything,
so you never actually write
anything in the memo table.
So I want to make sure
that it's acyclic.
I mean, this is
really the same thing
we're talking about in
this erased row, which
is topological ordering.
Personally, I like to-- you
could argue that it's acyclic.
I like to just write down,
here's a topological order.
That's a nice proof
that it's acyclic.
If you write that
down as for loops,
then you actually
have a bottom up dp.
If you just take the recurrence,
stick it inside the for loop,
you're done, which
we'll do in a moment.
I guess I need a row for that.
And finally, you need to
solve the original problem.
And then, there's
analysis and so on.
But for specifying
the algorithm,
these are the key
things you need to know.
The hard part is figuring out
what the subproblems should be,
so that your dp becomes fast.
Running time is going to
be number of subproblems
times time per subproblem.
For each subproblem,
usually we're
going to want to guess some
feature of the solution
to that problem.
Once we do that, the recurrence
becomes pretty trivial.
Just for each guess, you
say what it should be.
So these are the
really hard two steps.
And then, OK, we checked
that it's acyclic.
And we make sure
that we can actually
solve our original problem
using one of the subproblems.
Sometimes, our original problems
are some of the subproblems.
I think that will happen here.
But sometimes you need to do a
little bit of post-computation
to get your answer.
All right.
So what am I going to
do for subproblems?
Well obviously, I have a bunch
of different problems involving
pairs of vertices.
I want to find delta of u,v for
all u and v. That, I erased.
But that's the problem.
So I want to know what is
the weight of the shortest
path from u to v?
If I stop there and said
that was my subproblems,
bad things are going to
happen, because I will end up--
there's no natural way to
make this thing acyclic.
If I want to solve u to
v using, I don't know,
u to x, and then
x to v, something
like that-- there's
no way to get out
of the infinite recursion loop.
OK?
So I need to add
more subproblems
to add more features to my
solution, something that
makes it so that when I
try to solve my subproblem,
I reduce it to
other subproblems.
Things get smaller, and so I
could actually make progress.
So there are actually two
natural ways to do this.
I'll call them method
one a method two.
Method two is actually
Floyd Warshall.
But any suggestions on
how we might do this?
This is a harder problem.
This is in some sense,
a kind of guessing,
but it's like I'm
going to guess ahead
of time that somehow
there's an important feature
of the shortest path.
I'm going to
parameterize by that,
and somehow it's
going to get smaller.
Yeah?
STUDENT: [INAUDIBLE]
PROFESSOR: Right,
shortest path--
it uses at most a
given number of edges.
Let's parameterize
by how many edges.
I think I'll use m.
So using at most m edges.
Very good.
So good, I think it
deserves a purple Frisbee.
All right, I'm getting
better, slowly.
By the end the semester,
I'll be pro at frisbee.
I should enter a competition.
So this is, of course, an
additional restriction.
But at the end of the day,
the problem I want to solve
is going to be essentially
duv, let's say, n minus 1.
If I want a shortest
path that does not
repeat any vertices,
then certainly
it has at most n minus 1 edges.
So in fact, the claim would
be that duv equals duv n.
I mean, and so on.
If you go larger than n minus
1, it shouldn't help you.
If you know that your
shortest paths are
simple-- if you know that
shortest paths don't repeat
vertices.
So this would be if there are
no negative weight cycles.
If there are no
negative weight cycles,
then we know it never
helps to repeat vertices.
So in that situation,
we would be
done, if we could
solve this for all m.
Now, slight catch--
well, how do we
know there's no
negative weight cycles?
You know, we could run
Bellman-Ford, I guess.
That's a little tricky,
because that only
finds reachable
negative weight cycles.
In fact from this
picture, we will end up
knowing whether there are
negative weight cycles.
So there will be no negative
weight cycles, if and only
if there's no negative diagonal
entry, say dvv n minus 1.
So it turns out,
this algorithm will
detect there's a
negative weight cycle
by finding that the distance
from v to v is negative.
Initially, it's going to be 0.
If it turns out
to be negative, we
know there's negative
weight cycle.
With more work,
you could actually
find all the reachable
pairs, and so on.
But I'm not going to worry.
I'm just going to say, hey,
a negative weight cycle.
I'm going to throw my
hands up in the air
and give up for today.
OK.
Cool.
So I can solve my
original problem,
if I can solve
these subproblems.
And now, things are easier,
because we can essentially
assume in solving this
problem that we've
solved smaller subproblems
for however we define smaller.
That's what's given by
this topological order.
Obvious notion of
smaller is smaller m.
Presumably, we
want to write this
with an m in terms of this
with an m minus 1 or smaller.
So this gets to
our guessing part.
What feature of a
shortest path could we
guess that would make
it one edge shorter?
There's probably
two good answers.
Yeah?
The next edge, which I guess
you mean the first edge?
Sure.
Could guess the
first edge, or you
could guess the second edge.
Or no, that would be harder.
Or I mean, the last edge,
that would also work.
Okay, this is a harder one.
Uh, nope.
Good thing there's students
everywhere to catch it.
Cool.
So I'm going to
guess the last edge.
That's just how I've
written the notes.
But first edge would
also work fine.
So I'll call the last
edge x comma v. We
know we end by going into v. So
let's guess the vertex previous
to it in the shortest path.
Now of course, we don't
know what that edge is.
The guessing just means try
them all as you saw last time.
So now, it's really
easy to write
the recurrence-- see if I can do
it without looking at my notes.
So we've got duv of m.
We want to write-- we want
to find the shortest path,
so it's probably going to
be a min on the outside.
And we're going to consider
the paths of the form-- d go
from u to x using fewer edges.
Right, if this is the last edge,
then we use m minus 1 edges
to get to x.
And then, we follow
the edge x comma v.
So I'll just add on the
weight of the edge, x comma
v. If x was the right answer,
this would be the cost
to get from u to v
via x, where xv is
a single edge at the very end.
We don't know what x
should be, so we're just
going to do for loop
over x for x in v.
So this is using
Python notation.
And that will find
the best answer.
Done, easy.
Once you know what
the subproblems are,
once you know what
the guessing is,
basically, I'm just adding
in a min and a for loop
to do the guessing.
So that's my recurrence, except
I should also have a base case.
Here it's especially important.
So base case is going to
be when m is the smallest.
So that, let's say, is 0.
What is the weight of getting
somewhere using 0 edges?
Well, typically it's
going to be infinity.
But there is an interesting
situation, where at 0 namely
when u equals v.
Hey, there is a way
to get from u to
itself with 0 edges.
And that costs 0.
But anywhere else is
going to cost infinity.
There's no path.
So the sort of a
definition, I should say.
If it exists, otherwise
it's infinity.
So then I get those infinities.
But this is kind of important,
because maybe I actually use
fewer than m edges.
I wrote less than
equal to m here.
This is also less than equal
to m minus 1 edges here.
But in some sense, I'm
including the case here,
where x equals v. So I just
stay at v at the very end.
So it's because I have
a 0 here, I'm implicitly
including a situation
where actually, I
just use duv m minus 1.
That's in that min here.
So it's important to
get the base case right.
Cool.
Almost done.
I need an acyclic ordering.
So as I said, things get
smaller when m is smaller.
So all that means is do the
for loop for m on the outside.
Then do the for loop for
u in v. For those, it
doesn't matter what
order you do it,
as long as you've done
all of m equals 0,
before you do all of m equals
1, before you do all of m
equals 2.
So that's the nested for loops
that gives you the right order.
And so, I guess I take this
line and put it on the top.
And I take this line, the
induction of the currents,
put it inside the for loops,
that is my bottom-up dp.
OK, I'm actually going to
write it down explicitly
here for kicks.
Should I bother?
Uh, what the hell.
So first we do a for loop on m.
Then, we're going to do
the for loops on u in V.
Now, inside the for loop,
I want to compute this min.
I could use this
single line, but I'm
going to rewrite it slightly to
connect this back to shortest
paths.
Because this type of
statement should look familiar
from Dijkstra and Bellman-Ford.
This is called the
relaxation step.
OK.
It probably would
look more familiar
if I wrote here w of x v.
You could also write wxv.
That's an alternative
for both of these--
probably should
write the same way.
But in either case, we call
this a relaxation step,
because-- it's kind
of a technical reason
but-- what we'd
like to satisfy--
we know that shortest paths
should satisfy the triangle
inequality.
If you look, there's three
vertices involved here,
u, v, and x.
We're looking at the
shortest path from u
to v compared to the
shortest path for u to x,
and the shortest
path from x to v.
Certainly, the shortest
way to get from u to v
should be less than or equal
to the shortest path of u
to x plus x to v, because
one way to get from u to v
is to go via x.
So this if condition
would be a violation
of the triangle inequality.
It means we definitely do
not have the right distance
estimates if duv is
greater than ux plus xv.
OK.
So if it's greater, we're
going to set it equal.
Because here, we know a way
to get from u to v via x.
We know that it's possible to do
this, assuming our d values are
always upper bounds on reality.
Then, this will be an
upper bound on the best way
to get from u to v.
So this is clearly
a valid thing to do.
Relaxations are never bad.
If you start high,
these will always
improve your shortest paths,
so you get better estimates.
And that's exactly what
Dijkstra and Bellman-Ford did,
maybe with w instead
of d, but they're all
about fixing the
triangle inequality.
And in general and
optimization, there's
this notion of, if
you have a constraint,
an inequality constraint
like the triangle inequality,
and it's violated,
then you try to fix it
by the successive relaxations.
So that's where the
term comes from--
doesn't really matter here,
but all of our shortest path
algorithms are going
to do relaxations.
All the shortest path algorithms
I know do relaxations.
So this is familiar, but it's
also doing the same thing here.
I just expanded out
the min as a for loop
over x, each time checking
whether each successive entry
is better than
what I had already.
And if it is, I update.
So in the end, this will
compute a min, more or less.
I cheated also because I
omitted the superscripts here.
If I put m here, and m
minus 1, here and 1 here,
it would be exactly
the same algorithm.
I'm omitting all
the superscripts,
because it can only help me.
Relaxing more can
only get better.
And if I was guaranteed
correct over there,
I'll still be guaranteed
correct over here.
You have to improve
the invariant,
but you never--
relaxation is always safe.
If you start with upper bounds,
you always remain upper bounds.
You're doing at least the
relaxations over there.
And so you will in the end
compute the correct shortest
path weights.
The advantage of that,
mainly, is I save space.
And also, it's simpler.
So now I only need
quadratic space.
If I had a superscript,
I'd need cubic space.
Right?
So I did a little simplification
going from the five step
process to here-- both of the
polynomial time and space,
but this is a little
bit, a little bit better.
But how slow is this algorithm?
How long does it take?
Yeah?
V cubed, that would be great.
V to the fourth, yeah, good.
Sadly, we're not doing so great
yet-- still V to the fourth.
V to the fourth, I guess
I already knew how to do.
That was if I just run
Bellman-Ford V times,
I already knew how to
do V to the fourth.
So I haven't actually
improved anything.
But at least you see, it's all
dynamic programming in there.
So n here is the size of V.
That's probably the first time.
Cool.
I omitted 0, because
there was the base case.
That's done separately in
the line that I didn't write.
So that was dp one.
Time for dp two, unless
there are questions?
Everyone clear so far?
Yeah?
STUDENT: When you
iterate over x,
why do you do you
every [INAUDIBLE]
PROFESSOR: As opposed to--?
STUDENT: Like, just adjacent
vertices [INAUDIBLE]
PROFESSOR: Oh, yeah.
OK.
Good, fair question-- why do I
iterate over all vertices, not
just the incoming?
If I'm writing w of xv, I
could afford to just say, just
consider the incoming vertices.
And that would let
me improve probably
from V to the fourth to V cubed
times-- V squared times E,
I think, if you do
the arithmetic right.
You could do that.
It would be better.
For dense graphs, it's
not going to matter.
For sparse graphs
it will improve
V squared to E, basically.
But we're going
to do even better,
so I'm not going to
try to optimize now.
But, good question.
When I say this at
the moment, if there
is no edge from x to v,
I'm imagining that w of xv
equals infinity.
So that will never
be the minimum choice
to use a non-edge.
I should say that.
If there's no edge here,
I define the weight
to be infinity.
That will just make
algorithms cleaner to write.
But you could optimize
it the way you said.
So, where were you?
Yeah.
Ah, perfect.
OK.
Other questions?
More Frisbee practice?
No?
OK.
So that was dp one.
Let me do dp two.
Not yet, sorry-- diversion.
Diversion is matrix
multiplication.
Before I get to dp
two, I want to talk
about matrix multiplication.
This is a cool connection.
It won't help us directly
for shortest paths,
but still pretty good.
And it will help-- it will
solve another problem especially
fast.
So shortest paths is
also closely linked
to matrix multiplication,
a problem we've
seen a couple of times,
first in the FFT lecture,
and then in the
randomization lecture
for checking matrix multiplies.
So you remember, you're
given two matrices, A and B.
And you want to
compute C equals A
times B. You've seen Strassen's
algorithm to do this.
There's also these--
here A and B are squared.
OK.
So the n by n, product
will be n by n.
So standard approach
for this is n cubed.
With Strassen, if
I can remember,
you can get n to the 2.807.
And if you use CopperSmith
Winograd, you get 2.376.
And then, if you use the
new Vassilevska-William's
algorithm, you get n to
the 2.3728 and so on.
And that's the best
algorithm we know now.
There's some evidence maybe
you can get 2 plus epsilon
for any epsilon.
Turns out, those are going
to help us too much here.
But what I want to show is
that matrix multiplication
is essentially doing
this, if you redefine
what plus and dot mean.
We redefine addition and
multiplication-- talk about
whether that's valid
in the moment--
so remember what is
matrix multiplication?
Cij is a dot product
of a row and a column.
So that's aik with bkIj.
K equals 1 to n.
OK.
Now that sum looks
a lot like that min.
Actually, more like the
way I wrote it over here,
with the d's instead of the w's.
This-- right-- x is the
thing that's varying here.
So this is like, aik plus
bkj, except, I have plus here,
whereas I have times over here.
And I have a min out here,
but I have a sum over here.
So it sounds crazy, but let's
define-- be very confusing
if I said define dot
equals plus, so I'm
going to define a new
world called circle world.
So if I put a circle around
a dot, what I mean is plus.
And if I put a circle around
a plus, what I mean is min.
OK?
So now, if I put a
circle around this dot,
I mean circle everything.
So I've got to circle the
summation, circle this thing.
So then I get shortest paths.
Crazy.
So all right, I'm going to
define d to the m-th power.
I should probably
circle-- whatever.
Slightly different.
OK, I want to define,
like, three things at once.
So let me write them down,
and then talk about them.
So many infinities.
All right.
OK.
I guess, I should
write this too.
OK.
If I define the vert-- suppose
I number the vertices 1
through n, OK?
I just assume all vertices are
an integer between 1 and n.
So then, I can actually express
things in a matrix, namely
the weight matrix.
This kind of defines
the graph, especially
if I say wij is infinity
if there is no edge.
Then, this is the matrix of
all pairwise edge weights.
For every i and j, I
have a weight of ij.
That gives me a matrix, once
I set V to be 1 through n.
Now, I'm also defining this
distance estimate matrix.
So remember, we
defined duvm-- I'm
going to now call it dijm,
because the vertices are
integers.
That is, the weight
of the shortest path
using, at most, m edges.
If I define it that
way, then I can put it
into a matrix, which is for
all pairs of vertices ij.
What is the distance,
shortest pathways
that uses it at most m edges?
That gives me a matrix,
d parenthesis m.
Then, I claim that if I take
circle product between d
of m minus 1 and w, that is
exactly what's happening here,
if you stare at it long enough.
This is the inner
product between row u
of d to the m minus
1, and column v of w.
And that's exactly what this
circle product will compute.
So this is dp.
But when you look at
that statement, that's
saying that d to
the parentheses m
is really w to the
circle power m, right?
This is a definition
in some sense of power,
of exponentiation,
using circle product.
So when I circle
the exponent, that
means I'm doing circle
exponentiation in circle land,
OK?
OK so far?
So this is circle land.
So you might say, well, then I
should compute these products
using matrix multiplication.
Now, just to see how
good we're doing,
if I execute this operation
n times, because I
have to get to d
to the n minus 1--
so it's basically d to the n.
If I do this product n
times, and for each one of I
spend n cubed time, then I get
an n to the four algorithm.
Same algorithm in fact,
exactly the same algorithm--
I've just expressed it
in this new language.
OK, there are two ideas
on the table though.
One is, maybe I could use a
better matrix multiplication
algorithm.
Let's shelve that for a moment.
The other possibility
is, well, maybe I
can exponentiate faster than
multiplying by myself n times,
or multiplying by w n times.
How should I do it?
Repeated squaring, good.
You've seen that
probably in 006.
Repeated squaring idea is, we
take-- to compute w-- well,
I take w to the 0.
I multiply it by w to the 0.
Sorry, circle 0--
that's this thing.
Oh, that seems weird.
Let's start with 1.
1 seems better.
I'm not going to get much if
I multiply that by itself.
I should get exactly
the same matrix.
So I take the circle product
between w to the 1, w to the 1.
That gives me w to the 2.
And then, I take w to
the 2 times w to the 2.
Everything's circled.
I get w to the 4.
Oh cool, I doubled my exponent
with one multiplication.
If I take w to the 4 by w to the
4, I get w to the 8, and so on.
My goal is to get to n, so I
have to do this log n times.
Log n squaring operations,
each squaring operation
is an n cubed thing.
So this is repeated squaring.
And I get V cubed log V--
finally, an improvement.
So we went from V to the 4,
which was in the dense case
the same performance
as Bellman-Ford,
running it V times.
But now in the dense case,
I'm getting V cubed log V,
which is actually pretty good.
It's not quite V
cubed, but close.
All right.
I'm pointing at V cubed.
This is actually the one
result that is not optimal.
This is the one we
want to improve.
But we're kind of-- we're
in this space right now.
We're getting close to as
good is this algorithm,
the Johnson's algorithm.
But we still a log V Factor.
So this is great,
just by translating
into matrix multiplication.
Now technically,
you have to check
that repeated squaring actually
gives you the same result.
Basically, this works because
products are associative.
Circle products of
matrices are associative,
which works because circle
land is a semi-ring.
If you want the
abstract algebra,
a ring is something that
you wear on your a finger.
No.
A ring is an algebra where
you define plus and times,
and you have distributivity.
Semi-ring, there's no minus,
because min has no inverse.
There's no way from
the min to re-compute
the arguments, right?
No matter what you apply to
it, you can't-- you've lost
information.
So that's the semi-ring.
Normally, you have a minus.
But semi-ring is enough
for the repeated squaring
to give you the right answer.
However, semi-ring is not enough
for all these fancy algorithms.
So if you look at Strassen's
algorithm, the one you've seen,
it uses minus.
There's no way to get around
that, as far as we know.
So if you have no minus, n cubed
is the best we know how to do.
So sadly, we cannot improve
beyond this with this
technique.
It sucks, but that's life.
However, we can do something.
If we just change
the problem, there
is another problem which
this is the best way to do.
So let me briefly tell
you about that problem.
It's called transitive closure.
Transitive closure
is, I just want
to know is there a
path from i to j.
So it's going to be 1, if there
exists a path from i to j.
And it's going to
be 0 otherwise.
OK.
I guess it's kind of like
if you set all the weights
to 0 or infinity.
Then, either there's
going to be as 0 way path,
or there's no path, meaning
there's an infinite way path.
So it's not quite the same.
Here, I want 1 and 0.
I flipped.
It used to be, this was
infinity, and this was 0.
This is one saying there is a
path from i and j, 0 otherwise.
If I write it this
way, and then I
think about what
I need to do here,
it is still in some sense
plus and min, but not really.
Because I just want to
know, is there a path?
So if I have a way to get
there and a way to get there,
instead of adding
up those values,
really I'm taking
some other operator.
So I want to know OR.
Yeah, exactly-- who said OR?
Yeah, all right, tough one.
Close, close, close.
So here, we have basically
a circle product is OR,
and circle sum is AND.
OK?
I mean plus and min
would work, but it's
a little bit nicer over here.
Sorry, it's the other
way around, I think.
It's definitely Booleans.
We want to know there is a
way to get to x, and then
from x to where we're going.
That's an AND.
And then, to get
a path in general,
it has to work for some x.
So that's the OR.
And this is a ring.
And once you're ring,
you have negation.
You can apply
Vassilevska-Williams.
And you solve this problem
in n to the 2.3728.
And if I just make a little
change in the dot dot dot,
I can absorb the log.
So you could put a log n here.
And it's log n if you get
the exponent exactly right.
But if you just tweak the
exponent by 0.00000001,
that's bigger than log n.
So we usually omit
the log there.
Cool.
Transitive closure-- so it's
a problem you didn't know you
want to solve, but it is
actually a common problem.
And this is the
best way we know how
to solve it for dense graphs.
OK, it beats, you know, V cubed.
This is the algorithm we're
aiming for for dense graphs.
For sparse graphs,
we can do better.
But for dense graphs,
this is better.
Finally, we get to go to
dynamic programming number
two, also known as the
Floyd-Warshall algorithm.
So we had this dp
in V the fourth.
If we forget about
transitive closure,
we've now are down
to V cubed log V.
Our next goal is to achieve V
cubed, no log V. Let's do that.
So again, I'm going to
express it in my five steps.
First step is, what
are subproblems?
And this is the key
difference, and the key insight
for Floyd-Warshall is to
redefine the dij problems.
To avoid conflict,
I'm going to call
them cij, or in this case,
cuv, because here, the matrix
product view will
not work, I think.
Yeah, it won't work.
So it's a totally
different universe.
I'm still going to assume that
my vertices are numbered 1
through n.
And now, the idea
is, first I'm going
to think about the graph formed
by the vertices 1 though k,
roughly.
And I want to know for every
vertex u and every vertex v,
what is the shortest
path from u to v,
or the weight of the
shortest path from u to v
that only uses intermediate
vertices from 1 through k.
So actually, u and
v might not be--
they might be larger than k.
But I want all the vertices
in the path to be 1 through k.
This is a different way
to slice up my space,
and it's the right way.
This is going to do
a factor of n better.
It turns out, and
that's just an insight
you get from trying all the
dp's you could think of.
And eventually, Floyd and
Warshall found this one,
I think in the '70s.
So it was easier back
then to get a new result.
But I mean, this is very
clever-- so very cool idea.
So now the question is,
what should I guess?
Before I guessed what
the last edge was.
That's not going to
be so useful here.
Can anyone think of a
different thing to guess?
We're trying to solve
this problem where
I get to use
vertices 1 through k,
and presumably I want
to use subproblems
that involve smaller k,
that say involve vertices 1
through k minus 1.
So vertex k is relevant.
What should I guess
about vertex k?
Yeah?
STUDENT: Guess that vertex
k is the [INAUDIBLE]
PROFESSOR: You want
to guess vertex k is
the i-th intermediate vertex.
That would work, but I would
need to parameterize by i here,
and I lose another
factor of n if I do that.
So I'd like to avoid that.
That is a good idea.
Yeah?
STUDENT: [INAUDIBLE] visit
k, before you visit v.
PROFESSOR: You're going
to guess that I visit k,
and then I go to where
I'm trying to go.
OK.
That's not a-- OK.
That's a statement.
But to guess, I should
have multiple choices.
What's my other choice?
STUDENT: [INAUDIBLE]
PROFESSOR: Yes.
So either I use
vertex k, or I don't.
That's the guess-- is k in
the path at all from u to v?
So that's a weaker
thing than saying,
k is at position i in the path.
Here I'm just saying,
is k in the path at all?
And that's nice,
because as you say,
I already know how to get
there without using k.
Because that's cuvk minus 1.
And then, you just also have
to consider the situation where
I go to k, and then I leave.
So the recurrence is going to be
cuvk is the min of two things.
One is when k is not in the
path, that's cuvk minus 1.
And the other option is that
I go to x first-- or sorry,
I go to k first.
It used to be x.
Now, I've renamed it k.
I don't know why.
k minus 1-- and then I go
from k to the v-- k minus 1.
That's it.
Min of two things-- before, I
was taking the min of n things.
Before, there were n
choices for my guess.
Now, there are two
choices for my guess.
Number of subproblems is
the same, still V cubed.
But the guessing part
and the recurrence part
is now constant time
instead of linear time.
So I'm now V cubed
time-- progress.
OK?
This is pretty cool.
The old dp led us to this
world of matrix multiplication.
That's why I covered it.
This new dp is just
a different way
of thinking about
it-- turns out to be
faster, just by log factor,
but a little bit faster.
I need some base cases-- cuv
of 0 is going to be-- now
it's the weight of the edge uv.
It's a different base case.
Before, I was using 0 edges.
Now, it's not using any
intermediate vertices.
So that's how the weights
come into the picture,
because actually there are
no weights of edges up here.
So that's a little weird.
The only place the weights
come in is when k equals 0.
This is in some sense
still relaxation,
but it's a little bit weirder,
little bit different order.
I mean the key thing
here is, because the way
we set up these subproblems
with the intermediate vertices,
we know k is the only
vertex in question.
Before it's like, well, I don't
know where you go at the end.
But now we know that either
k is in there, or it's not.
And in each case, we can compute
it using smaller subproblems
and so we save
that linear factor.
STUDENT: Is this only
for [INAUDIBLE] graphs,
or is does it also [INAUDIBLE]
PROFESSOR: This is
for directed graphs.
u and v are ordered here.
And this is the weight from u
to v-- will work just as well.
It's probably a
little bit instructive
to write this down as
nested for loops again.
Why not?
Because then, you'll see
it's just relaxation again.
So I'll even write the base case
here, because it's very simple.
We're doing k in
order, let's say.
These are really the
same kinds of for loops.
But I'll write them slightly
differently, because here we
care about the order slightly.
Here, we do care
about the order.
Here, we don't care
about the order.
Vertices, and all we're
saying is-- almost exactly
the same code as before.
This is, again, just
a relaxation step.
We're just relaxing
different edges
in different orders, basically,
because k is evolved in here.
We do that for k equals 1.
Then for k equals 2, and so on.
But in the end, it's
just relaxations,
so you can use that to prove
that this actually computes
the right shortest paths.
I won't do that here.
But clearly, cubic time
instead of quartic.
Pretty cool.
That's Floyd-Warshall.
It's very simple.
And so a lot of
people-- if you need
to solve all-pairs shortest
paths in dense graphs,
this is the best
we know how to do.
So this is what you
should implement.
It's like, five lines of code.
And you achieve this bound.
But for our sparse
graphs, we can do better.
And the rest of
lecture is going to be
about Johnson's algorithm,
where for sparse graphs
we're going to get
closer to quadratic time.
We're going to match
running Dijkstra,
and it's V squared log
V plus E times V, sorry.
So when E is small, that's
going to be close to quadratic.
When E is big, it's
going to be cubic, again.
So we'll never be worse than
this Floyd-Warshall algorithm.
But for sparse
graphs it's better.
OK.
Johnson's algorithm.
I was going to make some joke
about Johnson and Johnson,
but I will pass.
So Johnson's algorithm--
I mean, dp is five steps,
but Johnson's algorithm's only
three steps-- clearly simpler.
It's actually much
more complicated,
but it's all about
what the steps are.
So here's the crazy idea
in Johnson's algorithm.
We're going to change
the weights on the edges.
And to do that, we're going
to assign weights to vertices.
We're going to
choose a function h.
Think of it as a height
function, I guess,
that maps vertices
to real numbers.
And then, we're going to
define w sub h of u,v.
This is a new way to think about
edge weights that depends on h
that's defined in a simple way.
It's the old edge weight
plus h of u minus h of v.
You could define
it the other way,
but it's better to
be consistent here.
So this is a way to
tweak edge weights.
For every edge-- this is
for directed graphs clearly.
For u, u is the beginning,
the head of the-- I
don't know if it's
the head or the tail--
the beginning of the edge.
v is the end of the edge.
I'm going to add
on the height of h,
and subtract out
the height of v. OK?
Why?
Because that's the definition.
I want this to be greater
than or equal to 0.
That's the such that.
I want to assign a function
h, so that these new weights
are all greater or equal to 0.
This is for all u and
v. Why would I do that?
STUDENT: To use
Dijkstra instead of--
PROFESSOR: To use Dijkstra
instead of Bellman-Ford,
exactly.
So that's step 2.
Run Dijkstra on, I
guess, the usual graph.
But now, this new
weight function,
w sub h, if all the
weights are non-negative,
I can run Dijkstra.
So this will give me what I
call the shortest path sub
h of u comma v for all u and v.
It doesn't give me the actual
shortest path weights I want.
It gives me the shortest
path weights using this wh.
But I claim that's
almost the same.
I claim that this
re-weighting preserves
which paths are shortest.
Because-- so in particular,
I claim that delta of u,v is
delta sub h of u,v--
should be the other way--
minus h of u plus h of v. OK.
If this was a single edge, you
can see I'm just cancelling off
these terms.
But in fact, I claim for a whole
path, every path from u to v
gets changed by exactly
the same amount.
So this is a claim about
the shortest path--
in effect, a claim
for every path from u
to v, shortest or not.
If I measure it in
regular weights w,
versus weights w sub
h, the only difference
is this fixed amount,
which depends only on u
and v-- does not
depend on the path.
And therefore, which paths
are shortest are preserved.
And so when we compute
these shortest path weights,
we can translate
them back to what
they should be in the
original weighting function.
And furthermore, if you
have parent pointers,
and you actually find the paths,
the paths will be the same.
Shortest paths will be the same.
OK.
So let's prove that claim.
It's actually really simple.
Let's look at a
path from u to v.
So I'm going to label the
vertices along the path.
V0 is going to be u.
That's the first one, then
V1, then V2, and so on.
Let's say path has length k.
And Vk is v. OK, that's just
a generic path from u to v.
And now, I want to compute
the w sub h of that path.
Excuse me.
So the weight of a
path is just the sum
of the weights the edges.
So I could write this as
a sum from i equals 1 to k
of w sub h of Vi
minus 1 comma Vi.
I think that works,
got to be careful not
to get the indices wrong.
OK, now, w sub h is defined
to be this thing-- w
plus h of u minus h of v. So
this is the sum i equals 1 to k
of w Vi minus 1 Vi plus h
of Vi minus 1 minus h of Vi.
What does the sum do?
Telescope.
So success-- this Vi is going
to-- this negative h of Vi
is going to cancel with
the plus h of Vi minus 1
in the next term, except for
the very first one and the very
last one.
So this is going to be
this sum, which is just
the weight of the path according
to regular weight function,
plus h of V0 minus h of Vk.
And that is just the weight to
the path plus h of u minus h
of v. Did I get it right?
Nope.
Yes?
STUDENT: [INAUDIBLE]
subtract [INAUDIBLE]
PROFESSOR: But it's not-- it's
opposite of what I claimed.
So right, because it's
the other side, good.
This has h on the
right hand side.
This has not h on
the left hand side.
But here, I have h on the
left hand side, and not h
on the right hand side.
So if I flip it around,
if I take these two terms,
put them on the
left hand side, then
I get this with the right sign.
Cool, whew.
Self-consistent.
OK.
So this was talking
about are arbitrary path.
And so this is proving
the stronger thing
I said, that every
path gets lengthened
by this function,
which is purely
a function of the endpoints.
So in particular, that means
the shortest path in w land
will still be the shortest
path in w sub h land--
slightly less cool name than
circle land, but oh well.
All right, so this means
shortest paths are preserved.
Shortest paths are
still shortest.
And therefore, if I look at
the delta function, which
is about the shortest path
weights, this claim holds.
So that's the
proof of the claim.
Cool.
There's one gaping problem
with this algorithm,
which is how in the
world do you find this h?
If we could find h, then we
know we could run Dijkstra,
and we can do this thing.
And Dijkstra is going to
cost the VE plus V squared
log V. I didn't say it, but
we run V times Dijkstra.
All right, we run it V times.
That's going to take V
squared log V plus VE to do.
This is just going to
take quadratic time,
V squared to update
all the weights,
update all the delta functions.
The missing step is how do
we find this weight function?
I claim this problem of finding
h that has this property,
is very closely related
to shortest paths.
It's weird, but we're going
to use shortest paths to solve
shortest paths.
So let's do it.
Step 1, finding h.
What I want to do, so I
want to have w of u,v--
let me just copy that down--
plus h of u plus h of v to be
greater than or equal to 0.
Whoops, minus.
I'm going to put the h's
on to the right hand side,
and then flip it all around.
So this is like saying h of
v minus h of u is less than
or equal to w of
u,v for all u and v.
This is a problem we
want to solve, right?
w's are given.
h's are unknowns.
This is called a system
of difference constraints.
If you've heard about linear
programming, for example,
this is a special case
of linear programming.
Don't worry if you haven't
heard, because this
is an easy special case.
We're going to solve it
much faster than we know
how to solve lineal programs.
It's a particular kind of thing.
This is actually useful problem.
You could think of,
these are maybe times
that various events happen.
And these are constraints
about pairs of them.
Says, well, the start
time of this event
minus the end time of that
event should be less than
or equal to 1 second.
You can use this to do
temporal programming,
if you could solve
these systems.
We're going to solve
these systems, when
they have a solution.
They don't always have a
solution, which is a bit weird,
because we're relying on them
always having a solution.
How can that be?
Negative weight cycles.
This is all going to work, when
we don't have negative weight
cycles.
And that's exactly going
to be the case when
this system of difference
constraints has no solution.
So let me show you that
in a couple of steps.
First theorem is that if the
graph V,E,w has a negative
weight cycle, then that
system has no solution--
no solution to the
difference constraints.
This is going to be,
again, an easy proof,
kind of similar to
last one actually.
So consider a
negative weight cycle.
Let's call it V0, to V1,
to V2, to Vk, back to V0.
So the claim is the sum of
these weights is negative.
And now, I'm just going to
write down these constraints,
which are supposed to have a
solution, or maybe they won't.
So if it has a
solution, then this
must be true, where u
and v are plugged into be
Vi and Vi minus 1, because
those are all edges.
So I'm going to write h of
V1 minus h of V0 is less than
or equal to w of V0 wV1.
And then h of V2 minus
h of V1 less than
or equal to w of V1 V2.
Repeat that k times,
I'm going to get
h of Vk minus h of Vk minus 1.
And then, the last
one, the wrap around h
of V0 minus h of Vk w of V--
did I get it right-- Vk V0.
What do I do with
these inequalities?
Sum them.
Time for a Good Will Hunting
moment-- do you remember?
I hope we've all seen
Good Will Hunting.
I don't have a
janitor here, so I
have to do all cancels
by hand-- and this.
So I end up with
0 at the bottom.
Everything cancels, and then,
over here I have less than
or equal to the weight
of the whole cycle.
I'm just adding up the
weight of the cycle.
I didn't give the cycle
a name-- call it C.
Now, the cycle has
negative weight.
So this is less than zero,
strictly less than zero.
So we're saying that 0
is strictly less than 0.
That's not true.
So that means
there's no way to get
all of these constraints
simultaneously true-- proof
by a contradiction.
So that establishes a
connection in the direction
we don't want it.
What we want is
they're converse,
which is if there's no
negative weight cycle,
then there is a solution.
Luckily, that is also true.
But this is a little
easier to see.
So now, we do the other half.
OK.
And this will-- I mean, it's
going to be constructive proof.
So we're going to
actually know how to solve
this problem with an algorithm.
So it's going to be-- there
is a negative weight cycle if
and only if there's no solution.
So in particular, the
case we care about
is if there's no
negative weight cycle,
then there is a solution.
We kind of care about both,
but this is the more practical
direction.
So let's prove it.
You can already
see-- you've seen
that there's a connection
in negative weight cycles.
Now, I'm going to show there's
a real connection to shortest
paths.
Negative weight
cycles are just kind
of a symptom of the shortest
paths being involved.
So now, we're going
to use shortest paths.
Suppose we have some graph.
I'm going to draw a simple
little graph with weights.
What I'd like to do is
compute shortest path
from a single source
in this graph.
The question is, which source?
Because none of the vertices--
I guess in this case,
this would be a pretty good
source, because it can reach.
From here, I can
get to every node.
But in general-- maybe
there's another vertex
here-- draw a more
complicated picture.
It could be, there's
no one vertex
that can reach all the others.
For example, it may be
the graph is disconnected.
That's a good example.
So there's no single source
that can reach everywhere.
I really want to
reach everywhere.
So what am I going to do?
Add a new source.
Call it s.
I'm going to add an edge
to every other vertex.
Now, I can get
everywhere from s.
OK?
What are the weights?
0.
0 sounds good.
I don't want to change the
weights, in some sense.
So I put 0, and add
0 to everything.
That's not going to change much.
Now, notice I add no
cycles to the graph.
So if there were no negative
weight cycles before,
still no negative weight
cycles, because the cycles are
the same as they were before.
But now, from s I
can reach everywhere.
If there's no negative
weight cycles,
that means there's a
well-defined, finite value
for delta of s comma v
for all V. And that is h.
What?
It's crazy man.
All right, so we add s to
V. We're going to add s
comma v to e for all
V. That's the old V.
And I'm going to set
the weight of s comma v
to be 0 for all V. OK,
that's what i just did.
And so now, delta of s
comma v is finite for all V.
It's not plus
infinity, because I
know there is-- it's got
to be less than 0, right?
I can get from s to everywhere.
So it's less than
positive infinity.
It's also not negative
infinity, because I've
assumed there's no negative
weight cycles anywhere.
So I'm going to let h
of v be delta of s,v.
I claim that just
works, magically.
That's insane.
Every time I see it, it's
like, got to be crazy
man-- crazy but correct.
That's Johnson.
It's like you just pray that
this happens, and it works.
Why would it happen?
Why would it be that--
what do we want to say--
w of u,v plus h of
u minus h of v--
we want this to be greater
than or equal to 0.
I guess I had already
rewritten this way.
Neither way is the right
way, so it doesn't matter.
So let's see.
We have a weight of u,v. We have
the shortest path from s to u.
And we have this the
shortest pathway from s
to v. We want that to be
greater than or equal to 0.
Why?
Put this over there, and I get
delta s,v is less than or equal
to delta of s,u plus
w of u,v, which is?
Triangle inequality,
which is true.
It turns out, this thing we've
been staring at for so long
is actually just
triangle inequality.
So of course we want to
compute shortest paths,
because shortest paths
satisfy triangle inequality.
The whole name of the
game in shortest paths
is to find a place
where you don't satisfy
triangle inequality and fix it.
So if it makes sense, if
that's possible to do,
Bellman-Ford will do it.
So how we're going to do step 1?
We're going to run
Bellman-Ford once.
We're going to add this
source vertex, so that there
is a clear source to
run Bellman-Ford from,
and then, run Bellman-Ford
from there only.
That will give us
a weight function
for the vertices,
namely how long
does it take to get from
s to those vertices.
Those weights will
actually all be negative.
But then, we're going
to modify all the edge
weights according to
this formula, which
negates some of them.
So some of them are going
to go up some, some of them
are going to go down.
It's kind of weird.
But when we're done,
all of the weights
will be non-negative because
we had triangle inequality.
And now, we can run
Dijkstra from every vertex.
So it's like we
bootstrap a little bit.
We run Bellman-Ford
once, because we know
it handles negative weights.
It will also tell us if there
are any negative weight cycles.
That's why we want this theorem.
Maybe Bellman-Ford says, I can't
satisfy triangle inequality,
because there's a
negative weight cycle.
I don't know what to do.
Then, we know, well actually,
then there was no solution.
OK, that's kind of interesting.
But then, we'll have to
deal with the shortest
paths-- sorry-- deal with
those negative weight cycles.
I won't cover how
to do that here.
But you can.
And otherwise, there's no
negative weight cycles, then
Bellman-Ford finds valid h.
Then, we plug that h into here.
Then, we have
non-negative weights.
So in VE time, we've
reduced to the non-negative
all-pair shortest paths.
And then, we run
Dijkstra V times.
Then, we get almost
our answers, but we
have to modify them to get
back the correct weights
on our shortest paths.
And so we computed shortest
paths in V squared log V
plus VE, because this is
how much Dijkstra costs,
and because Bellman-Ford
takes less time.
We're good.
That's the magic.
And that's all-pairs
shortest paths.
in this video we'll cover Johnston's
very cool algorithm it shows how to use
the rewriting technique we introduced in
the last video to reduce the all pair
shortest path problem in graphs that can
have negative edge links to a single
invocation of the bellman-ford shortest
path algorithm followed by n indications
of Dijkstra's shortest path algorithm
we concluded last video daydreaming
about this best case scenario where we
have a graph with negative edge lengths
but somehow we come up with these
magical vertex weights that transforms
all of the edge links to be non-negative
and it turns out that the magical vertex
weights which will realize this
best-case scenario our best computed by
a shortest path algorithm so before
describing Johnson's algorithm in
general let me just walk you through
some of the steps in an example
so I've drawn a directed graph with six
vertices and seven edges I've annotated
each edge with its cost in blue notice
that some of the edges do indeed have
negative costs so it is not currently an
option to run Dijkstra's algorithm on
this graph
there is no negative cycle however in
this graph it only has one cycle the
directed triangle at the top and its
overall cost is one so we could run the
bellman-ford algorithm on this graph if
we want
so our strategy is to compute a magical
set of vertex weights so that when we
transform the edge links using the
rewetting technique described in the
previous video we wind up with a graph
that now has only non-negative edge
lengths
so where do these vertex weights come
from well the great idea in Johnson's
algorithm is to compute them using a
subroutine for the single source
shortest path problem
to implement this we need a well-defined
instance of the single source shortest
path problem in particular we need a
source vertex
so that's a pretty good idea there's
only one small problem with it which is
that when you pick your arbitrary source
vertex it might not be able to reach all
of the other vertices and to get our
magical vertex weights we're really
going to want finite shortest path
distances from our arbitrary source to
everybody else for example in the graph
that I've drawn here on the slide it
doesn't matter which of the six vertices
you choose as your source vertex you
will not be able to reach all of the
other five
so how do we get around this issue well
with a simple hack we're just going to
add a new vertex so it's 7th vertex in
this example and we're going to connect
this new vertex which I'm going to just
call s2 all of the original vertices
with a direct arc of length 0
we are then going to compute shortest
paths from this new artificial source
vertex s to all of the vertices from the
original graph notice that by
construction we're going to get a finite
shortest path distance from s to all
other vertices we've installed a direct
path from s to everybody else
notice that because this new vertex s
has no edges going into it it's
effectively invisible from the
perspective of all of the original
vertices in the graph G so in particular
the shortest path distance between any
pair of vertices U and V in the original
graph is unchanged by this addition of
the vertex s similarly whether or not G
has a negative cycle is unaffected by
the addition of the vertex s
the next step of Johnson's algorithm is
to invoke a single source shortest path
algorithm using this newly added vertex
s as our source vertex
now in this example and in general we're
thinking about the case of negative edge
lengths so we're not going to be able to
use Dijkstra's algorithm to solve this
single source version of the problem
we're going to have to use the
bellman-ford algorithm to do this
computation
so let's now go ahead and figure out
what are the shortest path distances
from s to the other six vertices in this
graph on the slot
so let's start with the vertex a what's
the shortest path distance from s to a
well it's certainly no worse than zero
there's a path directly from s to a of
length zero and indeed in general all
six of the shortest path distances that
we compute will be zero or less
so the question is there a path from s
to a that is negative that's better than
zero well what are the other options we
could go from s to C at link zero but
then we'd pay for to get back to a so
that is length four so that's no good a
little bit better but still not good
enough would be the zip straight from s
to B that has length zero from B to C
now we're up to now we're down to minus
one but then we have to go from C to a
and so we add four so we get three so we
conclude that the shortest path from s
to a is indeed the direct one hot path
of length zero
most of the other vertices are more
interesting however think for example
about vertex B we could of course zip
straight from s to B along a path of
length zero but we can get shorter than
that
if we go first from s to a at length
zero and then from A to B then that path
has combined length minus two and that
is in fact the shortest path distance
from s to B
the shortest path distance to see is
just to take that same shortest path to
be and then concatenate the edge of cost
minus one from B to C that is the
shortest path from s to T goes s to A to
B to C for a combined length of 0 plus
minus 2 plus minus 1 minus 3 in all
so now let's move to the bottom of the
graph the vertex Z is pretty easy to
think about there's only one path in the
entire graph that's the direct 1 from s
to Z so Z is just going to have the
trivial shortest path distance of 0
so now for the vertex why you got a
bunch of different options you could of
course go straight from s to Y at length
zero but there's a lot of things better
than that you can also go from s to Z
and then along the minus four arc to Y
that would give you a path of length
minus four but you can do even better
than that by going first to a then to B
then to C and then to Y so that gives
you a path whose edge costs are 0 minus
2 minus 1 and minus 3 that gives you a
combined total of minus 6 the shortest
path distance to why
the shortest path to get to X the best
thing to do is to accumulate all of the
negative way to the top of the graph go
via vertex C and it's true you have to
pay the cost of two to get from C to X
but you still end up with a net length
of minus one which outperforms the
direct zero link path from s to X
now the brilliant insight in Johnson's
algorithm is that this shortest path
computation is extremely useful in fact
these computed shortest path distances
are exactly the magical vertex weights
weights that were seeking they're going
to transform all of the edge lengths
from general to non-negative
so let's define the weights piece of V
of a vertex V from the original graph
that is one of the six vertices in this
example that we started with as the
shortest path distance we just computed
from the extra vertex s to that vertex V
what I want to do next is see the effect
of reweighed in using these vertex
weights so to do that let me redraw the
example
so let's recall the formula for the
re-weighting technique given vertex
weights like these you define the new
length C prime e of an edge e say from u
to V as its original length C e plus the
weight of its tail piece of u minus the
weight of its head piece of B
so let's just do this computation for
each of the seven edges let's start at
the top edge a comma B so we start with
original length minus 2 we add the
weight of the tail so we're adding 0 and
then we subtract the weight at the head
so we're subtracting minus 2 that is
we're adding 2 so we get minus 2 plus 2
or 0 for the new length C prime e for
the edge a comma B
similarly for the edge B comma C we take
the original length minus one we add to
it minus 2 and then we subtract minus 3
but as we add 3 and again it all cancels
out we get 0 for the new cost of arc B
comma C
for the ark see comma a we take the
original length for we add minus three
and we subtract zero so the arc C comma
a has a strictly positive shifted length
it's now one
if we look at the arc C comma X so we
take the original length - we add minus
three and we subtract minus one so that
again all cancels out and C comma X has
a new cost of zero
same thing happens with the RFC comma
why we start with minus three we add
another minus three we subtract minus
six
that gives us zero
for the ark is Z comma Y we start with
one we add 0 we subtract minus 1 so we
get a new cost of 2 on the org Z comma X
finally for the ark Z comma Y we start
with minus 4 we add 0 we subtract minus
6 that is we add 6 so that gives us a
new length of 2
so I don't expect you to have intuition
or semantics for the computations that
we just did but at least in this example
the proof is in the pudding
we just used these shortest path
distances as weights and rhe waiting
magically made all seven edges have
non-negative edge links they all have
lengths either 0 1 or 2
so we've now done at least in this
example is realized the best-case
scenario we were dreaming about remember
what the key point of the rereading
technique video was we pointed out that
re-weighting preserves shortest paths if
you have some vertex weights some piece
of v's you change all the edge lengths
by weighting for every origin s and
every destination T you shift the length
of every s T path by exactly the same
amount by P sub s - piece of T the
difference between the vertex weights at
the origin and the destination so by
changing all paths by exactly the same
amount you preserve which path is the
shortest
so that's a cute party trick it's not
really clear would be useful but we're
hoping that maybe rhe waiting in a
shortest path preserving way could allow
us to transform the general edge links
version of a shortest path problem to
the non-negative edge links version of
the problem so that we are not stuck
with those slower bellman-ford algorithm
and instead we get to use the faster
Dijkstra's algorithm
and that's now exactly what we can get
away with here at least in this example
we did the transformation the new graph
has only non-negative edge links now we
can just run Dijkstra's algorithm once
for each choice of the source vertex to
compute all of the shortest path
distances
you
Thanks hey guys I'm Josh and yeah i'll
be talking about radius and diameter in
sparse graphs so a brief outline of what
I'll be talking about today I'll begin
with an overview of the radius and
diameter problems just as a quick recap
next I'll talk about the various
hardness conjectures we'll be using to
prove lower bounds for these two
problems so in this work we talked about
both upper and lower bounds and we
managed to get some of them to match so
I'll talk about what hardness conductors
we need to prove the lower bound half of
that I'll be going more briefly over our
results for truly sub quadratic
approximation in other words like how
well can you approximate these two
problems in n to the two minus epsilon
time and then I'll spend more time
trying to talk at depth about our fixed
parameters sub quadratic algorithms and
we get to that section talk I'll kind of
explain what that term means all right
so radius and diameter so some quick
definitions to begin suppose we have
graph G equals V come e and then we'll
define the distance between two nodes U
and V to be the length of the shortest
path from u to V and we'll define the
East centricity of node denoted a of V
there as the maximum distance from V to
any other node and so the radius problem
just asks what's the minimum
eccentricity of any note inside the
entire graph and that note is called the
center so it says like which node can
reach all the notes quickly and how long
does it take to reach every other node
from the center and the diamond problem
is just the maximum eccentricity but
it's really just the longest path in the
longest shortest path in the graph it's
not too hard to see that if you can
solve all pair shortest paths or apps up
you can solve this problem but apps up
kind of outputs n squared things so
there's n squared time just for
outputting the information because it
outputs the information for all pairs
and there are n things so there n
squared pairs so even when the graph is
sparse apps up will require n squared
time but rate isn't a mateur because
they just sort of output like one number
the e centricity of the minimum is
interested in actually eccentricity
maybe we could possibly do better
and in particular in this paper not only
do we study the normal undirected
definitions of radius in diameter we
have the following natural extensions
for directed graphs so sources necessity
is kind of like the undirected version
but like you only care about the case
where you want to get from V to the
other nodes in the graph round-trip
eccentricity is when you want to get to
the other node in the graph and you want
to get back to your center node V for
sources for maximum eccentricity you
care about the larger of the two
distances and the course the natural
follow-up definition is min e centricity
where you care about the smaller of the
two distances and it turns out that's
actually a pretty hard pretty hard
problem so for all of these definitions
we're still interested in the minimum
eccentricity for radius in the maximum
14 diameter so for example the source
radius problem asks what is the smallest
source eccentricity intergraph and this
leads to 7 directed problems because two
of them are actually the same if you
think about it source diameter and max
diameter or just asking you the same
thing because it doesn't matter which
way you take it for diameter alright
next I'm going to go briefly over the
hardness conjectures that we'll be using
so as everyone's probably really
familiar with the strong exponential
time hypothesis by in parklands Oh Peter
I Zayn just says there's no 2 minus
epsilon to the n algorithm for k set and
it's been used yeah well throughout not
only throughout this workshop but
throughout this quarter to prove a
variety of interesting lower bounds and
many of which show that look this
algorithm we wouldn't we weren't sure
could be improved is actually tight
specifically we will be using some more
polynomial time conjectures so before I
can list a conjecture I'll define a
problem so for the first one is the
orthogonal vectors problem which you're
probably all familiar with as well so
you're just giving two lists of n
vectors in 0 1 to the C log n and the
question is is there a pair of the
orthogonal pairs so vector a from
capital a of vector B from capital B
such that the two are orthogonal and the
orthogonal vectors conjecture just says
there's no truly sub quadratic algorithm
for this problem
so the orthogonal vectors conjecture was
actually used by Radha T and Virginia
who I don't think it's here at the
moment to show that diameter is hard so
we'll be using a different problem to
show why radius is hard and how I will
explain after I show you the next
structure why we kind of need this sort
of change in conjecture to explain why
radius is hard so here's a very related
problem it's called the heating set
existence problem so almost the same
setup you have two lists and vectors
there are C log n coordinates and you
want to know is there a vector in the
first list that is not orthogonal to any
vector in the second list in other words
is there us is there is there a vector
enlist a that hits every vector in this
b and then there's the analogous hitting
set conjecture which matches the
orthogonal vectors conjecture but just
for the hitting set existence problem
instead and the reason i put these
quantifiers up here is to explain better
why we want to move to the hitting set
existence problem for radius so for the
orthogonal vectors problem it's an
existing cyst is there a vector a and is
there a vector and B such that this
property holds the hitting set existence
problem on the other hand says does
there exist the vector in a such that
for all vectors in B we have a certain
property and yeah so the next slide
explains is quite well so how do we get
from these hardness conjectures to lower
bounds for radius in diameter and so for
this slide I'll show you how you can
reduce the orthogonal vectors problem to
a diameter ish looking graph so I've
written the problem up there again but
the key idea is we'll make a set of
nodes on the left corresponding to list
a each note on the left will represent
one of the vectors from this day and
we'll do the same thing for this be on
the right hand side of this graph and
then finally the center of the graph
will have one node for each one of the C
log n coordinates and then we simply
hook up two nodes if the vector here has
this coordinate on so if it's on we'll
draw an edge and so what does it mean
for a pair to be orthogonal so if a pair
is not orthogonal then
they have a coordinate where they share
a one in other words so if if there's
some vector here and some vector here
and they actually are orthogonal
there'll be a two-hot path going between
the two and if they're not orthogonal
there won't be any two iPads because
they won't share any of the coordinates
in common and you can kind of get a
sense of why this leads to a diameter
looking graph so we aren't quite a damn
it yet we have to take a few more steps
to get to any diameter problem but the
key idea is that whether there's an
orthogonal pair or not kind of
determines whether the diameter when you
only consider endpoints and a and
endpoints in b is two or three in this
graph when we had a few widgets okay
cool so how are these conjectures
related and how believable is the
hitting set existence conjecture so like
I said orthogonal vectors we use it to
show our diameter balance and we use
hitting set show radius downs Ryan
showed in 2004 that Seth implies
orthogonal vectors and we actually show
the strange result that hitting set
existence implies orthogonal vectors
which is slightly counterintuitive so if
you call a hitting set was the exists
for all version of the problem and it
has an alternating quantifier so you
kind of expect the problem to be more
difficult than the exists exists version
but this reduction that we've shown our
paper actually proves that the other way
around is true the exists exists problem
is harder than this is for all and it
can be used to solve the exists for all
problem yeah okay so i will briefly go
over our results on how well the radius
in diameter can be approximated in truly
sub quadratic time oh and we say sparse
graphs in the title of this talk because
we care about the case where m equals n
as for most of these results relevant
because we're figuring out when n
squared is the right bound so I guess
the inspiring paper for this work was
the one by Roddy and Virginia in 2013
where they use orthogonal vectors to
show that three halves is the right
answer for undirected diameter and
X diameter and then so this is n to the
roughly n to the 1.5 in a sparse graph
so what we do is we extend these results
to weighted setting so like I said the
minimum eccentricity problems are rather
difficult one interesting case for them
is the case of DAGs so on a dag minimum
eccentricity is the only non trivial
definition of eccentricity for
everything else because the distances
between u and v are always at least
infinity in one of the two directions
all the problems become trivial somme in
diameter remains hard even for dags and
then for round trip diameter so when I
say metric for the algorithm that just
means that because because the the
centricity obeys the triangle inequality
if you just any if you just pick any
point it serves as a two approximation
for the diameter and we managed to prove
of three halves bound from orthogonal
vectors but there's still a bit of a gap
so our results various are all new
because we're introducing the hitting
set conjecture we managed to show that
the algorithm for undirected radius
actually turns out to be tight under
this new conjecture but surprisingly we
actually find that a lot of other
algorithms are tight and not even at
three halves these other items were
tight at two so for source radius we
present a two approximation algorithm
and this one's not trivial because if
you think about it source radius is not
a metric so just because there's a
center that can reach everything quickly
doesn't mean if I pick an arbitrary node
I can get to the center and then get to
any other node quickly so we we spend a
bit of work proving a two bound and then
we also show a to lower bound from the
hitting set conjecture max radius we
show that the met the knife metric using
algorithm is tight and min radius is
still quite difficult although we get
just a three-to-two gap on DAGs and then
we show that the metric case for round
trip radius is tight and if you're
interested in graph algorithms for
radius and diameter you should
definitely take a look at the paper to
see how we do these but like I said for
this talk I really want to focus on our
fixed parameters sub or attic algorithms
so what does this mean so this is a new
framework called fixed parameter
tractable and P and I guess the basic
idea is that typically when you think
about primaries complexity you want to
know what problems can be solved in F of
K times poly end time and then sample k
in this case are like what is the size
of your clique or what is the clause
size of your set instance or what is the
tree with of your graph and typically
this is applied to NP problems it
doesn't make sense for problems in P
because for any parameter there by
definition fpt you just use the same
algorithm and you ignore the parameter
but a more fine-grain approach would be
to pin down what the correct exponent on
the running times in which case pulling
out this F of K factor sometimes does
matter maybe an algorithm is you know in
this case like maybe you think it should
take n squared time but if you pull out
F of K for the right choice of parameter
K you can beat n squared time so what is
the right exponent and for our
particular problems the natural question
is can reduce damage we solved when we
focus on tree with in truly sub
quadratic time after pulling out true
with and what functions of K do we need
yeah so Jenna palomares eosin
Niedermayer also propose an approach
independently this approach in the same
year but their paper only shows upper
bounds on our work both shows upper
bound and upper bound and the lower
bound and so it kind of shows we have
all the tools we need to kind of pin
down the correct running times and this
is pretty exciting new field some recent
results that have followed foamin at all
study max flow parametrized by tree with
and many other problems and they show
roughly linear time algorithms for them
and then a mirror and some other
co-authors also study the subtree
isomorphism problem and in this case the
parameter is the depth of the trees so
these are very recent results that have
come after this work
well and so the parameter we use is true
with I won't go too much in detail
because it was already covered in the
boot camp but just a very very brief
review of the definition it's the
parameterization of under of undirected
graphs problems and the kind of tries to
capture when a graph is very close to a
tree so tree sub tree with one and the
complete graph has tree with n minus 1
so the fundamental take away from true
with is if your graph problem is easy on
trees maybe it's FP t with respect to
true with and it turns out radius and
diameter are very easy on trees although
it's not that obvious if you just try to
do it at first glance so the algorithm
for diameter is folklore but it goes as
follows you start at any vertex V inside
the graph and you choose the you find
the furthest vertex from it and we claim
that that has to be one of the end
points of the diameter and then you can
just search from the furthest vertex for
that for the other end point and you
will know the diameter of the graph and
kind of the reason why this is true is
if the diameter doesn't involve you as
one of the endpoints and then you can
find a longer path inside the graph
because inside the tree there's really
only one path between two notes and then
you get radius for free after you solve
that just from the observation that the
diameter has to fall pretty closely
between to our minus one and two are so
for example if the diameters five you
know the radius has to just be three at
the diameter is 6 the radius still has
to be three so like you can just convert
the diameter are going to radius
algorithm with this equation cool so
what do we do for true with well we
solve all versions in 22 the order k log
k plus a little more than linear time
and we also show that so we solve all of
the versions we presented exactly in
that time so including the direction
versions and by true with on directed
graphs we actually mean the true width
of the underlying undirected graph so if
you just undirected edge and you measure
the truth of that graph and we also show
that even approximating any better than
1.5 into the little of K and truly
sub-project
time woodruff you the appropriate
hardness conjecture so if you did for a
radius problem you would violate the
hitting site conjecture and if you did
it for a diameter problem you would
violate the orthogonal vectors
conjecture and I will go more into the
algorithms in this talk but the lower
bound idea comes basically we take the
constructions that we've been using to
prove our previous quadratic bounds and
we implement them with a graph where the
tree with is small and in this paper
it's actually pretty easy because as is
like I think it's pretty commonly known
if you remove one node then you lower
the truth by most one and as you can see
in the construction if you just remove
all the nodes in the center there's only
C log n of them so and then disconnected
graph will just have constant tree with
cool so I'll be covering how we get this
runtime next so what is our gimmick
strategy so we will solve both radius
and diameter and more by computing the
eccentricity of every single node in the
entire graph and the only way we will be
using tree with is by the reduction to
the following problem so instead of a
general graph will be using the fact
that it has small tree with in order to
kind of sub divide it into two problems
so they'll be notes that on the left a
note set on the right and the only way
to get between these two is the K nodes
in the center so this this goes with
truth concepts like portable or
separator and you can see the tree with
K determines the size of the nodes in
the middle that we have to deal with and
the other property of truth that this
the truth gives us is that the left hand
side and the right hand side are rather
balanced so we will be recursive on this
cool so like I said our strategy will be
recursive s to find all the e centricity
within s and will be recursing on the
right-hand side as well and then we'll
just straight out run Dijkstra from the
KA nodes and center because there's only
a constant number of them or K Avila and
then the only thing the only bit of
information really missing in order to
fully figure out all these centricity is
given a node on the left hand side so I
give you a note on the left hand side
what's the furthest node on the right
hand side I already already know the
furthest node on the left hand side
I via the recursive call and I already
know the furthest note in the middle
because i have a Dykstra call from the
middle nodes alright so we now have the
following three layered problem so like
I said feature on the left what's the
furthest note on the right and we know
it has to go through one of the nodes in
the middle so we can just write down our
Dykstra distances has waited edges and
all of the distances we care about are
only two hot pads inside of this graph
so in order to get from a inside capital
a to its furthest node here we have to
go through one of these K nodes in the
middle so I have the distances from k
both to and from every other node in the
graph so it's just a two-hot paths at
this point okay so I just want to make a
note right here that if k is equal to 1
which happens only when the graph is a
tree this problem is very simple so the
reason is the answer is the same for
every node a on the left you just look
at the furthest node see from the one
single solitary middle node B and that
will be the furthest note from every
node in a so if k equals 1 it's very
easy and then we'll kind of extend that
intuition for K equals 2 so what happens
when K equals 2 so there's two nodes in
the center of our graph there's be one
and there's be too and suppose we are
processing and we find some way and we
want to know the answer for that a so
all the a really gives us is the disc
its distance the to be one and its
distance to be too and sort of the
natural question is which nodes in see
is that the right answer to go through
b1 and out of all those nodes which one
is the farthest from b1 because that's
the one we actually want to use for a
going through be one and then by
symmetry we can do the same thing for b2
and we would have the correct
eccentricity for a sew in equation form
this is a b1 is the optimal portal node
for a node see only if this equation
holds so the path go
through be one to see is better than the
path going through be two to get to see
and then using a pretty standard
rearrangement trick when you're trying
to make comparisons like this the
left-hand side is only the dependent on
a and the right-hand side is essentially
only dependent on C so remember we're
given these at the moment but we could
pre process these and that's the key
idea of the algorithm so the right hand
side can be pre computed ahead of time
so for every node in C will associate
with it well we will associate it with a
point on a line and it will be located
at this coordinate then when we get an A
inside the left set we care about all
the points on the line that are to the
right of this coordinate and then out of
all those points we just want to know
the one that is farthest from b1 because
that's the one that will be used if we
use any node that goes through be one so
like I said in this interval which point
is the furthest from b1 and if you're
familiar with some data structures this
is just data structure problem you can
solve it for example with a binary index
tree but in general when we have a
larger k we will have K minus 1
coordinates and it becomes the K minus 1
dimensional orthogonal range searching
problem so you're essentially given the
K minus 1 dimensional analogue of an
axis aligned rectangle and you want to
know what's the biggest point inside of
this like hyper rectangle and existing
data structures can solve it in n to the
log K minus 2n so sort of one log faster
than you would expect and then log to
the K minus 2n query time but we want to
make inquiries because we want to query
every point in a so our runtime is just
so we will we will make one of these
data structures for every point in the
middle set B and then we'll make n
queries to each data structure and so
that will cost k n log K minus 2 to the
N and then we have to recurse on the
left and right
and the last thing was the Dykstra call
but that's dominated by the first term
so I haven't written it down and then if
you kind of solve this and you use the
fact that the two sides were actually
balanced it turns out you pick up the
factor K because they're not completely
balanced they can be off by a factor of
K is the way the proof goes and you pick
up the log from the current but you get
this runtime and then I guess the
homework exercise is use standard fixed
parameter tractable analysis to show
that this is dominated by the original
fbt time I wrote at the beginning cool
yes so some open questions so there was
a there was an approximation gap for
truly sub quadratic algorithms for
round-trip diameter and there was an
upper bound of two which is just a
metric case and we show three halves is
their way to close this gap and then
perhaps more interestingly for the fixed
parameter side there's quite an
interesting gap between this upper bound
and this lower bound so first of all
this term compared to the little o of K
term this gap is o of is it possible to
like narrow to improve this all the way
down to 0 of K and it might be necessary
there's also a gap between this and this
because this is almost one and that's
almost too so maybe you can improve this
if you increase this one a little bit
and the other no I want to make is our
upper bound was exact and this lower
bound even applies to anything up two or
three halves approximation yeah I think
that's actually a really interesting
question and then finally a a challenge
as most speakers have done is like what
happens if you take the FP T&amp;P framework
and you apply it to your favorite
problem in p can you say anything about
how much the running time improves when
you're allowed to characterized by a
parameter
any questions yes more like your
comments on quite sure that the
algorithm for that there is an algorithm
for diameter that runs in time 2 to the
K times look Delta with dell times the
diameter x m ok it's delta in the
exponent or ok i see a time for the
other sorry i can replace you lost a
bionic elta sometimes better i see if
the damage is constant I because then I
see ya that means that you'll know about
this types those cases in which the
diameter is 2 or 3 as it is in all your
lower buns Yeah right ok those cases the
lower bound is baby answer is so
restricted to of the regime where the
diameter is 2 or 3 or 15 or your
favorite constant 2 to the K times n is
the right answer which is kind of leader
ya understand the round trip diameter in
this regime so should the directive ramp
diameter for this are you talking about
the fut version I am I think this is the
same gap by because this algorithm also
holds and the lower bound for rent robe
diameter goes to tree with so we get
these two but there's still the gap yeah
we have our algorithm runs for all the
directed variants even min
well so two comes one as a I think it's
really nice what you did here I think
taking credit for being doing fpt on
phenomenal time problem i'm sure that
has been not i mean people apt have
always tried to reduce complexity in the
polynomial and all this kind of stuff
I'm just taking that this is I just
saying it's not a completely new thing I
thing to apply apt to poonam real time
that's all I think we actually didn't
know of any previous work that I've done
it but maybe it easily yeah it's okay
you can think of some sense
what's new is that where chidiya to
worry about enter the two minus epsilon
right yeah lowering actually the only
one time when we know that under some
plausible assumption and squared is the
best then our happens when you so this
thing otherwise
in our previous lesson we introduced you
to graphs we defined graph as a
mathematical or logical model and talked
about some of the properties and
applications of graph now in this lesson
we will discuss some more properties of
graph but first I want to do a quick
recap of what we have discussed in our
previous lesson a graph can be defined
as an ordered pair of a set of vertices
and a set of edges we use this formal
mathematical notation G equal V II to
define a graph here V is set of vertices
and E is set of edges ordered pair is
just a pair of mathematical objects in
which order of objects in the pair
matters it matters which element is
first and which element is second in the
pair now as we know to denote number of
elements in a set that we also call
cardinality of a set we use the same
notation that we use for modulus or
absolute value so this is how we can
denote number of vertices and number of
edges in a graph number of vertices
would be number of elements in set V and
number of edges would be number of
elements in set E moving forward this is
how I am going to denote number of
vertices and number of edges in all my
explanations now as we have discussed
earlier edges in a graph can either be
directed that is one-way connections or
undirected that is two-way connections a
graph with only directed edges is called
a directed graph or digraph and a graph
with only undirected edges is called an
undirected graph now sometimes all
connections in a graph cannot be treated
as equal so we label edges with some
weight or cost like what I'm showing
here and a graph in which some value is
associated to connections as cost or
weight is called a weighted graph a
graph is unweighted if there is no cost
distinction among edges okay now we can
also have some special kind of edges in
a graph these edges complicate
algorithms and make working with graphs
difficult but I'm going to talk about
them anyway an edge is called a self
loop or self edge if it involves only
one vertex
if both endpoints of energy are same
then it's called a self-loop we can have
a self-loop in both directed and
undirected graphs but the question is
why would we ever have a self-loop in a
graph well sometimes if edges are
depicting some relationship or
connection that's possible with the same
node as origin as well as destination
then we can have a self loop for example
as we have discussed in our previous
lesson interlinked web pages on the
internet or the world wide web can be it
presented as a directed graph a page
with a unique URL can be a node in the
graph and we can have a directed edge if
a page contains link to another page now
we can have a self loop in this graph
because it's very much possible for a
web page to have a link to itself have a
look at this web page my code school
comm / videos in the header we have
links for workouts page problems page
and read your age right now I'm already
on videos page but I can still click on
videos link and all that will happen
with the click is a refresh because I am
already on videos page my origin and
destination are same here so if I'm
representing world wide web as a
directed graph the way we just discussed
then we have a self loop here now the
next special type of edge that I want to
talk about is Multi edge and edge is
called a multi edge if it occurs more
than once in a graph once again we can
have a multi edge in both directed and
undirected graphs
first multi edge that I'm showing you
here is undirected and the second one is
directed now once again the question why
should we ever have a multi edge well
let's say we are representing flight
Network between cities as a graph a city
would be a node and we can have an edge
if there is a direct flight connection
between any two cities but then there
can be multiple flights between a pair
of cities these flights would have
different names and may have different
costs if I want to keep the information
about all the flights in my graph I can
draw multi edges I can draw one directed
edge for each flight and then I can
label
and edge with its cost or any other
property I just labeled edges here with
some random flight numbers now as we
were saying earlier selfloops and multi
edges often complicate working with
graphs the presence means we need to
take extra care while solving problems
if a graph contains no self-loop or
multi edge it's called a simple graph in
our lessons we will mostly be dealing
with simple graphs now I want you to
answer a very simple question given
number of vertices in a simple graph
that is a graph with no self loop or
multi-edge what would be maximum
possible number of edges well let's see
let's say we want to draw a directed
graph with four vertices I have drawn
forward DC's here I will name these
vertices v1 v2 v3 and v4 so this is my
set of vertices number of elements in
set V is 4 now it's perfectly fine if I
choose not to draw any edge here this
will still be a graph set of edges can
be empty nodes can be totally
disconnected so minimum possible number
of edges in a graph is 0 now if this is
a directed graph what do you think can
be maximum number of edges here well
each node can have directed edges to all
other nodes in this figure here each
node can have directed edges to 3 other
nodes we have 4 nodes in total so
maximum possible number of edges here is
4 into 3 that is 12 I have shown edges
originating from a vertex in same color
here this is the maximum that we can
draw if there is no self loop or
multi-edge in general if there are n
vertices then maximum number of edges in
a directed graph would be n into n minus
1 so in a simple directed graph number
of edges would be in this range 0 to n
into n minus 1 now what do you think
would be the maximum for an undirected
graph in an undirected graph we can have
only one bi-directional edge between a
pair of nodes we can't have two edges in
different directions so here the maximum
would be half of the maximum for
directed
so if the graph is simple and undirected
number of edges would be in the range 0
to n into n minus 1 by 2 remember this
is true only if there is no self loop or
multi-edge now if you can see number of
edges in a graph can be really really
large compared to number of vertices
for example if number of vertices in a
directed graph is equal to 10 maximum
number of edges would be 90 if number of
vertices is 100 maximum number of edges
would be 9900 maximum number of edges
would be close to square of number of
vertices a graph is called dense if
number of edges in the graph is close to
maximum possible number of edges that is
if the number of edges is of the order
of square of number of vertices and a
graph is called sparse if the number of
edges is really less typically close to
number of vertices and not more than
that there is no defined boundary for
what can be called dense and what can be
called sparse it all depends on context
but this is an important classification
while working with graphs a lot of
decisions are made based on whether the
graph is dense or sparse for example we
typically choose a different kind of
storage structure in computer's memory
for a dense graph we typically store a
dense graph in something called
adjacency matrix and for a sparse graph
we typically use something called
adjacency list I'll be talking about
adjacency matrix and adjacency lists in
next lesson ok now the next concept that
I want to talk about is concept of path
in a graph a part in a graph is a
sequence of vertices where each adjacent
pair in the sequence is connected by an
edge I'm highlighting a path here in
this example graph the sequence of
vertices a B F H is a path in this graph
now we have an undirected graph here
edges are bi-directional in a directed
graph all edges must also be aligned in
one direction the direction of the path
part is called simple path if no
vertices are repeated and if what
disease are not repeated then edges will
also not be repeated so in a simple path
both vertices and edges are not repeated
this path a bfh that I have highlighted
here is a simple path but we could also
have a path like this here start vertex
is a and end vertex is D in this path
one edge and two vertices are repeated
in graph theory there is some
inconsistency in use of this term path
most of the time when we say path we
mean a simple path and if repetition is
possible we use this term walk so a path
is basically a walk in which new
vertices or edges are repeated of walk
is called a trail if what disease can be
repeated but edges cannot be repeated I
am highlighting a trail here in this
example graph ok now I want to say this
once again walk and path are often used
as synonyms but most often when we say
path we mean simple path a path in which
vertices and edges are not repeated
between two different vertices if there
is a walk in which vertices or edges are
repeated like this walk that I am
showing you here in this example graph
then there must also be a path or simple
path that is a walk in which what
disease or edges would not be repeated
in this walk that I'm showing you here
we are starting at a and we are ending
our walk at C there is a simple path
from A to C with just one edge all we
need to do is we need to avoid going to
be e H D and then coming back again to a
so this is why we mostly talk about
simple path between two vertices because
if any other walk is possible simple
path is also possible and it makes most
sense to look for a simple path so this
is what I'm going to do throughout our
lessons I'm going to say path and by
path L mean simple path and if it's not
a simple path I will say it explicitly
our graph is called strongly connected
if in the graph there is a path from any
vertex to any other vertex if it's an
undirected graph we simply call it
connected and if it's a directed graph
we call it strongly connected in
leftmost and rightmost graphs that I'm
showing you here we have a path from any
vertex to any other vertex but in this
graph in the middle we do not have a
path from any vertex to any other vertex
we cannot go from vertex C to a we can
go from A to C but we cannot go from C
to a so this is not a strongly connected
graph remember if it's an undirected
graph we simply say connected and if
it's a directed graph we say strongly
connected if a directed graph is not
strongly connected but can be turned
into connected graph by treating all
ages as undirected then such a directed
graph is called weakly connected if we
just ignore the directions of the edges
here this is connected but I would
recommend that you just remember connect
it and strongly connected this leftmost
or undirected graph is connected I
removed one of the edges and now this is
not connected now we have two disjoint
connected components here but the graph
overall is not connected connectedness
of a graph is a really important
property if you remember intra-city road
network road network within a city that
would have a lot of one-ways can be
represented as a directed graph now an
intra-city road network should always be
strongly connected we should be able to
reach any street from any street any
intersection to any intersection ok now
that we understand concept of a path
next I want to talk about cycle in a
graph a walk is called a closed walk if
it starts and ends at same vertex like
what I'm showing here and there is one
more condition the length of the walk
must be greater than 0 length of a walk
or path is number of edges in the path
like for this closed walk
that I'm showing you here length is five
because we have five edges in this walk
so a closed walk is walk that starts and
ends at same vertex and the length of
which is greater than zero now some may
call closed walk a cycle but generally
we use the term cycle for a simple cycle
a simple cycle is a closed walk in which
other than start and end vertices no
other vertex or edge is repeated right
now what I'm showing you here in this
example graph is a simple cycle or we
can just say cycle a graph with no
cycles is called an acyclic graph a tree
if drawn with undirected edges would be
an example of an undirected acyclic
graph here in this tree we can have a
closed walk but we cannot have a simple
cycle in this closed walk that I'm
showing you here our edge is repeated
there would be no simple cycle in a tree
and apart from tree we can have other
kind of undirected acyclic graphs also a
tree also has to be connected now we can
also have a directed acyclic graph as
you can see here also we do not have any
cycle you cannot have a path of length
greater than 0 starting and ending at
the same vertex or directed acyclic
graph is often called a dag cycles in a
graph caused a lot of issues in
designing algorithms for problems like
finding shortest route from one vertex
to another and we will talk about cycles
a lot when we will study some of these
at one still go our thumbs and coming
lessons for this lesson I will stop here
now in our next lesson we will discuss
ways of creating and storing graph in
computer's memory this is it for this
lesson thanks for watching
